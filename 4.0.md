https://github.com/kelseyhightower/kubernetes-the-hard-way

### vmware workstation模板脚本

```BASH
#!/bin/bash
#----------------------------------------------
# Author        : 349925756
# Email         : 349925756@qq.com
# Last modified : 2021-06-08 21:31
# Filename      : uuid.sh
# Description   : 
# Version       : 1.1 
#----------------------------------------------

#Notes:  
#!/bin/bash
#uuid  ip
path_eth0="/etc/sysconfig/network-scripts/ifcfg-eth0"
sed -i "/UUID/c UUID=$(uuidgen)" $path_eth0
sed -i "s/$1/$2/g" $path_eth0
echo "$3" >/etc/hostname
systemctl stop firewalld && systemctl disable firewalld
sed -i "s/SELINUX=.*/SELINUX=disabled/g" /etc/selinux/config
\cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
systemctl enable chronyd
yum install -y vim wget net-tools bash-completion tree nmap dos2unix lrzsz nc lsof tcpdump htop iftop iotop sysstat nethogs 
echo 'source /usr/share/bash-completion/bash_completion' >> ~/.bashrc
echo 'source <(kubectl completion bash)' >> ~/.bashrc
sed -ri 's/.*swap.*/#&/' /etc/fstab                                               
reboot

```

### 服务检查脚本

```BASH
[root@master01 ~]# cat service_check.sh
#!/bin/bash
#后面做个判断的
echo "Kube-apiserver_Check......"
#apiserver 检查
echo "+-------------------------------------------------------+";
for host in master01 master02 master03;do for i in kube-apiserver kube-controller-manager kube-scheduler;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";
echo "Etcd_Check......"
echo "+-------------------------------------------------------+";

#etcd检查
for host in master01 master02 master03;do echo -e "    $host  etcd is  |  \c" && ssh $host systemctl status etcd|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";

echo "docker_Check......"
echo "+-------------------------------------------------------+";
#docker检查
for host in master01 master02 master03 node01 node02;do echo -e "    $host  docker is  |  \c" && ssh $host systemctl status docker|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";


echo "Kube-proxy kubelet_Check......"
#kubelet,proxy检查
echo "+-------------------------------------------------------+";
for host in master01 master02 master03 node01 node02;do for i in kube-proxy kubelet;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";
```

```BASH
[root@master01 ~]# sh service_check.sh 
Kube-apiserver_Check......
+-------------------------------------------------------+
    master01  kube-apiserver is : running
    master01  kube-controller-manager is : running
    master01  kube-scheduler is : running
    master02  kube-apiserver is : running
    master02  kube-controller-manager is : running
    master02  kube-scheduler is : running
    master03  kube-apiserver is : running
    master03  kube-controller-manager is : running
    master03  kube-scheduler is : running
+-------------------------------------------------------+
Etcd_Check......
+-------------------------------------------------------+
    master01  etcd is  |  running
    master02  etcd is  |  running
    master03  etcd is  |  running
+-------------------------------------------------------+
docker_Check......
+-------------------------------------------------------+
    master01  docker is  |  running
    master02  docker is  |  running
    master03  docker is  |  running
    node01  docker is  |  running
    node02  docker is  |  running
+-------------------------------------------------------+
Kube-proxy kubelet_Check......
+-------------------------------------------------------+
    master01  kube-proxy is : running
    master01  kubelet is : running
    master02  kube-proxy is : running
    master02  kubelet is : running
    master03  kube-proxy is : running
    master03  kubelet is : running
    node01  kube-proxy is : running
    node01  kubelet is : running
    node02  kube-proxy is : running
    node02  kubelet is : running
+-------------------------------------------------------+

```



## 一览表

![image-20210630213802774](C:\Users\goo\AppData\Roaming\Typora\typora-user-images\image-20210630213802774.png)



**硬件**

![image-20210630213621845](C:\Users\goo\AppData\Roaming\Typora\typora-user-images\image-20210630213621845.png)

软件

![image-20210630213643840](C:\Users\goo\AppData\Roaming\Typora\typora-user-images\image-20210630213643840.png)

### 组件下载

| **名称**           | **下载页面**                                                 |
| ------------------ | ------------------------------------------------------------ |
| **Centos**         | https://www.centos.org/download/                             |
| **Docker**         | https://download.docker.com/linux/static/stable/x86_64/      |
| **docker-compose** | https://github.com/docker/compose/releases/                  |
| **Kubernetes**     | https://github.com/kubernetes                                |
| **Calico**         | https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises |
| **Coredns**        | https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/coredns |
| **Dashboard**      | https://github.com/kubernetes/dashboard/releases             |
| **cfssl**          | https://github.com/cloudflare/cfssl/releases                 |
| **Etcd**           | https://github.com/etcd-io/etcd/releases                     |
| **CNI**            | https://github.com/containernetworking/plugins/releases/tag/v0.9.1 |

准备工作

有些没有创建，实际操作到的时候再创建

**创建目录**

```bash
--存放软件的目录
[root@master01 ~]# mkdir -p /server/soft/

--存放tls证书目录，三套系统，避免一团糟
[root@master01 ~]# mkdir -p tls/{etcd,kubernetes,harbor}

--etcd工作目录 01 02 03
[root@master01 ~]# mkdir -p /opt/etcd/{cfg,logs,ssl}

--kubernetes工作目录
[root@master01 ~]# mkdir -p /opt/kubernetes/{cfg,logs,ssl}

--harbor服务目录只在主机节点使用
[root@master01 ~]# mkdir /opt/harbor

```

**设置主机名**

```BASH
[root@master01 ~]# echo -e "172.16.0.30 master01\n172.16.0.31 master02\n172.16.0.32 master03\n172.16.0.35 node01\n172.16.0.36 node02\n" >>/etc/hosts
[root@master01 ~]# hostname -i
172.16.0.30
[root@master01 ~]# hostname -s
master01
--其他主机信息一样的操作。我才用xshell 同时5台一起撰写
```

互信

```BASH
--xshell 撰写功能
[root@master01 ~]#  ssh-keygen -t rsa
[root@master01 ~]# ssh-copy-id master01
[root@master01 ~]# ssh-copy-id master02
[root@master01 ~]# ssh-copy-id master03
[root@master01 ~]# ssh-copy-id node01
[root@master01 ~]# ssh-copy-id node02
[root@master01 ~]# for i in `cat /root/host.txt`;do ssh $i hostname -s;done
master01
master02
master03
node01
node02
```

内核参数

```BASH
{ #all
[root@master01 ~]# cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
[root@master01 ~]# sysctl --system
}
```

时间同步

集群最重要的功能

```BASH
{
[root@master01 ~]# yum install ntpdate -y 
[root@master01 ~]# systemctl start ntpdate && systemctl enable ntpdate 
[root@master01 ~]# echo 'ntpdate time.nist.gov' >>/etc/rc.local
[root@master01 ~]# chmod +x /etc/rc.d/rc.local
}
```

```bash
cat > /etc/sysctl.d/k8s.conf << EOF 
 net.bridge.bridge-nf-call-ip6tables = 1 
 net.bridge.bridge-nf-call-iptables = 1 
 EOF 
 sysctl --system # 生效 
```



## cfssl

### cfssl 安装

```BASH
[root@master01 /server/soft]# chmod +x cfssl*
[root@master01 /server/soft]# ll
-rwxr-xr-x 1 root root  16377936 Jun 30 22:23 cfssl_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  13245520 Jun 30 22:23 cfssl-certinfo_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  10892112 Jun 30 22:23 cfssljson_1.6.0_linux_amd64
-rw-r--r-- 1 root root  69725147 Jun 30 22:24 docker-20.10.7.tgz
-rw-r--r-- 1 root root  19389988 Jun 30 22:23 etcd-v3.5.0-linux-amd64.tar.gz
-rw-r--r-- 1 root root 342258563 Jun 30 22:25 kubernetes-server-linux-amd64_1.21.2.tar.gz
[root@master01 /server/soft]# mv cfssl-certinfo_1.6.0_linux_amd64 /usr/local/bin/cfssl-certinfo
[root@master01 /server/soft]# mv cfssl_1.6.0_linux_amd64 /usr/local/bin/cfssl
[root@master01 /server/soft]# mv cfssljson_1.6.0_linux_amd64 /usr/local/bin/cfssljson
```

自签名证书

https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md

```BASH
#初始化证书颁发机构
mkdir ~/cfssl
cd ~/cfssl
cfssl print-defaults config > ca-config.json
cfssl print-defaults csr > ca-csr.json

ca-config.json默认
profile:www with server auth(TLS服务端验证)  x509 v3 client auth（客户端验证）
expiry:默认8760h 默认是一年
样本：ca-config.json
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "server": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth"
                ]
            },
            "client": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "client auth"
                ]
            },
            "peer": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}

样本：ca-csr.json
{
    "CN": "My own CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "US",
            "L": "CA",
            "O": "My Company Name",
            "ST": "San Francisco",
            "OU": "Org Unit 1",
            "OU": "Org Unit 2"
        }
    ]
}

#生成证书和密钥
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

ca-key.pem   #密钥，此证书允许在CA中创建任何类型的证书
ca.csr
ca.pem

生成服务器证书
cfssl print-defaults csr > server.json
```

服务器证书最重要的是CN和host必须修改

```BASH
...
    "CN": "coreos1",
    "hosts": [
        "192.168.122.68",
        "ext.example.com",
        "coreos1.local",
        "coreos1"
    ],
...

生成服务器证书和密钥
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server

没有上面的json文件可以使用下面的替代
echo '{"CN":"coreos1","hosts":[""],"key":{"algo":"rsa","size":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname="192.168.122.68,ext.example.com,coreos1.local,coreos1" - | cfssljson -bare server
一般建议留文件方便后期查阅

```

生成对等证书

```BASH
cfssl print-defaults csr > member1.json

...
    "CN": "member1",
    "hosts": [
        "192.168.122.101",
        "ext.example.com",
        "member1.local",
        "member1"
    ],
...

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1

或者：
echo '{"CN":"member1","hosts":[""],"key":{"algo":"rsa","size":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname="192.168.122.101,ext.example.com,member1.local,member1" - | cfssljson -bare member1
对每个etcd成员主机名重复这些步骤。


```

生成客户端连接证书

```BASH
cfssl print-defaults csr > client.json
对于客户端证书，我们可以忽略主机值并仅将通用名称 (CN) 设置为客户端值：
...
    "CN": "client",
    "hosts": [""],
...

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client

或者
echo '{"CN":"client","hosts":[""],"key":{"algo":"rsa","size":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client

验证数据
openssl x509 -in ca.pem -text -noout 
openssl x509 -in server.pem -text -noout 
openssl x509 -in client.pem -text -noout
不要把你的ca-key.pem放到 Container Linux Config 中，建议存放在安全的地方。此密钥允许生成尽可能多的证书。
妥善保管关键文件。不要忘记设置适当的文件权限，即chmod 0600 server-key.pem.
此TLDR示例中的证书具有server auth和client authX509 V3 扩展，您可以将它们用于服务器和客户端的身份验证。
您也可以自由地为通配符*地址生成密钥和证书。他们将在任何机器上工作。它将简化证书程序，但会增加安全风险。
```



## Etcd部署

http://play.etcd.io/install

```BASH
[root@master01 ~]# cd tls/etcd/
---------------------------------------------------------------------------------
cat > ca-csr.json <<EOF
{
    "CN": "etcd",
    "hosts": [],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Yunnan",
            "L": "Kunming",
            "O": "etcd",
            "OU": "ops"
        }                                                                                                                    
    ]
}
EOF

#ca.csr  ca-csr.json  ca-key.pem  ca.pem
--------------------------------------------------------------------------------
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
--------------------------------------------------------------------------------
# verify 核实
openssl x509 -in ca.pem -text -noout
--------------------------------------------------------------
# cert-generation configuration  证书生成配置
cat > ca-config.json << EOF
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "server": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth"
                ]
            },
            "client": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "client auth"
                ]
            },
            "peer": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
EOF

-------------------------------------------------------------
# CSR 配置
ca-csr.json 

#企业社会责任
ca.csr

# 自签名根 CA 公钥
ca.pem

# 自签名根 CA 私钥
ca-key.pem

# 其他 TLS 资产的证书生成配置
ca-config.json

--------------------------------
#使用本地私钥颁布证书供远程主机访问，这里有3台服务器就生成三台

cat > server-csr.json <<EOF
{
    "CN": "etcd",
    "hosts": [
        "172.16.0.30",
        "172.16.0.31",
        "172.16.0.32",
        "172.16.0.33",
        "172.16.0.34"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Yunnan",
            "L": "Kunming",
            "O": "etcd",
            "OU": "ops"
        }
    ]
}
EOF

#server.csr  server-csr.json  server-key.pem  server.pem
------------------------------------
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server-csr.json | cfssljson -bare server
------------------------------------
# verify
openssl x509 -in server.pem -text -noout

#上面ca-config.json 被分成server,client,peer  其中server ,peer配置相同，client 配置CN就是客户端host忽略，空即可。etcd中只用到peer，但是apiserver就是etcd的客户端。所以这里要生成。

cat > client-csr.json <<EOF
{
    "CN": "client",
    "hosts": [],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Yunnan",
            "L": "Kunming",
            "O": "etcd",
            "OU": "ops"
        }
    ]
}
EOF
#client.csr  client-csr.json  client-key.pem  client.pem

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client-csr.json | cfssljson -bare client

openssl x509 -in client.pem -text -noout
```

```BASH
--------------------------------------
通过人工方式校验
--------------------------------------
basicConstraints CA:FALSE基本约束，实际是为了标志当前签发的证书是否为CA证书，CA:FALSE表明为非CA证书，这种证书无法用来签发其他证书
也就是说这个证书是最终证书了

确认 Issuer 字段的内容和 ca-csr.json 一致；
确认 Subject 字段的内容和 kubernetes-csr.json 一致；
确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；
确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetesprofile 一致；
# cfssl-certinfo -cert kubernetes.pem
--------------------------------------
通过对比方式校验
--------------------------------------
[root@master01 ~/tls/etcd]# openssl verify -CAfile ca.pem server.pem
server.pem: OK
[root@master01 ~/tls/etcd]# openssl verify -CAfile ca.pem client.pem
client.pem: OK

校验证书是否被CA签名
https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md
```

把生成的证书分发到对应的目录

```BASH
#etcd 本身使用的凭证和密钥
[root@master01 ~/tls/etcd]# cp ca.pem server.pem server-key.pem /opt/etcd/ssl/  
#apiserver使用的凭证和密钥
[root@master01 ~/tls/etcd]# cp client.pem client-key.pem /opt/etcd/ssl/
[root@master01 ~/tls/etcd]# cp peer*.pem /opt/etcd/ssl/

```

下载并安装etcd

```BASH
[root@master01 /server/soft]# tar xf etcd-v3.5.0-linux-amd64.tar.gz 
[root@master01 /server/soft]# mv etcd-v3.5.0-linux-amd64/etcd* /usr/local/bin/
[root@master01 /server/soft]# etcd --version
etcd Version: 3.5.0
Git SHA: 946a5a6f2
Go Version: go1.16.3
Go OS/Arch: linux/amd64
```

配置文件

```BASH
[root@master01 /server/soft]#  for i in master01 master02 master03;do scp /usr/local/bin/etcd* $i:/usr/local/bin/;done
[root@master01 /server/soft]# for i in master01 master02 master03;do scp -r  /opt/etcd $i:/opt/;done
--------------------------------------------------------------------
ETCD_IP=$(hostname -i)
ETCD_NAME=$(hostname -s)
ETCD_PATH="/opt/etcd/ssl/"

cat > /etc/systemd/system/etcd.service <<EOF
[Unit]
Description=ETCD Server
Documentation=https://github.com/coreos/etcd
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --name ${ETCD_NAME} \\
  --cert-file ${ETCD_PATH}peer.pem \\
  --key-file ${ETCD_PATH}peer-key.pem \\
  --peer-cert-file ${ETCD_PATH}peer.pem \\
  --peer-key-file ${ETCD_PATH}peer-key.pem \\
  --trusted-ca-file ${ETCD_PATH}ca.pem \\
  --peer-trusted-ca-file ${ETCD_PATH}ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --listen-client-urls https://${ETCD_IP}:2379 \\
  --advertise-client-urls https://${ETCD_IP}:2379 \\
  --listen-peer-urls https://${ETCD_IP}:2380 \\
  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\
  --initial-cluster master01=https://172.16.0.30:2380,master02=https://172.16.0.31:2380,master03=https://172.16.0.32:2380 \\
  --initial-cluster-token etcd_cluster \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd/default.etcd 
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
----------------------------------------------------------------
  ETCDCTL_API=3 etcdctl \
  --endpoints https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \
  --cacert /opt/etcd/ssl/ca.pem \
  --cert /opt/etcd/ssl/peer.pem \
  --key /opt/etcd/ssl/peer-key.pem \
  endpoint health \
  --write-out=table
 --------------------------------------------------------------
 +--------------------------+--------+-------------+-------+
|         ENDPOINT         | HEALTH |    TOOK     | ERROR |
+--------------------------+--------+-------------+-------+
| https://172.16.0.30:2379 |   true |  9.652858ms |       |
| https://172.16.0.31:2379 |   true |  9.723983ms |       |
| https://172.16.0.32:2379 |   true | 13.150944ms |       |
+--------------------------+--------+-------------+-------+

  
# to start service
systemctl daemon-reload && systemctl start etcd && systemctl enable etcd
[root@master01 ~]# for host in master01 master02 master03 ;do ssh $host systemctl status etcd | grep Active ;done


# to get logs from service
systemctl status etcd.service -l --no-pager
journalctl -u etcd -l --no-pager|less
journalctl -f -u etcd

# to stop service
sudo systemctl stop s1.service
sudo systemctl disable s1.service
```



#### etcd配置文件说明

```BASH
#客户端到服务器端的通信
--cert-file=<path>：用于与etcd 的SSL/TLS 连接的证书。设置此选项后，advertise-client-urls 可以    使用 HTTPS 架构
--key-file=<path>: 证书密钥。必须是未加密的
--client-cert-auth: 设置此选项后，etcd 将检查所有传入的 HTTPS 请求以获取由受信任的 CA 签署的客    户端证书，不提供有效客户端证书的请求将失败。如果启用了身份验证，则证书会为 Common Name          字段提供的用户名提供凭据
--trusted-ca-file=<path>: 受信任的证书颁发机构
--auto-tls：使用自动生成的自签名证书与客户端进行 TLS 连接
#对等通信（服务器到服务器，集群）
--peer-cert-file=<path>：用于对等方之间的 SSL/TLS 连接的证书。这将用于侦听对等地址以及向其他对    等方发送请求
--peer-key-file=<path>: 证书密钥。必须是未加密的
--peer-client-cert-auth：设置后，etcd 将检查来自集群的所有传入对等请求，以获取由提供的 CA 签署    的有效客户端证书
--peer-trusted-ca-file=<path>: 受信任的证书颁发机构
--peer-auto-tls：使用自动生成的自签名证书进行对等方之间的 TLS 连接

如果提供了客户端到服务器或对等证书，则还必须设置密钥。所有这些配置选项也可通过环境变量ETCD_CA_FILE，ETCD_PEER_CA_FILE等等。
--cipher-suites：服务器/客户端和对等点之间支持的 TLS 密码套件的逗号分隔列表（空将由 Go 自动填充）。可用于 v3.2.22+、v3.3.7+ 和 v3.4+。
```

验证

```BASH
#启动
$ etcd --name infra0 --data-dir infra0 \
  --cert-file=/path/to/server.crt --key-file=/path/to/server.key \
  --advertise-client-urls=https://127.0.0.1:2379 --listen-client-urls=https://127.0.0.1:2379
----------------------------------------
[root@master01 ~]# curl --cacert /opt/etcd/server.pem https://172.16.0.30:2379/v2/keys/foo -XPUT -d value=bar -v
* About to connect() to 172.16.0.30 port 2379 (#0)
*   Trying 172.16.0.30...
* Connected to 172.16.0.30 (172.16.0.30) port 2379 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* Closing connection 0

$ curl --cacert /path/to/ca.crt --cert /path/to/client.crt --key /path/to/client.key \
  -L https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v
  
  https://etcd.io/docs/v3.5/op-guide/security/
```

## Docker

docker安装

```BASH
[root@master01 /server/soft]# chmod +x docker-compose-Linux-x86_64 
[root@master01 /server/soft]# mv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose
[root@master01 /server/soft]# for i in `cat /root/host.txt`;do scp /usr/local/bin/docker-compose $i:/usr/local/bin/;done
[root@master01 /server/soft]# docker-compose --version
docker-compose version 1.29.2, build 5becea4c
[root@master01 /server/soft]# for i in `cat /root/host.txt`;do scp docker/* $i:/usr/bin/;done
[root@master01 /server/soft]# docker --version
Docker version 20.10.7, build f0df350
[root@master01 /server/soft]# cat > /etc/systemd/system/docker.service << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target
[Service]
Type=notify
ExecStart=/usr/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s
[Install]
WantedBy=multi-user.target
EOF

[root@master01 /server/soft]# for i in `cat /root/host.txt`;do scp /etc/systemd/system/docker.service $i:/etc/systemd/system/;done

[root@master01 /server/soft]# for i in `cat /root/host.txt`;do
  ssh $i systemctl daemon-reload && systemctl start docker && systemctl enable docker && systemctl status docker |grep Active;echo -e "--------$i--------";done
  
```

重启之后的检查

```BASH
[root@master01 ~]#   ETCDCTL_API=3 etcdctl   --endpoints https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379   --cacert /opt/etcd/ssl/ca.pem   --cert /opt/etcd/ssl/peer.pem   --key /opt/etcd/ssl/peer-key.pem   endpoint health   --write-out=table
+--------------------------+--------+-------------+-------+
|         ENDPOINT         | HEALTH |    TOOK     | ERROR |
+--------------------------+--------+-------------+-------+
| https://172.16.0.31:2379 |   true | 10.917071ms |       |
| https://172.16.0.32:2379 |   true | 11.898965ms |       |
| https://172.16.0.30:2379 |   true | 11.793519ms |       |
+--------------------------+--------+-------------+-------+

--因为之前的脚本导入kubectl tab补全，所以所有机器必须有kubectl命令不然一直提示找不到命令
[root@master01 /server/soft/kubernetes/server/bin]# for i in `cat /root/host.txt` ;do scp kubectl kubeadm $i:/usr/local/bin;done
[root@master01 /server/soft/kubernetes/server/bin]# for i in master{01..03} ;do scp kube-apiserver kube-controller-manager kube-scheduler $i:/usr/local/bin;done
[root@master01 /server/soft/kubernetes/server/bin]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.21.2
k8s.gcr.io/kube-controller-manager:v1.21.2
k8s.gcr.io/kube-scheduler:v1.21.2
k8s.gcr.io/kube-proxy:v1.21.2
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0

```



# kubernetes部署

## kubernetes组件证书

install kubectl

```BASH
[root@master01 /server/soft]# tar xf kubernetes-server-linux-amd64_1.21.2.tar.gz 
[root@master01 /server/soft]# cd kubernetes/server/bin/
[root@master01 /server/soft/kubernetes/server/bin]# cp kubectl kubeadm /usr/local/bin/
[root@master01 /server/soft/kubernetes/server/bin]# kubectl version --client
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.2", GitCommit:"092fbfbf53427de67cac1e9fa54aaa09a28371d7", GitTreeState:"clean", BuildDate:"2021-06-16T12:59:11Z", GoVersion:"go1.16.5", Compiler:"gc", Platform:"linux/amd64"}

```

### 配置CA并生成TLS证书

首先配置CA机构然后生成：kube-apiserver 、kube-controller-manager 、kube-scheduler 、kubelet kube-proxy

etcd已经部署好了。/opt/etcd/ssl/中  peer是etcd 集群使用的证书。server是服务器端，client 是客户端，也就是链接到etcd的，kube-apiserver就需要client.pem

```BASH
#CA配置文件，服务组件证书依赖
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "87600h"
      }
    }
  }
}
EOF

#CA机构
cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca
#ca-key.pem  ca.pem
```

客户端和服务器证书

为每个客户端生成一个证书和服务器组件证书，并生成一个admin用户证书

### 管理员客户端证书

```BASH
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem  -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
  
  #admin-key.pem  admin.pem
```

### kubelet客户端证书

Kubernetes 使用一种称为 Node Authorizer[的专用授权模式](https://kubernetes.io/docs/admin/authorization/node/)，它专门授权[Kubelets 发出的](https://kubernetes.io/docs/concepts/overview/components/#kubelet)API 请求。为了获得节点授权器的授权，Kubelets 必须使用一个凭据来标识它们在`system:nodes`组中，用户名是`system:node:<nodeName>`。

```BASH
for i in master01 master02 master03 node01 node02;do
cat > ${i}-csr.json <<EOF
{
  "CN": "system:node:${i}",
   "hosts": [
    "127.0.0.1",
    "172.16.0.30",
    "172.16.0.31",
    "172.16.0.32",
    "172.16.0.33",
    "172.16.0.34",
    "172.16.0.35",
    "172.16.0.36",
    "172.16.0.37"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes ${i}-csr.json | cfssljson -bare ${i}
done

#master01-key.pem master01.pem  master02-key.pem   master02.pem master03-key.pem master03.pem  node01-key.pem node01.pem node02-key.pem node02.pem

```

### kube-controller-manager

```BASH
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes The Hard Way",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

#kube-controller-manager-key.pem  kube-controller-manager.pem
```

### kube-proxy

```BASH
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:node-proxier",
      "OU": "Kubernetes The Hard Way",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

#kube-proxy-key.pem  kube-proxy.pem

```

### kube-scheduler

```BASH
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes The Hard Way",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler

#kube-scheduler-key.pem  kube-scheduler.pem
```

### apiserver



```BASH
cat > kube-apiserver-csr.json <<EOF
{
  "CN": "kubernetes",
  "hosts": [
  "127.0.0.1",
  "10.0.0.1",
  "172.16.0.30",
  "172.16.0.31",
  "172.16.0.32",
  "172.16.0.33",
  "172.16.0.34",
  "172.16.0.35",
  "172.16.0.36",
  "172.16.0.37",
  "*.kubernetes.*",
  "kubernetes.default",
  "kubernetes.default.svc",
  "kubernetes.default.svc.cluster",
  "kubernetes.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json  -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver

#kube-apiserver-key.pem  kube-apiserver.pem

```

### service-account

```BASH
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes service-account-csr.json | cfssljson -bare service-account

#service-account-key.pem  service-account.pem

```

分发客户端和服务器证书

```BASH
#没有的就创建一个目录
[root@node01 ~]# mkdir -p /opt/kubernetes/{cfg,logs,ssl}
-------------------------------------------------------------
客户端
------
for i in master01 master02 master03 node01 node02;do echo "+----------${i}----------+";scp ca.pem ${i}.pem ${i}-key.pem ${i}:/opt/kubernetes/ssl;done

服务器
-------
for i in master01 master02 master03;do echo "+----------${i}----------+";scp ca.pem kube-apiserver*.pem  service-account*.pem ${i}:/opt/kubernetes/ssl;done

#[root@master01 ~/tls/kubernetes]# tree /opt/kubernetes/
/opt/kubernetes/
├── cfg
├── logs
└── ssl
    ├── ca.pem
    ├── kube-apiserver-key.pem
    ├── kube-apiserver.pem
    ├── master01-key.pem
    ├── master01.pem
    ├── service-account-key.pem
    └── service-account.pem


```

## kubeconfig配置文件生成

kubeconfigs是kubernetes客户端能够定位API服务并对其身份验证

客户端配置

生成：kube-controller-manager 、kube-scheduler 、kubelet、 kube-proxy 、admin

kubeconfig公共IP：这里使用LB 的vip地址最佳  。我们没有LB环境，先用master01的apiserver地址

```BASH
KUBE_APISERVER="172.16.0.30"
```

### kubelet.kubeconfig

```BASH
for i in master01 master02 master03 node01 node02; do
  kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBE_APISERVER}:6443 \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config set-credentials system:node:${i} \
    --client-certificate=${i}.pem \
    --client-key=${i}-key.pem \
    --embed-certs=true \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:node:${i} \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig
done

[root@master01 ~/tls/kubernetes]# tree /opt/kubernetes/cfg/
/opt/kubernetes/cfg/
├── master01.kubeconfig
├── master02.kubeconfig
├── master03.kubeconfig
├── node01.kubeconfig
└── node02.kubeconfig
```

### kube-proxy

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-proxy.kubeconfig"

 kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBE_APISERVER}:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-proxy \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
  
  #生成
├── kube-proxy.kubeconfig

```

### kube-controller-manager

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-controller-manager.kubeconfig"

  kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=kube-controller-manager.pem \
    --client-key=kube-controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

├── kube-controller-manager.kubeconfig

```

### kube-scheduler

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-scheduler.kubeconfig"

kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.pem \
    --client-key=kube-scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-scheduler \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

├── kube-scheduler.kubeconfig

```

### admin-kubeconfig

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/admin.kubeconfig"

kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=admin \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

├── admin.kubeconfig
```

### 分发配置文件

```BASH
客户端  复制$i.kubeconfig kube-proxy.kubeconfig
------
for i in master01 master02 master03 node01 node02;do echo "+----------${i}----------+";scp /opt/kubernetes/cfg/${i}.kubeconfig /opt/kubernetes/cfg/kube-proxy.kubeconfig ${i}:/opt/kubernetes/cfg;done
+----------master01----------+
master01.kubeconfig                   100% 6403     7.7MB/s   00:00    
kube-proxy.kubeconfig                 100% 6285     6.4MB/s   00:00    
+----------master02----------+
master02.kubeconfig                   100% 6403     3.8MB/s   00:00    
kube-proxy.kubeconfig                 100% 6285   312.1KB/s   00:00    
+----------master03----------+
master03.kubeconfig                   100% 6403     4.8MB/s   00:00    
kube-proxy.kubeconfig                 100% 6285     7.5MB/s   00:00    
+----------node01----------+
node01.kubeconfig                     100% 6399     4.1MB/s   00:00    
kube-proxy.kubeconfig                 100% 6285    71.4KB/s   00:00    
+----------node02----------+
node02.kubeconfig                     100% 6399     3.9MB/s   00:00    
kube-proxy.kubeconfig                 100% 6285     6.7MB/s   00:00    

服务器 复制kube-controller-manage kube-scheduler
-------
for i in master01 master02 master03;do echo "==========${i}==========";scp /opt/kubernetes/cfg/{kube-controller-manager,kube-scheduler}.kubeconfig ${i}:/opt/kubernetes/cfg;done
==========master01==========
kube-controller-manager.kubeconfig    100% 6353    11.4MB/s   00:00    
kube-scheduler.kubeconfig             100% 6303     7.9MB/s   00:00    
==========master02==========
kube-controller-manager.kubeconfig    100% 6353     4.4MB/s   00:00    
kube-scheduler.kubeconfig             100% 6303     6.0MB/s   00:00    
==========master03==========
kube-controller-manager.kubeconfig    100% 6353     3.4MB/s   00:00    
kube-scheduler.kubeconfig             100% 6303     6.9MB/s   00:00    

```

### 生成数据加密配置和密钥

kubernetes支持对静态数据进行加密的能力

```BASH
#生成加密密钥
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
```

创建encryption-config.yaml加密配置文件

```BASH
cat > /opt/kubernetes/cfg/encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

把加密文件复制到每个控制器上

```BASH
#master 主机
for i in master01 master02 master03;do echo "#####${i}";scp /opt/kubernetes/cfg/encryption-config.yaml ${i}:/opt/kubernetes/cfg;done
```



校对ETCD

```BASH
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://172.16.0.30:2379 \
  --cacert=/opt/etcd/ssl/ca.pem \
  --cert=/opt/etcd/ssl/client.pem \
  --key=/opt/etcd/ssl/client-key.pem
322eb3d030ebf559, started, master01, https://172.16.0.30:2380, https://172.16.0.30:2379, false
7015b46a66a16d1d, started, master03, https://172.16.0.32:2380, https://172.16.0.32:2379, false
d3fd314d63fa3eba, started, master02, https://172.16.0.31:2380, https://172.16.0.31:2379, false

ETCDCTL_API=3 etcdctl member list \
>   --endpoints=https://172.16.0.30:2379 \
>   --cacert=/opt/etcd/ssl/ca.pem \
>   --cert=/opt/etcd/ssl/peer.pem \
>   --key=/opt/etcd/ssl/peer-key.pem
322eb3d030ebf559, started, master01, https://172.16.0.30:2380, https://172.16.0.30:2379, false
7015b46a66a16d1d, started, master03, https://172.16.0.32:2380, https://172.16.0.32:2379, false
d3fd314d63fa3eba, started, master02, https://172.16.0.31:2380, https://172.16.0.31:2379, false

server.pem不可以访问这种方式是c/s模式
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://172.16.0.30:2379 \
  --cacert=/opt/etcd/ssl/ca.pem \
  --cert=/opt/etcd/ssl/etcd.pem \
  --key=/opt/etcd/ssl/etcd-key.pem \
    endpoint health \
  --write-out=table
```

## 配置kubernetes

在每台master主机安装上kube-apiserver、kube-controller-manager、kube-scheduler

```BASH
[root@master01 /server/soft/kubernetes/server/bin]# for i in master{01..03};do scp kube-apiserver kube-controller-manager kube-scheduler $i:/usr/local/bin ;done
kube-apiserver                           100%  116MB 101.4MB/s   00:01    
kube-controller-manager                  100%  111MB  95.2MB/s   00:01    
kube-scheduler                           100%   45MB  96.4MB/s   00:00    
kube-apiserver                           100%  116MB 116.4MB/s   00:01    
kube-controller-manager                  100%  111MB 110.9MB/s   00:01    
kube-scheduler                           100%   45MB 113.8MB/s   00:00    
kube-apiserver                           100%  116MB 116.4MB/s   00:01    
kube-controller-manager                  100%  111MB 115.1MB/s   00:00    
kube-scheduler                           100%   45MB 115.9MB/s   00:00    

```

检查必须的证书

```BASH
[root@master01 ~]# ls /opt/kubernetes/ssl/
ca.pem                  kube-apiserver.pem  master01.pem             service-account.pem
kube-apiserver-key.pem  master01-key.pem    service-account-key.pem
差ca-key.pem
for i in master{01..03};do scp /root/tls/kubernetes/ca-key.pem $i:/opt/kubernetes/ssl/ ;done
[root@master01 ~]# ls /opt/kubernetes/ssl/
ca-key.pem  kube-apiserver-key.pem  master01-key.pem  service-account-key.pem
ca.pem      kube-apiserver.pem      master01.pem      service-account.pem

#检查encryption-config.yaml 
[root@master01 ~]# ls /opt/kubernetes/cfg
admin.kubeconfig                    kube-proxy.kubeconfig      master02.kubeconfig  node02.kubeconfig
encryption-config.yaml              kube-scheduler.kubeconfig  master03.kubeconfig
kube-controller-manager.kubeconfig  master01.kubeconfig        node01.kubeconfig
```

配置

```BASH
KUBE_APISERVER_IP=$(hostname -i)

cat <<EOF |  tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--etcd-servers=https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \\
--bind-address=0.0.0.0 \\
--secure-port=6443 \\
--advertise-address=${KUBE_APISERVER_IP} \\
--allow-privileged=true \\
--apiserver-count=3 \\
--service-cluster-ip-range=10.0.0.0/24 \\
--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
--authorization-mode=RBAC,Node \\
--event-ttl=1h \\
--encryption-provider-config=/opt/kubernetes/cfg/encryption-config.yaml \\
--service-node-port-range=30000-32767 \\
--kubelet-certificate-authority=/opt/kubernetes/ssl/ca.pem \\
--kubelet-client-certificate=/opt/kubernetes/ssl/kube-apiserver.pem \\
--kubelet-client-key=/opt/kubernetes/ssl/kube-apiserver-key.pem \\
--tls-cert-file=/opt/kubernetes/ssl/kube-apiserver.pem \\
--tls-private-key-file=/opt/kubernetes/ssl/kube-apiserver-key.pem \\
--client-ca-file=/opt/kubernetes/ssl/ca.pem \\
--runtime-config=api/all=true \\
--service-account-issuer=https://${KUBE_APISERVER_IP}:6443 \\
--service-account-key-file=/opt/kubernetes/ssl/service-account.pem \\
--service-account-signing-key-file=/opt/kubernetes/ssl/service-account-key.pem \\
--etcd-cafile=/opt/etcd/ssl/ca.pem \\
--etcd-certfile=/opt/etcd/ssl/client.pem \\
--etcd-keyfile=/opt/etcd/ssl/client-key.pem \\
--proxy-client-cert-file=/opt/kubernetes/ssl/kube-apiserver.pem \\
--proxy-client-key-file=/opt/kubernetes/ssl/kube-apiserver-key.pem \\
--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\
--requestheader-allowed-names=kubernetes \\
--requestheader-extra-headers-prefix=X-Remote-Extra- \\
--requestheader-group-headers=X-Remote-Group \\
--requestheader-username-headers=X-Remote-User \\
--enable-aggregator-routing=true \\
--audit-log-maxage=30 \\
--audit-log-maxbackup=3 \\
--audit-log-maxsize=100 \\
--audit-log-path=/opt/kubernetes/logs/k8s-audit.log 
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
-------------------------------------------------------------------------

systemctl daemon-reload && systemctl start kube-apiserver.service && systemctl enable kube-apiserver.service 

```

问题：

```BASH
https://github.com/kubernetes/kubernetes/issues/76956

Unable to remove old endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: /registry/masterleases/172.16.0.30, ResourceVersion: 0, AdditionalErrorMsg:

[root@master01 ~]# kube-apiserver -h | grep enable-admission-plugins
查看默认启动那些插件
https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/
```

## 配置kube-controller-manager

```BASH
cat <<EOF | tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.244.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\
  --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --service-account-private-key-file=/opt/kubernetes/ssl/service-account-key.pem \\
  --service-cluster-ip-range=10.0.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
-----------------------
systemctl daemon-reload && systemctl start kube-controller-manager.service && systemctl enable kube-controller-manager.service 

```

## 配置kube-scheduler

```BASH
--创建kube-scheduler.yaml配置文件
cat <<EOF | tee /opt/kubernetes/cfg/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/opt/kubernetes/cfg/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
------------------------------------------------------
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler --logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--leader-elect=true \\
--kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig \\
--bind-address=127.0.0.1
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
------------------------------------------------------
 systemctl daemon-reload && systemctl start kube-scheduler.service && systemctl enable kube-scheduler.service 

systemctl daemon-reload && systemctl restart kube-scheduler.service 

```

```BASH
[root@master01 /server/soft]# kubectl get cs --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig 
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
controller-manager   Healthy   ok                              
scheduler            Healthy   ok                              
etcd-1               Healthy   {"health":"true","reason":""}   
etcd-0               Healthy   {"health":"true","reason":""}   
etcd-2               Healthy   {"health":"true","reason":""}   

```



### LB负载均衡检查

nginx 将被安装和配置为接受端口上的 HTTP 健康检查，`80`并将连接代理到 上的 API 服务器`https://127.0.0.1:6443/healthz`

国内采用keepalived  nginx 代理

`/healthz`默认情况下，API 服务器端点不需要身份验证。

```BASH
先弄过来后期再调整
cat > kubernetes.default.svc.cluster.local <<EOF
server {
  listen      80;
  server_name kubernetes.default.svc.cluster.local;

  location /healthz {
     proxy_pass                    https://127.0.0.1:6443/healthz;
     proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem;
  }
}
EOF

{
  sudo mv kubernetes.default.svc.cluster.local \
    /etc/nginx/sites-available/kubernetes.default.svc.cluster.local

  sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/
}

```

确认

```BASH
[root@master01 ~]# kubectl cluster-info --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig

测试nginx 
curl -H "Host: kubernetes.default.svc.cluster.local" -i http://127.0.0.1/healthz

```

### RBAC(用于kubelet)

配置 RBAC 权限以允许 Kubernetes API 服务器访问每个工作节点上的 Kubelet API。在 pod 中检索指标、日志和执行命令需要访问 Kubelet API。

Kubelet--authorization-mode标志设置为Webhook. Webhook 模式使用SubjectAccessReview API 来确定授权。

下面的命令将影响整个集群，并且只需要从控制器节点之一运行一次

```BASH
创建system:kube-apiserver-to-kubelet ClusterRole有权限访问Kubelet API，并执行与管理相关的pod最常见的任务：

cat <<EOF | kubectl apply --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF

```

Kubernetes API 配置文件中：kubernetes使用--kubelet-client-certificate 来定义的客户端证书以用户身份向 Kubelet 进行身份验证。

将system:kube-apiserver-to-kubeletClusterRole绑定到kubernetes用户：

```BASH
cat <<EOF | kubectl apply --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```

```BASH
[root@master01 ~]# curl --cacert /opt/kubernetes/ssl/ca.pem https://127.0.0.1:6443/version
{
  "major": "1",
  "minor": "21",
  "gitVersion": "v1.21.2",
  "gitCommit": "092fbfbf53427de67cac1e9fa54aaa09a28371d7",
  "gitTreeState": "clean",
  "buildDate": "2021-06-16T12:53:14Z",
  "goVersion": "go1.16.5",
  "compiler": "gc",
  "platform": "linux/amd64"
}
```

## 工作节点

准备工作

```BASH
[root@master01 ~]# yum install -y install socat conntrack ipset
#socat 二进制文件启用对kubectl port-forward命令的支持。

[root@master01 /server/soft]# mkdir /opt/cni/bin -p
[root@master01 /server/soft]# tar xf cni-0.8.1.tar.gz -C /opt/cni/bin

```

### 配置CNI网络

```BASH

[root@master01 /server/soft]# wget https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz

[root@master01 /server/soft]# tar xvf cni-plugins-linux-amd64-v0.9.1.tgz -C /opt/cni/bin/

#创建bridge网络配置文件
cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.4.0",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "10.244.0.0/16"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF
#创建loopback网络配置文件
cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.4.0",
    "name": "lo",
    "type": "loopback"
}
EOF

```

同传到其他几台机器上去

```BASH
[root@master01 ~]# for i in master02 master03 node01 node02;do echo "+----------$i----------+";scp -r /opt/cni/ $i:/opt/;done
[root@master01 ~]# for i in master02 master03 node01 node02;do echo "+----------$i----------+";scp -r /etc/cni/ $i:/etc/;done

```





### kubelet

创建kubelet-config.yaml配置文件

```BASH
cat <<EOF | sudo tee /opt/kubernetes/cfg/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/opt/kubernetes/ssl/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.0.0.2"
podCIDR: "10.244.0.0/16"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/opt/kubernetes/ssl/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/opt/kubernetes/ssl/${HOSTNAME}-key.pem"
EOF
```

```BASH
创建上面提到的resolvconf DNSw
echo 'mkdir -p /run/systemd/resolve/ && ln -s /etc/resolv.conf /run/systemd/resolve/resolv.conf' >>/etc/rc.local

```



该resolvConf配置用于在运行的系统上使用 CoreDNS 进行服务发现时避免循环systemd-resolved

创建systemd  https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet/

```BASH
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/opt/kubernetes/cfg/kubelet-config.yaml \\
  --container-runtime=docker \\
  --container-runtime-endpoint=unix:///var/run/dockershim.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/opt/kubernetes/cfg/${HOSTNAME}.kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
--------------------------------------------------------------
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --container-runtime=docker \\
  --container-runtime-endpoint=unix:///var/run/dockershim.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/opt/kubernetes/cfg/${HOSTNAME}.kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --cluster-dns=10.0.0.2 \\
  --cluster-domain=cluster.local \\
  --pod-cidr=10.244.0.0/16 \\
  --authorization-mode=Webhook \\
  --client-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --tls-cert-file=/opt/kubernetes/ssl/${HOSTNAME}.pem \\
  --tls-private-key-file=/opt/kubernetes/ssl/${HOSTNAME}-key.pem \\
  --pod-infra-container-image=louwy001/pause:3.4.1  \\
  --runtime-request-timeout=15m \\
  --resolv-conf=/run/systemd/resolve/resolv.conf \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
--------------------------------------------------------------
```

--container-runtime要使用的容器运行时。目前支持 `docker、remote`。

--container-runtime-endpoint 默认值：`unix:///var/run/dockershim.sock`             

​                                                      unix:///var/run/containerd/containerd.sock 

[实验性特性] 远程运行时服务的端点。目前支持 Linux 系统上的 UNIX 套接字和 Windows 系统上的 npipe 和 TCP 端点。例如： `unix:///var/run/dockershim.sock`、 `npipe:////./pipe/dockershim`

### kube-proxy

创建kube-proxy-config.yaml配置文件

```BASH
cat <<EOF | sudo tee /opt/kubernetes/cfg/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/opt/kubernetes/cfg/kube-proxy.kubeconfig"
hostnameOverride: ${hostname}
clusterCIDR: "10.244.0.0/16"
EOF
```

systemd

```BASH
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/opt/kubernetes/cfg/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
------------------------------------------------------------------
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig \\
--cluster-cidr=10.244.0.0/16 \\
--hostname-override= ${hostname}
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
------------------------------------------------------------------
```

启动服务

```BASH
[root@master01 /server/soft/kubernetes/server/bin]# for i in master01 master02 master03 node01 node02;do echo "-----$i----";scp kubelet kube-proxy $i:/usr/local/bin ;done

systemctl daemon-reload && systemctl start kubelet kube-proxy && systemctl enable kubelet kube-proxy

```

### 配置kubectl

```BASH
[root@master01 ~]# mkdir -p /root/.kube
[root@master01 ~]# cp /opt/kubernetes/cfg/admin.kubeconfig .kube/config
[root@master01 ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
scheduler            Healthy   ok                              
controller-manager   Healthy   ok                              
etcd-0               Healthy   {"health":"true","reason":""}   
etcd-1               Healthy   {"health":"true","reason":""}   
etcd-2               Healthy   {"health":"true","reason":""}   

```

一些基本查看

```BASH
[root@master01 ~]# kubectl get nodes 
NAME       STATUS   ROLES    AGE   VERSION
master01   Ready    <none>   49m   v1.21.2
master02   Ready    <none>   49m   v1.21.2
master03   Ready    <none>   49m   v1.21.2
node01     Ready    <none>   49m   v1.21.2
node02     Ready    <none>   49m   v1.21.2

[root@master01 ~]# kubectl get pod --all-namespaces 

[root@master01 ~]# kubectl describe nodes master01|grep Taints
Taints:             <none>

#其他master节点
mkdir -p /root/.kube
[root@master01 ~]# scp ~/.kube/config master02:~/.kube/
config                                                                          100% 6135     4.8MB/s   00:00    
[root@master01 ~]# scp ~/.kube/config master03:~/.kube/
config                                                                          100% 6135     5.7MB/s   00:00    
```

### CoreDNS

```BASH
kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns-1.8.yaml
首先下载这个文件

[root@master01 /opt/kubernetes/yaml]# kubectl create -f coredns-1.8.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created

[root@master01 /opt/kubernetes/yaml]# kubectl get pod -n kube-system 
NAME                       READY   STATUS              RESTARTS   AGE
coredns-8494f9c688-m2zpc   0/1     ContainerCreating   0          24s
coredns-8494f9c688-vv67c   0/1     ContainerCreating   0          24s

[root@master01 /opt/kubernetes/yaml]# kubectl describe pod coredns-8494f9c688-m2zpc -n kube-system 

kubelet            Failed to create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory
[root@master01 /opt/kubernetes/yaml]# mkdir -p /run/systemd/resolve/
[root@master01 /opt/kubernetes/yaml]# cp /etc/resolv.conf /run/systemd/resolve/
[root@master01 /opt/kubernetes/yaml]# scp -r /run/systemd/resolve/ master02:/run/systemd/
resolv.conf         100%   78    77.1KB/s   00:00    
[root@master01 /opt/kubernetes/yaml]# scp -r /run/systemd/resolve/ master03:/run/systemd/
resolv.conf          100%   78    98.3KB/s   00:00    
[root@master01 /opt/kubernetes/yaml]# scp -r /run/systemd/resolve/ node01:/run/systemd/
resolv.conf           100%   78   118.5KB/s   00:00    
[root@master01 /opt/kubernetes/yaml]# scp -r /run/systemd/resolve/ node02:/run/systemd/

```

## 遇到的问题

```bash
 Failed to create pod sandbox: rpc error: code = Unknown desc = failed pulling image "k8s.gcr.io/pause:3.4.1": Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

[root@master01 /opt/kubernetes/yaml]# docker search pause:3.4.1
NAME             DESCRIPTION               STARS     OFFICIAL   AUTOMATED
louwy001/pause   k8s.gcr.io/pause:3.4.1    0                    
ninokop/pause    k8s.gcr.io/pause:3.4.1    0                    
[root@master01 /opt/kubernetes/yaml]# docker pull louwy001/pause:3.4.1
3.4.1: Pulling from louwy001/pause
fac425775c9d: Pull complete 
Digest: sha256:9ec1e780f5c0196af7b28f135ffc0533eddcb0a54a0ba8b32943303ce76fe70d
Status: Downloaded newer image for louwy001/pause:3.4.1
docker.io/louwy001/pause:3.4.1
[root@master01 /opt/kubernetes/yaml]# docker images
REPOSITORY       TAG       IMAGE ID       CREATED        SIZE
louwy001/pause   3.4.1     0f8457a4c2ec   5 months ago   683kB
[root@master01 /opt/kubernetes/yaml]# docker tag 0f8457a4c2ec k8s.gcr.io/pause:3.4.1
[root@master01 /opt/kubernetes/yaml]# docker images
REPOSITORY         TAG       IMAGE ID       CREATED        SIZE
louwy001/pause     3.4.1     0f8457a4c2ec   5 months ago   683kB
k8s.gcr.io/pause   3.4.1     0f8457a4c2ec   5 months ago   683kB


```

![image-20210702233716732](C:\Users\goo\AppData\Roaming\Typora\typora-user-images\image-20210702233716732.png)



```bash
systemctl daemon-reload && systemctl restart kube-apiserver.service kube-controller-manager.service kube-scheduler.service kubelet.service kube-proxy.service 
```

