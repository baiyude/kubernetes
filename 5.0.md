4.0是etcd kubernetes CA分开版本。5.0把他们合到一起

# 准备工作

| **名称**           | **下载页面**                                                 |
| ------------------ | ------------------------------------------------------------ |
| **Centos**         | https://www.centos.org/download/                             |
| **Docker**         | https://download.docker.com/linux/static/stable/x86_64/      |
| **docker-compose** | https://github.com/docker/compose/releases/                  |
| **Kubernetes**     | https://github.com/kubernetes                                |
| **Calico**         | https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises |
| **Coredns**        | https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/coredns |
| **Dashboard**      | https://github.com/kubernetes/dashboard/releases             |
| **cfssl**          | https://github.com/cloudflare/cfssl/releases                 |
| **Etcd**           | https://github.com/etcd-io/etcd/releases                     |
| **CNI**            | https://github.com/containernetworking/plugins/releases/tag/v0.9.1 |
| **calicoctl**      | https://github.com/projectcalico/calicoctl/releases          |

准备工作

```BASH
#!/bin/bash
#----------------------------------------------
# Author        : 349925756
# Email         : 349925756@qq.com
# Last modified : 2021-06-08 21:31
# Filename      : uuid.sh
# Description   : 
# Version       : 1.1 
#----------------------------------------------
#修改网卡UUID IP地址 主机名，关闭防火墙，selinux 安装常用工具关闭swap 导入kubectl tab补全
#uuid  ip
path_eth0="/etc/sysconfig/network-scripts/ifcfg-eth0"
sed -i "/UUID/c UUID=$(uuidgen)" $path_eth0   
sed -i "s/$1/$2/g" $path_eth0
echo "$3" >/etc/hostname
systemctl stop firewalld && systemctl disable firewalld
sed -i "s/SELINUX=.*/SELINUX=disabled/g" /etc/selinux/config
\cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
systemctl enable chronyd
yum install epel-release -y
yum install -y vim wget net-tools bash-completion tree nmap dos2unix lrzsz nc lsof tcpdump htop iftop iotop sysstat nethogs git iptables conntrack ipvsadm ipset jq sysstat libseccomp
echo 'source /usr/share/bash-completion/bash_completion' >> ~/.bashrc
echo 'source <(kubectl completion bash)' >> ~/.bashrc
sed -ri 's/.*swap.*/#&/' /etc/fstab 
iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT
reboot

cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

cat > /etc/sysctl.d/k8s.conf << EOF 
net.bridge.bridge-nf-call-ip6tables = 1 
net.bridge.bridge-nf-call-iptables = 1 
EOF
sysctl --system  # 生效 

```

检查服务脚本

```BASH
[root@master01 ~]# cat service_check.sh
#
echo "Nginx keepalived_Check......"
#nginx,keepalived检查
echo "+-------------------------------------------------------+";
for host in master{01..03};do for i in nginx keepalived;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";


"Kube-apiserver_Check......"
#apiserver 检查
echo "+-------------------------------------------------------+";
for host in master{01..03};do for i in kube-apiserver kube-controller-manager kube-scheduler;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";
echo "Etcd_Check......"
echo "+-------------------------------------------------------+";

#etcd检查
for host in master{01..03};do echo -e "    $host  etcd is  |  \c" && ssh $host systemctl status etcd|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";

echo "docker_Check......"
echo "+-------------------------------------------------------+";
#docker检查
for host in master{01..03} node{01,02};do echo -e "    $host  docker is  |  \c" && ssh $host systemctl status docker|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";


echo "Kube-proxy kubelet_Check......"
#kubelet,proxy检查
echo "+-------------------------------------------------------+";
for host in master{01..03} node{01,02};do for i in kube-proxy kubelet;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";                                                                            


```

tab补全

https://github.com/scop/bash-completion

```BASH
./configure
make           # GNU make required
make check     # optional, requires python3 with pytest >= 3.6, pexpect
make install   # as root
```





修改limit

```BASH
vim /etc/systemd/system.conf
DefaultLimitNOFILE=100000
DefaultLimitNPROC=65535
```

创建master节点目录

```BASH
mkdir -p tls \
               /opt/{etcd,kubernetes}/{cfg,logs,ssl} \
               /etc/cni/net.d  \
               ~/.kube \
               /run/systemd/resolve \
               /opt/cni/bin
               
[root@master01 ~]# tree /opt
/opt
├── cni
│   └── bin
├── etcd
│   ├── cfg
│   ├── logs
│   └── ssl
└── kubernetes
    ├── cfg
    ├── logs
    └── ssl
[root@master01 ~]# mkdir /opt/yaml
[root@master01 ~]# cd /opt/yaml/
[root@master01 /opt/yaml]# 

```

创建work节点目录

```BASH
mkdir -p /opt/kubernetes/{cfg,logs,ssl} \
               /etc/cni/net.d   \
               /run/systemd/resolve \
               /opt/cni/bin
               
[root@node02 ~]# tree /opt
/opt
├── cni
│   └── bin
└── kubernetes
    ├── cfg
    ├── logs
    └── ssl
```

上传软件到soft目录

```BASH
#master01
mkdir -p /server/soft && cd /server/soft
[root@master01 /server/soft]# chmod +x cfssl* docker-compose-Linux-x86_64 
[root@master01 /server/soft]# ll
total 512120
-rwxr-xr-x 1 root root  16377936 Jul  3 20:51 cfssl_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  13245520 Jul  3 20:51 cfssl-certinfo_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  10892112 Jul  3 20:51 cfssljson_1.6.0_linux_amd64
-rw-r--r-- 1 root root  39771622 Jul  3 20:52 cni-plugins-linux-amd64-v0.9.1.tgz
-rw-r--r-- 1 root root  69725147 Jul  3 20:52 docker-20.10.7.tgz
-rwxr-xr-x 1 root root  12737304 Jul  3 20:52 docker-compose-Linux-x86_64
-rw-r--r-- 1 root root  19389988 Jul  3 20:52 etcd-v3.5.0-linux-amd64.tar.gz
-rw-r--r-- 1 root root 342258563 Jul  3 20:52 kubernetes-server-linux-amd64_1.21.2.tar.gz


```

移动对应的命令到/usr/local/bin

```BASH
[root@master01 /server/soft]# tar xf docker-20.10.7.tgz 
[root@master01 /server/soft]# tar xf etcd-v3.5.0-linux-amd64.tar.gz 
[root@master01 /server/soft]# tar xf kubernetes-server-linux-amd64_1.21.2.tar.gz 
[root@master01 /server/soft]# tar xf cni-plugins-linux-amd64-v0.9.1.tgz -C /opt/cni/bin
[root@master01 /server/soft]# ls /opt/cni/bin/
bandwidth  dhcp      flannel      host-local  loopback  portmap  sbr     tuning  vrf
bridge     firewall  host-device  ipvlan      macvlan   ptp      static  vlan
[root@master01 /server/soft]# cp docker/* /usr/local/bin
[root@master01 /server/soft]# cp etcd-v3.5.0-linux-amd64/etcd* /usr/local/bin
[root@master01 /server/soft]# cd kubernetes/server/bin/
[root@master01 /server/soft/kubernetes/server/bin]# cp kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy kubectl kubeadm /usr/local/bin
```

把上面的命令分发到对应的主机上

建立互信

```BASH
ssh-keygen -t rsa
 for i in {30,31,32,35,36};do ssh-copy-id root@172.16.0.$i;done


echo -e "172.16.0.30 master01\n172.16.0.31 master02\n172.16.0.32 master03\n172.16.0.35 node01\n172.16.0.36 node02\n" >>/etc/hosts

[root@master01 ~]# hostname -i
172.16.0.30
[root@master01 ~]# hostname -s
master01
[root@master01 ~/tls]# sed -i "8,14d" /etc/hosts

```

分发命令

```BASH
[root@master01 ~]# for i in master{01..03};do scp -r /usr/local/bin $i:/usr/local;done
[root@master02 ~]# rm -f /usr/local/bin/cfssl*
[root@master03 ~]# rm -f /usr/local/bin/cfssl*
[root@master01 ~]# for i in node{01,02};do scp -r /usr/local/bin $i:/usr/local;done
[root@node01 ~]# rm -f /usr/local/bin/{cfssl*,kube-apiserver,kube-controller-manager,kube-scheduler}
[root@node02 ~]# rm -f /usr/local/bin/{cfssl*,kube-apiserver,kube-controller-manager,kube-scheduler}
```

# CA

整个kubernetes都是采用TLS方式验证，安全性有保障

```BASH
[root@master01 ~]# cd tls/

cat > ca-csr.json <<EOF
{
    "CN": "CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Yunnan",
            "L": "Kunming",
            "O": "CA",
            "OU": "System"
        }                                                                                                                    
    ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

cat > ca-config.json << EOF
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "etcd": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            },
            "kubernetes": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
EOF
------------------------------------------
# CSR 配置
ca-csr.json 
#企业社会责任
ca.csr
# 自签名根 CA 公钥
ca.pem
# 自签名根 CA 私钥
ca-key.pem
# 其他 TLS 资产的证书生成配置
ca-config.json
----------------------------------------
```



etcd
---------------------
```BASH
cat > etcd-csr.json <<EOF
{
    "CN": "etcd-ca",
    "hosts": [
        "localhost",
        "127.0.0.1",
        "master01",
        "master02",
        "master03",
        "172.16.0.30",
        "172.16.0.31",
        "172.16.0.32",
        "172.16.0.33",
        "172.16.0.34"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Yunnan",
            "L": "Kunming"
        }
    ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd etcd-csr.json | cfssljson -bare etcd

校验：openssl verify -CAfile ca.pem xxx.pem
[root@master01 ~/tls]# openssl verify -CAfile ca.pem etcd.pem
etcd.pem: OK
```




admin
-------------------
```BASH
cat > admin-csr.json <<EOF
{
  "CN": "kubernetes-admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:masters",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem  -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin

[root@master01 ~/tls]#  openssl verify -CAfile ca.pem admin.pem
admin.pem: OK
```




kubelet
-----------------
```BASH
for i in master{01..03} node{01,02};do
cat > ${i}-csr.json <<EOF
{
  "CN": "system:node:${i}",
   "hosts": [
   "localhost",
    "127.0.0.1",
    "172.16.0.30",
    "172.16.0.31",
    "172.16.0.32",
    "172.16.0.33",
    "172.16.0.34",
    "172.16.0.35",
    "172.16.0.36",
    "172.16.0.37",
    "master01",
    "master02",
    "master03",
    "node01",
    "node02"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:nodes",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes ${i}-csr.json | cfssljson -bare ${i}
done

[root@master01 ~/tls]# for i in master{01..03} node{01,02};do openssl verify -CAfile ca.pem ${i}.pem;done
master01.pem: OK
master02.pem: OK
master03.pem: OK
node01.pem: OK
node02.pem: OK

```

## kube-controller-manager

```BASH
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:kube-controller-manager",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

openssl verify -CAfile ca.pem kube-controller-manager.pem
```

## kube-scheduler

```BASH
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:kube-scheduler",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
openssl verify -CAfile ca.pem kube-scheduler.pem
```

## kube-apiserver

```BASH
cat > kube-apiserver-csr.json <<EOF
{
  "CN": "kube-apiserver",
  "hosts": [
  "127.0.0.1",
  "10",
  "172.16.0.30",
  "172.16.0.31",
  "172.16.0.32",
  "172.16.0.33",
  "172.16.0.34",
  "172.16.0.35",
  "172.16.0.36",
  "172.16.0.37",
  "master01",
  "master02",
  "master03",
  "node01",
  "node02",
  "kubernetes",
  "kubernetes.default",
  "kubernetes.default.svc",
  "kubernetes.default.svc.cluster",
  "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:masters",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json  -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver
openssl verify -CAfile ca.pem kube-apiserver.pem
```

## kube-proxy

```BASH
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:node-proxy",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
openssl verify -CAfile ca.pem kube-proxy.pem
```

## service-account

```BASH
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:masters",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes service-account-csr.json | cfssljson -bare service-account
openssl verify -CAfile ca.pem service-account.pem
```

检查证书信息是否正确

```BASH
[root@master01 ~/tls]# ls *.pem|grep -v key
admin.pem
ca.pem
etcd.pem
kube-apiserver.pem
kube-controller-manager.pem
kube-proxy.pem
kube-scheduler.pem
master01.pem
master02.pem
master03.pem
node01.pem
node02.pem
service-account.pem

openssl x509 -in  -text -noout  ca.pem

for i in `ls *.pem|grep -v key`;do openssl verify -CAfile ca.pem  $i;done
admin.pem: OK
ca.pem: OK
etcd.pem: OK
kube-apiserver.pem: OK
kube-controller-manager.pem: OK
kube-proxy.pem: OK
kube-scheduler.pem: OK
master01.pem: OK
master02.pem: OK
master03.pem: OK
node01.pem: OK
node02.pem: OK
service-account.pem: OK

for i in `ls *.pem|grep -v key`;do openssl x509 -in $i -text -noout |grep CA:;done
```

|                | CN                             | C    | L       | ST     | O                              | OU     |
| -------------- | ------------------------------ | ---- | ------- | ------ | ------------------------------ | ------ |
| ca             | CA                             | CN   | Kunming | Yunnan | CA                             | System |
| admin          | admin                          | -    | -       | -      | system:masters                 | -      |
| etcd           | etcd                           | -    | -       | -      | system:etcd                    | -      |
| apiserver      | kubernetes                     | -    | -       | -      | system:masters                 | -      |
| controller-m   | system:kube-controller-manager | -    | -       | -      | system:kube-controller-manager | -      |
| scheduler      | system:kube-scheduler          | -    | -       | -      | system:kube-scheduler          | -      |
| proxy          | system:kube-proxy              | -    | -       | -      | system:node-proxy              | -      |
| master01       | system:node:master01           |      |         |        | system:nodes                   |        |
| maste02        | system:node:master02           |      |         |        | system:nodes                   |        |
| master03       | system:node:master03           |      |         |        | system:nodes                   |        |
| node01         | system:node:node01             |      |         |        | system:nodes                   |        |
| node02         | system:node:node02             |      |         |        | system:nodes                   |        |
| service accunt | service-accounts               |      |         |        | system:masters                 |        |
|                |                                |      |         |        |                                |        |

分发证书到对应的主机

```BASH
#client
for i in master{01..03} node{01,02};do echo "+----------${i}----------+";scp ca.pem  ${i}.pem ${i}-key.pem ${i}:/opt/kubernetes/ssl;done

#server
for i in master{01..03};do echo "+----------${i}----------+";scp ca*.pem kube-apiserver*.pem  service-account*.pem ${i}:/opt/kubernetes/ssl;done

#ETCD
for i in master{01..03};do echo "+----------${i}----------+";scp ca.pem etcd*.pem ${i}:/opt/etcd/ssl;done
```



# ETCD部署

systemd

```BASH
ETCD_IP=$(hostname -i)
ETCD_NAME=$(hostname -s)
ETCD_PATH="/opt/etcd/ssl/"

cat > /etc/systemd/system/etcd.service <<EOF
[Unit]
Description=ETCD Server
Documentation=https://github.com/coreos/etcd
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --name ${ETCD_NAME} \\
  --cert-file ${ETCD_PATH}etcd.pem \\
  --key-file ${ETCD_PATH}etcd-key.pem \\
  --peer-cert-file ${ETCD_PATH}etcd.pem \\
  --peer-key-file ${ETCD_PATH}etcd-key.pem \\
  --trusted-ca-file ${ETCD_PATH}ca.pem \\
  --peer-trusted-ca-file ${ETCD_PATH}ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --listen-client-urls https://${ETCD_IP}:2379 \\
  --advertise-client-urls https://${ETCD_IP}:2379 \\
  --listen-peer-urls https://${ETCD_IP}:2380 \\
  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\
  --initial-cluster master01=https://172.16.0.30:2380,master02=https://172.16.0.31:2380,master03=https://172.16.0.32:2380 \\
  --initial-cluster-token etcd_cluster \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd/default.etcd 
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
----------------------------------------------------------------
#客户端到服务器端的通信
--cert-file=<path>：用于与etcd 的SSL/TLS 连接的证书。设置此选项后，advertise-client-urls 可以    使用 HTTPS 架构
--key-file=<path>: 证书密钥。必须是未加密的
--client-cert-auth: 设置此选项后，etcd 将检查所有传入的 HTTPS 请求以获取由受信任的 CA 签署的客    户端证书，不提供有效客户端证书的请求将失败。如果启用了身份验证，则证书会为 Common Name          字段提供的用户名提供凭据
--trusted-ca-file=<path>: 受信任的证书颁发机构
--auto-tls：使用自动生成的自签名证书与客户端进行 TLS 连接
#对等通信（服务器到服务器，集群）
--peer-cert-file=<path>：用于对等方之间的 SSL/TLS 连接的证书。这将用于侦听对等地址以及向其他对    等方发送请求
--peer-key-file=<path>: 证书密钥。必须是未加密的
--peer-client-cert-auth：设置后，etcd 将检查来自集群的所有传入对等请求，以获取由提供的 CA 签署    的有效客户端证书
--peer-trusted-ca-file=<path>: 受信任的证书颁发机构
--peer-auto-tls：使用自动生成的自签名证书进行对等方之间的 TLS 连接

如果提供了客户端到服务器或对等证书，则还必须设置密钥。所有这些配置选项也可通过环境变量ETCD_CA_FILE，ETCD_PEER_CA_FILE等等。
--cipher-suites：服务器/客户端和对等点之间支持的 TLS 密码套件的逗号分隔列表（空将由 Go 自动填充）。可用于 v3.2.22+、v3.3.7+ 和 v3.4+。
----------------------------------------------------------------

[root@master01 ~]# systemctl daemon-reload && systemctl start etcd && systemctl enable etcd

[root@master01 ~]# for host in master{01..03} ;do ssh $host systemctl status etcd | grep Active ;done
   Active: active (running) since Sat 2021-07-03 23:04:38 CST; 11s ago
   Active: active (running) since Sat 2021-07-03 23:04:38 CST; 11s ago
   Active: active (running) since Sat 2021-07-03 23:04:39 CST; 11s ago


 [root@master01 ~]#  
 ETCDCTL_API=3 etcdctl \
  --endpoints https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \
  --cacert /opt/etcd/ssl/ca.pem \
  --cert /opt/etcd/ssl/etcd.pem \
  --key /opt/etcd/ssl/etcd-key.pem \
  endpoint health \
  --write-out=table
+--------------------------+--------+-------------+-------+
|         ENDPOINT         | HEALTH |    TOOK     | ERROR    |
+--------------------------+--------+-------------+-------+
| https://172.16.0.30:2379 |   true | 11.927472ms |          |
| https://172.16.0.32:2379 |   true | 13.521489ms |          |
| https://172.16.0.31:2379 |   true | 14.421469ms |          |
+--------------------------+--------+-------------+-------+
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://172.16.0.30:2379 \
  --cacert=/opt/etcd/ssl/ca.pem \
  --cert=/opt/etcd/ssl/etcd.pem \
  --key=/opt/etcd/ssl/etcd-key.pem \
    endpoint health \
  --write-out=table
  +------------------+---------+----------+--------------------------+--------------------------+------------+
|        ID                      | STATUS  |   NAME   |        PEER ADDRS        |       CLIENT ADDRS       | IS LEARNER |
+------------------+---------+----------+--------------------------+--------------------------+------------+
| 322eb3d030ebf559  | started | master01 | https://172.16.0.30:2380 | https://172.16.0.30:2379 |      false |
| 7015b46a66a16d1d | started | master03 | https://172.16.0.32:2380 | https://172.16.0.32:2379 |      false |
| d3fd314d63fa3eba   | started | master02  | https://172.16.0.31:2380 | https://172.16.0.31:2379 |      false |
+------------------+---------+----------+--------------------------+--------------------------+------------+

使用负载均衡的多节点 etcd 集群
要运行负载均衡的 etcd 集群：

建立一个 etcd 集群。
在 etcd 集群前面配置负载均衡器。例如，让负载均衡器的地址为 $LB。
使用参数 --etcd-servers=$LB:2379 启动 Kubernetes API 服务器
```

# Docker

```BASH
[root@master01 ~/tls]# docker-compose --version
docker-compose version 1.29.2, build 5becea4c
[root@master01 ~/tls]# docker --version
Docker version 20.10.7, build f0df350
```

全部主机

```BASH
{
cat > /etc/systemd/system/docker.service << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target
[Service]
Type=notify
ExecStart=/usr/local/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s
[Install]
WantedBy=multi-user.target
EOF


[root@master01 ~/tls]# systemctl daemon-reload && systemctl start docker && systemctl enable docker && systemctl status docker |grep Active
}
```

检查一下

```BASH
[root@master01 ~/tls]# sh ~/service_check.sh 
Kube-apiserver_Check......
+-------------------------------------------------------+
    master01  kube-apiserver is : Unit kube-apiserver.service could not be found.
    master01  kube-controller-manager is : Unit kube-controller-manager.service could not be found.
    master01  kube-scheduler is : Unit kube-scheduler.service could not be found.
    master02  kube-apiserver is : Unit kube-apiserver.service could not be found.
    master02  kube-controller-manager is : Unit kube-controller-manager.service could not be found.
    master02  kube-scheduler is : Unit kube-scheduler.service could not be found.
    master03  kube-apiserver is : Unit kube-apiserver.service could not be found.
    master03  kube-controller-manager is : Unit kube-controller-manager.service could not be found.
    master03  kube-scheduler is : Unit kube-scheduler.service could not be found.
+-------------------------------------------------------+
Etcd_Check......
+-------------------------------------------------------+
    master01  etcd is  |  running
    master02  etcd is  |  running
    master03  etcd is  |  running
+-------------------------------------------------------+
docker_Check......
+-------------------------------------------------------+
    master01  docker is  |  running
    master02  docker is  |  running
    master03  docker is  |  running
    node01  docker is  |  running
    node02  docker is  |  running
+-------------------------------------------------------+
Kube-proxy kubelet_Check......
+-------------------------------------------------------+
    master01  kube-proxy is : Unit kube-proxy.service could not be found.
    master01  kubelet is : Unit kubelet.service could not be found.
    master02  kube-proxy is : Unit kube-proxy.service could not be found.
    master02  kubelet is : Unit kubelet.service could not be found.
    master03  kube-proxy is : Unit kube-proxy.service could not be found.
    master03  kubelet is : Unit kubelet.service could not be found.
    node01  kube-proxy is : Unit kube-proxy.service could not be found.
    node01  kubelet is : Unit kubelet.service could not be found.
    node02  kube-proxy is : Unit kube-proxy.service could not be found.
    node02  kubelet is : Unit kubelet.service could not be found.
+-------------------------------------------------------+

```

问题

```BASH
sudo mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

systemctl daemon-reload && systemctl restart docker
```



# kubeconfig

### kubelet

```BASH
VIP="172.16.0.37"

for instance in master{01..03} node{01,02}; do
  kubectl config set-cluster kubernetes  \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${VIP}:6443 \
    --kubeconfig=/opt/kubernetes/cfg/${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=/opt/kubernetes/cfg/${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:node:${instance} \
    --kubeconfig=/opt/kubernetes/cfg/${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/${instance}.kubeconfig
done
```

### kube-proxy

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-proxy.kubeconfig"
VIP="172.16.0.37"

kubectl config set-cluster kubernetes  \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${VIP}:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-proxy \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
```

### kube-controller-manager

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-controller-manager.kubeconfig"
VIP="172.16.0.37"

kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${VIP}:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=kube-controller-manager.pem \
    --client-key=kube-controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

### kube-scheduler

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
VIP="172.16.0.37"

 kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${VIP}:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.pem \
    --client-key=kube-scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-scheduler \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

### admin

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/admin.kubeconfig"
VIP="172.16.0.37"

kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${VIP}:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=admin \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
```

```BASH
[root@master01 ~/tls]# cp /opt/kubernetes/cfg/admin.kubeconfig /root/.kube/config

```

# LB

![image-20210704231834003](C:\Users\goo\AppData\Roaming\Typora\typora-user-images\image-20210704231834003.png)

### 安装nginx 

```BASH

nginx stream 在1.19以上都需要手动加载--with-stream，所以使用源码编译

#3台都安装
[root@master01 /server/soft]# wget http://nginx.org/download/nginx-1.20.1.tar.gz
[root@master01 /server/soft]# tar xf nginx-1.20.1.tar.gz -C /opt/
[root@master01 /server/soft]# cd /opt/nginx-1.20.1/
[root@master01 /opt/nginx-1.20.1]# ls
auto  CHANGES  CHANGES.ru  conf  configure  contrib  html  LICENSE  man  README  src
[root@master01 /opt/nginx-1.20.1]# useradd -s /sbin/nologin -M nginx
[root@master01 /opt/nginx-1.20.1]# yum install -y pcre pcre-devel openssl openssl-devel gcc-c++
[root@master01 /opt/nginx-1.20.1]#./configure  \
--user=nginx \
--group=nginx \
--prefix=/usr/share/nginx \
--sbin-path=/usr/local/bin \
--conf-path=/etc/nginx/nginx.conf \
--error-log-path=/var/log/nginx/error.log \
--http-log-path=/var/log/nginx/access.log \
--pid-path=/var/run/nginx.pid \
--with-http_stub_status_module \
--with-http_ssl_module \
--with-stream 

[root@master01 /opt/nginx-1.20.1]#make && make install
[root@master01 /opt/nginx-1.20.1]# nginx -v
nginx version: nginx/1.20.1
[root@master01 /opt/nginx-1.20.1]#  ln -s /usr/share/nginx /opt/nginx

[root@master01 /opt/nginx-1.20.1]# yum install -y keepalived
[root@master01 ~]# vim /etc/nginx/nginx.conf
user  nginx;
worker_processes  auto;

include /etc/nginx/conf.d/k8s.conf
                                                                                                                                             
http {
..
注意修改http上面的

```

传到另外两台

```BASH
[root@master01 /server/soft]# for i in master{02,03};do scp -r nginx-1.20.1 $i:/opt;done
重复上面的组件安装，编译
[root@master02 /opt/nginx-1.20.1]# ln -s /usr/share/nginx /opt/nginx

#传NGINX文件
[root@master01 /server/soft]# for i in master{02,03};do scp -r /etc/nginx $i:/etc;done  

#keepalved
[root@master01 /server/soft]# for i in master{02,03};do scp -r /etc/keepalived $i:/etc;done

```

### 修改配置L4LB

```BASH
################################
[root@master01 ~/tls]# cat /etc/nginx/nginx.conf
...
events {
    worker_connections 1024;
}

include /etc/nginx/conf.d/k8s.conf;      #l4必须放在http标签外

http {
...
###################################

cat /etc/nginx/conf.d/k8s.conf 
# 四层负载均衡，为两台Master apiserver组件提供负载均衡
stream {
	log_format  main  '$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent';
    access_log  /var/log/nginx/k8s-access.log  main;

	upstream k8s-apiserver {
		server 172.16.0.30:6443;  
		server 172.16.0.31:6443;  
		server 172.16.0.32:6443;  
						   }
				    
	server {
		listen 16443; # 由于nginx与master节点复用，这个监听端口不能是6443，否则会冲突
		proxy_pass k8s-apiserver;
		   }
}

[root@master01 ~]# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful

开机启动
cat >/etc/systemd/system/nginx.service<<EOF 
[Unit]
Description=Nginx Server
Documentation=http://nginx.org
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=forking
PIDFile=/var/run/nginx/nginx.pid 
ExecStartPre=/usr/local/bin/nginx -t -c /etc/nginx/nginx.conf
ExecStart=/usr/local/bin/nginx -c /etc/nginx/nginx.conf
ExecReload=/usr/local/bin/nginx -s reload
ExecStop=/usr/local/bin/nginx  -s stop
PrivateTmp=true
[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload && systemctl start nginx && systemctl enable nginx




[root@master01 ~/tls]# cat /etc/keepalived/keepalived.conf 
! Configuration File for keepalived

global_defs {
   router_id NGINX_MASTER
}

#check_nginx
vrrp_script check_nginx {
		    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.0.37/24
    }

#track_script
    track_script {
		        check_nginx
				} 

}

[root@master01 ~/tls]# cat /etc/keepalived/check_nginx.sh 
#!/bin/bash
count=$(ps -ef|grep -c [n]ginx)
if [ "$count" -le 2 ];then
    systemctl stop keepalived
fi

chmod +x /etc/keepalived/check_nginx.sh

systemctl daemon-reload && systemctl start keepalived && systemctl enable keepalived

```



```BASH
[root@master01 ~/tls]# for i in master{02,03};do scp -r /etc/keepalived/ $i:/etc/ ;done
keepalived.conf                                                                                            100%  476   424.8KB/s   00:00    
check_nginx.sh                                                                                             100%   74    62.3KB/s   00:00    
keepalived.conf                                                                                            100%  476   716.3KB/s   00:00    
check_nginx.sh                                                                                             100%   74    75.6KB/s   00:00    
[root@master01 ~/tls]# for i in master{02,03};do scp -r /etc/nginx/nginx.conf $i:/etc/nginx/ ;done
nginx.conf                                                                                                 100% 2376     1.8MB/s   00:00    
nginx.conf                                                                                                 100% 2376     1.6MB/s   00:00    
[root@master01 ~/tls]# for i in master{02,03};do scp -r /etc/nginx/conf.d/ $i:/etc/nginx/ ;done
k8s.conf                                                                                                   100%  515   466.4KB/s   00:00    
k8s.conf                                                                                                   100%  515   352.3KB/s   00:00    


#master02
! Configuration File for keepalived

global_defs {
   router_id NGINX_BACKUP01
}

#check_nginx
vrrp_script check_nginx {
            script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state BACKUP01
    interface eth0
    virtual_router_id 51
    priority 125
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.0.37/24
    }
                                                                                                                                             
#track_script
    track_script {
                check_nginx
                }

}

#master03
[root@master03 ~]# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived

global_defs {
   router_id NGINX_BACKUP02
}

#check_nginx
vrrp_script check_nginx {
            script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state BACKUP02
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {                                                                                                                      
        172.16.0.37/24
    }

#track_script
    track_script {
                check_nginx
                }

}

```

补充

```BASH
    server {
        listen       80;
        server_name  172.16.0.37;

         location /healthz {
		     proxy_pass                https://127.0.0.1:6443/healthz;
			 proxy_ssl_trusted_certificate /opt/kubernetes/ssl/ca.pem;
		 
		 }

}

[root@master01 /opt/kubernetes/yaml]# curl -H "Host:172.16.0.37" -i http://127.0.0.1/healthz
HTTP/1.1 200 OK
Server: nginx/1.20.1
Date: Tue, 06 Jul 2021 15:34:22 GMT
Content-Type: text/plain; charset=utf-8
Content-Length: 2
Connection: keep-alive
Cache-Control: no-cache, private
X-Content-Type-Options: nosniff
X-Kubernetes-Pf-Flowschema-Uid: 53a10c41-c0b3-4610-b471-b6394e0bfd94
X-Kubernetes-Pf-Prioritylevel-Uid: e1606cc0-77bb-439a-ba76-a8287d922a82

```



```BASH
systemctl daemon-reload && systemctl start nginx keepalived &systemctl enable nginx keepalived

```

遇到的问题

```BASH
因为上面大意nginx.pid放一个不存在的目录，导致不能启动。
echo 'mkdir -p /var/run/nginx' >>/etc/rc.d/rc.local 
chmod +x /etc/rc.d/rc.local 

上面文档已修改正确
etcd 服务也不知道为啥全变成第一台的了。重新修改即可

```

```BASH
[root@master01 ~]# sh service_check.sh 
Nginx keepalived_Check......
+-------------------------------------------------------+
    master01  nginx is : running
    master01  keepalived is : running
    master02  nginx is : running
    master02  keepalived is : running
    master03  nginx is : running
    master03  keepalived is : running
+-------------------------------------------------------+
Kube-apiserver_Check......
+-------------------------------------------------------+
    master01  kube-apiserver is : Unit kube-apiserver.service could not be found.
    master01  kube-controller-manager is : Unit kube-controller-manager.service could not be found.
    master01  kube-scheduler is : Unit kube-scheduler.service could not be found.
    master02  kube-apiserver is : Unit kube-apiserver.service could not be found.
    master02  kube-controller-manager is : Unit kube-controller-manager.service could not be found.
    master02  kube-scheduler is : Unit kube-scheduler.service could not be found.
    master03  kube-apiserver is : Unit kube-apiserver.service could not be found.
    master03  kube-controller-manager is : Unit kube-controller-manager.service could not be found.
    master03  kube-scheduler is : Unit kube-scheduler.service could not be found.
+-------------------------------------------------------+
Etcd_Check......
+-------------------------------------------------------+
    master01  etcd is  |  running
    master02  etcd is  |  running
    master03  etcd is  |  running
+-------------------------------------------------------+
docker_Check......
+-------------------------------------------------------+
    master01  docker is  |  running
    master02  docker is  |  running
    master03  docker is  |  running
    node01  docker is  |  running
    node02  docker is  |  running
+-------------------------------------------------------+
Kube-proxy kubelet_Check......
+-------------------------------------------------------+
    master01  kube-proxy is : Unit kube-proxy.service could not be found.
    master01  kubelet is : Unit kubelet.service could not be found.
    master02  kube-proxy is : Unit kube-proxy.service could not be found.
    master02  kubelet is : Unit kubelet.service could not be found.
    master03  kube-proxy is : Unit kube-proxy.service could not be found.
    master03  kubelet is : Unit kubelet.service could not be found.
    node01  kube-proxy is : Unit kube-proxy.service could not be found.
    node01  kubelet is : Unit kubelet.service could not be found.
    node02  kube-proxy is : Unit kube-proxy.service could not be found.
    node02  kubelet is : Unit kubelet.service could not be found.
+-------------------------------------------------------+

    inet 172.16.0.37/24 scope global secondary eth0

```

经过多次测试是正常的，只要master01 nginx 故障就关闭keepalived实现VIP飘移。

# kubernetes

### 生成kubeconfig配置文件

#### kubelet

```BASH
KUBE_APISERVER="https://172.16.0.37:16443"

for i in master{01..03} node{01,02}; do
  kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config set-credentials system:node:${i} \
    --client-certificate=${i}.pem \
    --client-key=${i}-key.pem \
    --embed-certs=true \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:node:${i} \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig
done

#master01.kubeconfig  master02.kubeconfig  master03.kubeconfig  node01.kubeconfig  node02.kubeconfig

```

#### kube-proxy

```BASH
KUBE_APISERVER="https://172.16.0.37:16443"
KUBE_CONFIG="/opt/kubernetes/cfg/kube-proxy.kubeconfig"

kubectl config set-cluster kubernetes  \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes  \
    --user=system:kube-proxy \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
  
  #kube-proxy.kubeconfig
```

#### kube-controller-manager

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-controller-manager.kubeconfig"
kubectl config set-cluster kubernetes  \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=kube-controller-manager.pem \
    --client-key=kube-controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes  \
    --user=system:kube-controller-manager \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
  
  #kube-controller-manager.kubeconfig 
```

#### kube-scheduler

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.pem \
    --client-key=kube-scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-scheduler \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
  
  #kube-scheduler.kubeconfig
```

#### admin

```BASH
 KUBE_CONFIG="/opt/kubernetes/cfg/admin.kubeconfig"
 
 kubectl config set-cluster kubernetes  \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes  \
    --user=admin \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
  
  #admin.kubeconfig 
  
[root@master01 ~/tls]# cp /opt/kubernetes/cfg/admin.kubeconfig ~/.kube/config
[root@master01 ~/tls]# for i in master{01..03};do scp ~/.kube/config $i:~/.kube ;done
config                                                                                                     100% 6131     7.9MB/s   00:00    
config                                                                                                     100% 6131     2.9MB/s   00:00    
config                                                                                                     100% 6131     4.3MB/s   00:00    
```

#### 分发kubeconfig配置文件

```BASH
[root@master01 ~/tls]# for i in master{02,03} node{01,02};do scp /opt/kubernetes/cfg/{kube-proxy,$i}.kubeconfig $i:/opt/kubernetes/cfg ;done
kube-proxy.kubeconfig                      100% 6182    12.0MB/s   00:00    
master01.kubeconfig                        100% 6304     4.8MB/s   00:00    
kube-proxy.kubeconfig                     00% 6182     3.7MB/s   00:00    
master02.kubeconfig                        100% 6304     6.3MB/s   00:00    
kube-proxy.kubeconfig                     00% 6182     3.7MB/s   00:00    
master03.kubeconfig                        100% 6304     7.7MB/s   00:00    
kube-proxy.kubeconfig                     00% 6182     1.6MB/s   00:00    
node01.kubeconfig                          100% 6304     2.5MB/s   00:00    
kube-proxy.kubeconfig                     00% 6182     3.2MB/s   00:00    
node02.kubeconfig                          100% 6300   360.0KB/s   00:00    

[root@master01 ~/tls]# for i in master{02,03};do scp /opt/kubernetes/cfg/{kube-controller-manager,kube-scheduler}.kubeconfig $i:/opt/kubernetes/cfg ;done
kube-controller-manager.kubeconfig                                                                   100% 6261    12.0MB/s   00:00   
kube-scheduler.kubeconfig                                                                                  100% 6203     6.0MB/s   00:00    
kube-controller-manager.kubeconfig                                                                  100% 6261     3.5MB/s   00:00    
kube-scheduler.kubeconfig                                                                                  100% 6203     6.8MB/s   00:00    
kube-controller-manager.kubeconfig                                                                   100% 6261     4.7MB/s   00:00    
kube-scheduler.kubeconfig                                                                                  100% 6203     6.4MB/s   00:00    

```

#### 生成密钥文件

```BASH
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
```

```BASH
cat > /opt/kubernetes/ssl/encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF

[root@master01 ~/tls]# for i in master{02,03};do scp /opt/kubernetes/ssl/encryption-config.yaml $i:/opt/kubernetes/ssl ;done
encryption-config.yaml                                                                                     100%  240   165.0KB/s   00:00    
encryption-config.yaml                                                                                     100%  240   155.9KB/s   00:00    

```

### systemd配置及启动

#### kube-apiserver

```BASH
KUBE_APISERVER=$(hostname -i)


cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${KUBE_APISERVER} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/opt/etcd/ssl/ca.pem \\
  --etcd-certfile=/opt/etcd/ssl/etcd.pem \\
  --etcd-keyfile=/opt/etcd/ssl/etcd-key.pem \\
  --etcd-servers=https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/opt/kubernetes/ssl/encryption-config.yaml \\
  --kubelet-certificate-authority=/opt/kubernetes/ssl/ca.pem \\
  --kubelet-client-certificate=/opt/kubernetes/ssl/kube-apiserver.pem \\
  --kubelet-client-key=/opt/kubernetes/ssl/kube-apiserver-key.pem \\
  --runtime-config=api/all=true \\
  --service-account-key-file=/opt/kubernetes/ssl/service-account.pem \\
  --service-account-signing-key-file=/opt/kubernetes/ssl/service-account-key.pem \\
  --service-account-issuer=https://${KUBE_APISERVER}:6443 \\
  --service-cluster-ip-range=10.0.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/opt/kubernetes/ssl/kube-apiserver.pem \\
  --tls-private-key-file=/opt/kubernetes/ssl/kube-apiserver-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

```

```BASH
[root@master01 ~/tls]# systemctl daemon-reload && systemctl start kube-apiserver && systemctl enable kube-apiserver

```

问题

```BASH
controlbuf.go:508] transport: loopyWriter.run returning. connection error: desc = "transport is closing"
这个是正常的日志

 authentication.go:63] "Unable to authenticate the request" err="invalid bearer token"
 
```

#### kube-controller-manager

```BASH
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.244.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\
  --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --service-account-private-key-file=/opt/kubernetes/ssl/service-account-key.pem \\
  --service-cluster-ip-range=10.0.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

启动

```BASH
[root@master01 ~/tls]# systemctl daemon-reload && systemctl start kube-controller-manager && systemctl enable kube-controller-manager

```



问题

```BASH
```

#### kube-scheduler

```BASH
cat <<EOF | sudo tee /opt/kubernetes/cfg/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/opt/kubernetes/cfg/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

```

启动

```BASH
[root@master01 ~/tls]# systemctl daemon-reload && systemctl start kube-scheduler && systemctl enable kube-scheduler

```

问题

```BASH
```

LB

```BASH
[root@master01 ~/tls]# curl --cacert /opt/kubernetes/ssl/ca.pem -i https://172.16.0.37:16443/version
HTTP/1.1 200 OK
Cache-Control: no-cache, private
Content-Type: application/json
X-Kubernetes-Pf-Flowschema-Uid: bad87ab6-d379-4d10-8578-9acfb9aa5a97
X-Kubernetes-Pf-Prioritylevel-Uid: 94cdbf65-f866-41c9-9403-6fa733ad06e9
Date: Mon, 05 Jul 2021 14:29:48 GMT
Content-Length: 263

{
  "major": "1",
  "minor": "21",
  "gitVersion": "v1.21.2",
  "gitCommit": "092fbfbf53427de67cac1e9fa54aaa09a28371d7",
  "gitTreeState": "clean",
  "buildDate": "2021-06-16T12:53:14Z",
  "goVersion": "go1.16.5",
  "compiler": "gc",
  "platform": "linux/amd64"

有其他问题参考：https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md
```

#### RBAC

```BASH
任意一台master操作
#创建system:kube-apiserver-to-kubelet ClusterRole有权限访问Kubelet API，并执行与管理相关的pod最常见的任务

cat <<EOF | kubectl apply --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF

#The Kubernetes API Server authenticates to the Kubelet as the kubernetes user using the client certificate as defined by the --kubelet-client-certificate flag.
#Bind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user:

cat <<EOF | kubectl apply --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```





## Node

创建CNI

```BASH
cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.4.0",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "10.0.0.0/24"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF


cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.4.0",
    "name": "lo",
    "type": "loopback"
}
EOF
```



### kubelet

```BASH
echo 'mkdir -p /run/systemd/resolve &&  ln -s /etc/resolv.conf /run/systemd/resolve/ '  >>/etc/rc.d/rc.local	
chmod +x /etc/rc.d/rc.local 

cat <<EOF | sudo tee /opt/kubernetes/cfg/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/opt/kubernetes/ssl/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.0.0.2"
podCIDR: "10.0.0.0/24"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/opt/kubernetes/ssl/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/opt/kubernetes/ssl/${HOSTNAME}-key.pem"
EOF

--------------------------------------------------------------------------------------

KUBELET_IP=$(hostname -s)

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/opt/kubernetes/cfg/kubelet-config.yaml \\
  --container-runtime=docker \\
  --container-runtime-endpoint=unix:///var/run/dockershim.sock   \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/opt/kubernetes/cfg/${KUBELET_IP}.kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --cgroup-driver=systemd \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


unix:///var/run/dockershim.sock    docker
unix:///var/run/containerd/containerd.sock   containerd
```

启动

```BASH
systemctl daemon-reload && systemctl start kubelet && systemctl enable kubelet

systemctl daemon-reload && systemctl restart kubelet

```

问题

```BASH
[root@master01 /opt/kubernetes/yaml]# for i in `kubectl get pod -n kube-system |awk 'NR>1 {print $1}'`;do kubectl logs $i -n kube-system ;done



```





### kube-proxy

```BASH
cat <<EOF | sudo tee /opt/kubernetes/cfg/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/opt/kubernetes/cfg/kube-proxy.kubeconfig"
mode: "iptables"
clusterCIDR: "10.244.0.0/16"
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/opt/kubernetes/cfg/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

启动

```BASH
systemctl daemon-reload && systemctl start kube-proxy && systemctl enable kube-proxy

```

问题

```BASH
```



### 检查

```BASH
[root@master01 ~]# sh service_check.sh 
Nginx keepalived_Check......
+-------------------------------------------------------+
    master01  nginx is : running
    master01  keepalived is : running
    master02  nginx is : running
    master02  keepalived is : running
    master03  nginx is : running
    master03  keepalived is : running
+-------------------------------------------------------+
Kube-apiserver_Check......
+-------------------------------------------------------+
    master01  kube-apiserver is : running
    master01  kube-controller-manager is : running
    master01  kube-scheduler is : running
    master02  kube-apiserver is : running
    master02  kube-controller-manager is : running
    master02  kube-scheduler is : running
    master03  kube-apiserver is : running
    master03  kube-controller-manager is : running
    master03  kube-scheduler is : running
+-------------------------------------------------------+
Etcd_Check......
+-------------------------------------------------------+
    master01  etcd is  |  running
    master02  etcd is  |  running
    master03  etcd is  |  running
+-------------------------------------------------------+
docker_Check......
+-------------------------------------------------------+
    master01  docker is  |  running
    master02  docker is  |  running
    master03  docker is  |  running
    node01  docker is  |  running
    node02  docker is  |  running
+-------------------------------------------------------+
Kube-proxy kubelet_Check......
+-------------------------------------------------------+
    master01  kube-proxy is : running
    master01  kubelet is : running
    master02  kube-proxy is : running
    master02  kubelet is : running
    master03  kube-proxy is : running
    master03  kubelet is : running
    node01  kube-proxy is : running
    node01  kubelet is : running
    node02  kube-proxy is : running
    node02  kubelet is : running
+-------------------------------------------------------+
[root@master01 ~]# kubectl get node
NAME       STATUS     ROLES    AGE   VERSION
master01   Ready      <none>   17s   v1.21.2
master02   NotReady   <none>   17s   v1.21.2
master03   NotReady   <none>   17s   v1.21.2
node01     NotReady   <none>   17s   v1.21.2
node02     NotReady   <none>   17s   v1.21.2

[root@master03 ~]# ls /opt/cni/bin/
[root@master01 ~]# for i in master{02,03} node{01,02};do scp -r /opt/cni/ $i:/opt/ ;done
[root@master03 ~]# ls /opt/cni/bin/
bandwidth  bridge  dhcp  firewall  flannel  host-device  host-local  ipvlan  loopback  macvlan  portmap  ptp  sbr  static  tuning  vlan  vrf

[root@master03 ~]# kubectl get node
NAME       STATUS   ROLES    AGE     VERSION
master01   Ready    <none>   3m31s   v1.21.2
master02   Ready    <none>   3m31s   v1.21.2
master03   Ready    <none>   3m31s   v1.21.2
node01     Ready    <none>   3m31s   v1.21.2
node02     Ready    <none>   3m31s   v1.21.2

[root@master02 ~]# kubectl version
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.2", GitCommit:"092fbfbf53427de67cac1e9fa54aaa09a28371d7", GitTreeState:"clean", BuildDate:"2021-06-16T12:59:11Z", GoVersion:"go1.16.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.2", GitCommit:"092fbfbf53427de67cac1e9fa54aaa09a28371d7", GitTreeState:"clean", BuildDate:"2021-06-16T12:53:14Z", GoVersion:"go1.16.5", Compiler:"gc", Platform:"linux/amd64"}

```

apiserver

```BASH
[root@master01 ~]# grep "172.16.0.37" /opt/kubernetes/cfg/*
/opt/kubernetes/cfg/kube-proxy.kubeconfig:    server: https://172.16.0.37:16443
/opt/kubernetes/cfg/master01.kubeconfig:    server: https://172.16.0.37:16443
[root@master01 ~]# grep "127.0.0.1" /opt/kubernetes/cfg/*
/opt/kubernetes/cfg/admin.kubeconfig:    server: https://127.0.0.1:6443
/opt/kubernetes/cfg/kube-controller-manager.kubeconfig:    server: https://127.0.0.1:6443
/opt/kubernetes/cfg/kube-scheduler.kubeconfig:    server: https://127.0.0.1:6443

[root@master02 ~]# grep "127.0.0.1" /opt/kubernetes/cfg/*
/opt/kubernetes/cfg/admin.kubeconfig:    server: https://127.0.0.1:6443
/opt/kubernetes/cfg/kube-controller-manager.kubeconfig:    server: https://127.0.0.1:6443
/opt/kubernetes/cfg/kube-scheduler.kubeconfig:    server: https://127.0.0.1:6443

```

从上面可以看到kubelet kube-proxy 的地址是LB的VIP地址，也就是说其中一台master 下线了，就会被另外两台接管。keepalived保持了高可用。



### 源码构建pause

这里还会出现 pause拉取失败的问题，国外镜像
  --pod-infra-container-image=louwy001/pause:3.4.1  \\
  在kubelet 配置文件中加上这个

怎么去寻找pause 镜像

```BASH
[root@master01 /opt/kubernetes/yaml]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.21.2
k8s.gcr.io/kube-controller-manager:v1.21.2
k8s.gcr.io/kube-scheduler:v1.21.2
k8s.gcr.io/kube-proxy:v1.21.2
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0

先确定匹配的版本
[root@master01 /opt/kubernetes/yaml]# docker search pause:3.4.1
NAME             DESCRIPTION               STARS     OFFICIAL   AUTOMATED
louwy001/pause   k8s.gcr.io/pause:3.4.1    0                    
ninokop/pause    k8s.gcr.io/pause:3.4.1    0                    

第三方的不知道安全怎么样。官网的很难下载到
```



```BASH
从github git源码
[root@master01 /server/soft]# mkdir k8s
[root@master01 /server/soft]# cd k8s/
[root@master01 /server/soft/k8s]# git clone https://github.com/kubernetes/kubernetes
Cloning into 'kubernetes'...
remote: Enumerating objects: 1247439, done.
remote: Counting objects: 100% (206/206), done.
remote: Compressing objects: 100% (141/141), done.
remote: Total 1247439 (delta 89), reused 72 (delta 65), pack-reused 1247233
Receiving objects: 100% (1247439/1247439), 759.96 MiB | 10.87 MiB/s, done.
Resolving deltas: 100% (898364/898364), done.

操作系统，编译器确认
[root@master01 /server/soft/k8s]# which cc
/usr/bin/cc
[root@master01 /server/soft/k8s]# gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[root@master01 /server/soft/k8s]# cat /etc/redhat-release 
CentOS Linux release 7.9.2009 (Core)

编译源码和makefile确认
[root@master01 /server/soft/k8s]# cd kubernetes/build/pause
[root@master01 /server/soft/k8s/kubernetes/build/pause]#mkdir bin
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]#cd linux
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# ls
orphan.c  pause.c

使用make pause执行编译
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# make pause 
cc     pause.c   -o pause
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# gcc -Os -Wall -Werror -static -o pause pause.c
/usr/bin/ld: cannot find -lc
collect2: error: ld returned 1 exit status
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# yum install glibc-static -y
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# gcc -Os -Wall -Werror -static -o pause pause.c
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]#ls -hl
total 884K
-rw-r--r-- 1 root root 1014 Jul  6 10:38 orphan.c
-rwxr-xr-x 1 root root 875K Jul  6 15:08 pause
-rw-r--r-- 1 root root 1.8K Jul  6 10:38 pause.c
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]#file pause
pause: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.32, BuildID[sha1]=4522396d14e766d204882c9c2646ab4d1bbbefe2, not stripped
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]#nm pause
00000000004211f0 T abort
00000000006c7860 B __abort_msg
0000000000456850 W access
0000000000456850 T __access
0000000000497a60 t add_fdes
00000000004653f0 t add_module.isra.1
0000000000459ee0 t add_name_to_object.isra.2
.......
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# strip pause
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# ls -lh pause
-rwxr-xr-x 1 root root 801K Jul  6 15:10 pause
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# file pause
pause: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.32, BuildID[sha1]=4522396d14e766d204882c9c2646ab4d1bbbefe2, stripped
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# nm pause
nm: pause: no symbols
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# cp pause ../bin/pause-adm64
[root@master01 /server/soft/k8s/kubernetes/build/pause/linux]# cd ..
[root@master01 /server/soft/k8s/kubernetes/build/pause]# ls
bin  CHANGELOG.md  cloudbuild.yaml  Dockerfile  Dockerfile_windows  linux  Makefile  OWNERS  windows
[root@master01 /server/soft/k8s/kubernetes/build/pause]# ls bin/pause-adm64 
bin/pause-adm64
------------------------------------------------------------------------------------------------
strip
通过上面的对比，可以看出strip后，pause文件由875K瘦身到801K。strip执行前后，不改变程序的执行能力。在开发过程中，strip用于产品的发布，调试均用未strip的程序。

file
通过file命令可以看到pause的strip状态

nm
通过nm命令，可以看到strip后的pause文件没有符号信息
---------------------------------------------------------------------------------------------------------

确认dockerfile
[root@master01 /server/soft/k8s/kubernetes/build/pause]# cat Dockerfile
# Copyright 2016 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ARG BASE
FROM ${BASE}
ARG ARCH
ADD bin/pause-linux-${ARCH} /pause
USER 65535:65535
ENTRYPOINT ["/pause"]  


构建镜像
docker build --build-arg BASE=scratch --build-arg ARCH=amd64 -t pause:latest .

修改为
FROM scratch
ARG ARCH
ADD bin/pause-${ARCH} /pause
USER 65535:65535
ENTRYPOINT ["/pause"]  


[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker build --build-arg ARCH=adm64 -t pause:latest .   
## docker build --build-arg ARCH=adm64 -t  k8s.gcr.io/pause:3.5
Sending build context to Docker daemon  1.669MB
Step 1/5 : FROM scratch
 ---> 
Step 2/5 : ARG ARCH
 ---> Using cache
 ---> 431e91b54fef
Step 3/5 : ADD bin/pause-${ARCH} /pause
 ---> 7114bedd662c
Step 4/5 : USER 65535:65535
 ---> Running in f1f35038e1b9
Removing intermediate container f1f35038e1b9
 ---> 33674ed72b0f
Step 5/5 : ENTRYPOINT ["/pause"]
 ---> Running in fa17d4966714
Removing intermediate container fa17d4966714
 ---> 4a7c59910f9c
Successfully built 4a7c59910f9c
Successfully tagged pause:latest
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker images
REPOSITORY   TAG       IMAGE ID       CREATED         SIZE
pause        latest    4a7c59910f9c   8 seconds ago   819kB
#
k8s.gcr.io/pause   3.5    （可以改进的地方）


```

测试pause

```BASH
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker images
REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
pause        latest    4a7c59910f9c   16 minutes ago   819kB
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker run -itd --name pause pause:latest
bf847b1d6d7e5748aa7066555fd25b8bdd588b8dfee1a1112559d8310a5c2ed3
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker ps
CONTAINER ID   IMAGE          COMMAND    CREATED         STATUS         PORTS     NAMES
bf847b1d6d7e   pause:latest   "/pause"   6 seconds ago   Up 5 seconds             pause

[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker run -itd --name busybox --net=container:pause --pid=container:pause --ipc=container:pause busybox:latest
Unable to find image 'busybox:latest' locally
latest: Pulling from library/busybox
b71f96345d44: Pull complete 
Digest: sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d
Status: Downloaded newer image for busybox:latest
8d508f855eaadc71472b82fcae9ce00a089f926d4be9d4132f9e4b594df1e9a7
docker: Error response from daemon: can't join IPC of container bf847b1d6d7e5748aa7066555fd25b8bdd588b8dfee1a1112559d8310a5c2ed3: non-shareable IPC (hint: use IpcMode:shareable for the donor container).
这里出现了错误IPC模式，应该是docker版本原因导致的
https://blog.csdn.net/qq_38592816/article/details/103107584

[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker run -itd --name busybox2 --net=container:pause --pid=container:pause  busybox:latest
bf2757ffc728789f986a4ee35e77b2eeb1f932f13aac350c8fab8a167c081bd8
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker ps -a
CONTAINER ID   IMAGE            COMMAND    CREATED         STATUS         PORTS     NAMES
bf2757ffc728   busybox:latest   "sh"       4 minutes ago   Up 4 minutes             busybox2
07d48fcef7fd   busybox          "sh"       5 minutes ago   Created                  busybox1
8d508f855eaa   busybox:latest   "sh"       5 minutes ago   Created                  busybox
bf847b1d6d7e   pause:latest     "/pause"   7 minutes ago   Up 7 minutes             pause
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker exec -it bf2757ffc728 /bin/sh
/ # ps aux
PID   USER     TIME  COMMAND
    1 65535     0:00 /pause      #进程是1目的达到
    7 root      0:00 sh
   13 root      0:00 /bin/sh
   19 root      0:00 ps aux
/ # 

[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker rm -f `docker ps -a -q`
bf2757ffc728
07d48fcef7fd
8d508f855eaa
bf847b1d6d7e

```

### pause 下载及上传

```BASH
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker tag 4a7c59910f9c k8s.gcr.io/pause:latest 
[root@master01 /server/soft/k8s/kubernetes/build/pause]# docker images
REPOSITORY         TAG       IMAGE ID       CREATED          SIZE
pause              latest    4a7c59910f9c   33 minutes ago   819kB
k8s.gcr.io/pause   latest    4a7c59910f9c   33 minutes ago   819kB

[root@master01 ~]# docker save -o k8s.gcr.io_pause_latest.tar k8s.gcr.io/pause 
[root@master01 ~]# ls
anaconda-ks.cfg  k8s.gcr.io_pause_latest.tar  service_check.sh  tls  uuid.sh

[root@master01 ~]# for i in master{01..03} node{01,02};do scp k8s.gcr.io_pause_latest.tar $i:~ ;done
k8s.gcr.io_pause_latest.tar                                          100%  812KB  98.7MB/s   00:00    
k8s.gcr.io_pause_latest.tar                                          100%  812KB  30.9MB/s   00:00    
k8s.gcr.io_pause_latest.tar                                          100%  812KB  37.9MB/s   00:00    
k8s.gcr.io_pause_latest.tar                                          100%  812KB  44.7MB/s   00:00    
k8s.gcr.io_pause_latest.tar                                          100%  812KB  43.4MB/s   00:00    

[root@master01 ~]# docker image rm -f `docker images -a -q`
[root@master01 ~]# docker load <k8s.gcr.io_pause_latest.tar 
2da9eb5d5fbb: Loading layer [==================================================>]  821.2kB/821.2kB
Loaded image: k8s.gcr.io/pause:latest
[root@master01 ~]# docker images
REPOSITORY         TAG       IMAGE ID       CREATED             SIZE
k8s.gcr.io/pause   latest    4a7c59910f9c   About an hour ago   819kB
#latest 是3.5版本

```



### calico

```BASH
[root@master01 ~]# mkdir /opt/kubernetes/yaml
[root@master01 ~]# cd /opt/kubernetes/yaml/
[root@master01 /opt/kubernetes/yaml]# curl https://docs.projectcalico.org/manifests/calico.yaml -O
[root@master01 /opt/kubernetes/yaml]# kubectl apply -f calico.yaml
[root@master01 /opt/kubernetes/yaml]# kubectl get pod -n kube-system -o wide
NAME                                       READY   STATUS              RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
calico-kube-controllers-78d6f96c7b-4llfp   0/1     ContainerCreating   0          25s   <none>        node01     <none>           <none>
calico-node-4gxth                          0/1     Init:0/3            0          26s   172.16.0.36   node02     <none>           <none>
calico-node-5drkk                          0/1     Init:0/3            0          25s   172.16.0.30   master01   <none>           <none>
calico-node-6q5fd                          0/1     Init:0/3            0          26s   172.16.0.31   master02   <none>           <none>
calico-node-9glkn                          0/1     Init:0/3            0          26s   172.16.0.35   node01     <none>           <none>
calico-node-b56ss                          0/1     Init:0/3            0          26s   172.16.0.32   master03   <none>           <none>

 Failed to create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory

#重启之后会创建

```

```BASH
哈哈我们还是应该改成匹配的版本
[root@master01 /opt/kubernetes/yaml]# docker tag 4a7c59910f9c k8s.gcr.io/pause:3.4.1
[root@master01 /opt/kubernetes/yaml]# docker images
REPOSITORY         TAG       IMAGE ID       CREATED             SIZE
k8s.gcr.io/pause   3.4.1     4a7c59910f9c   About an hour ago   819kB
k8s.gcr.io/pause   latest    4a7c59910f9c   About an hour ago   819kB

批量删除问题容器
[root@master01 /opt/kubernetes/yaml]# kubectl delete pod `kubectl get pod -n kube-system -o wide |grep -v Running|awk 'NR>1 {print $1}'` -n kube-system 
pod "calico-node-9n27r" deleted
pod "calico-node-k4jzk" deleted
pod "calico-node-pshp8" deleted
pod "calico-node-qk8p4" deleted

[root@master01 ~]# kubectl logs  calico-node-qxfx9 -n kube-system 
Error from server: Get "https://node01:10250/containerLogs/kube-system/calico-node-qxfx9/calico-node": x509: certificate is not valid for any names, but wanted to match node01
#授权问题
plugin/kubernetes: the server has asked for the client to provide credentials

批量查看容器日志
[root@master01 /opt/kubernetes/yaml]# for i in `kubectl get pod -n kube-system | awk 'NR>1 {print $1}'` ;do echo "+------------------$i----------------------------------+";kubectl logs $i -n kube-system ;done

rm -f  /etc/cni/net.d/calico-kubeconfig 
[root@harbor ~/tls/kubernetes]# cat > calico-csr.json <<EOF
{
  "CN": "calicon-cni",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "L": "kunming",
      "O": "system:nodes",
      "OU": "System",
      "ST": "yunnan"
    }
  ]
}
EOF

[root@harbor ~/tls/kubernetes]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  calico-csr.json | cfssljson -bare calico
  
[root@harbor ~/tls/kubernetes]# ls calico*
calico.csr  calico-csr.json  calico-key.pem  calico.pem

我们为 CNI 插件创建一个 kubeconfig 文件，用于访问 Kubernetes。
#生成配置文件
#KUBERNETES_IP="https://172.16.0.160:6443"
APISERVER=$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}')
KUBERNETES_CONFIG="/opt/kubernetes/cfg/calico-kubeconfig"

kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${APISERVER} \
    --kubeconfig=${KUBERNETES_CONFIG}

kubectl config set-credentials calico-cni \
    --client-certificate=calico.pem\
    --client-key=calico-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBERNETES_CONFIG}

kubectl config set-context default \
    --cluster=kubernetes \
    --user=calico-cni \
    --kubeconfig=${KUBERNETES_CONFIG}

kubectl config use-context default --kubeconfig=${KUBERNETES_CONFIG}


[root@master01 ~/tls]# cp /opt/kubernetes/cfg/calico.kubeconfig /etc/cni/net.d/calico-kubeconfig
[root@master01 ~/tls]# for i in master{02,03} node{01,02};do scp /etc/cni/net.d/calico-kubeconfig $i:/etc/cni/net.d ;done
calico-kubeconfig                                     100% 8696     5.9MB/s   00:00    
calico-kubeconfig                                     100% 8696     7.9MB/s   00:00    
calico-kubeconfig                                     100% 8696     6.9MB/s   00:00    
calico-kubeconfig                                     100% 8696     8.4MB/s   00:00

宿主机和容器内时间不一致问题！
3742       volumes:
3743         # Used by calico-node.
3744         - name: date-config
3745           hostPath: 
3746             path: /etc/localtime   


```

其实基本可以了解几个情况，calico需要配置。时间不同步，生成的kubeconfig密钥不对IP不对，还有一个问题就是存储不了。。。是否换etcd的。etcd的怎么搞？





### coredns

```BASH
Error from server: error dialing backend: x509: certificate is not valid for any names, but wanted to match node01


[root@node01 ~]# docker exec -it k8s_busybox_busybox_default_df95ae60-04bf-4957-b3cb-7d0be7927d60_0 /bin/sh
/ # nslookup kubernetes
Server:    10.0.0.2
Address 1: 10.0.0.2

nslookup: can't resolve 'kubernetes'
/ # exit


kubectl run busybox --image=busybox:latest --command -- sleep 3600
kubectl get pods -l run=busybox

POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
kubectl exec -ti $POD_NAME -- nslookup kubernetes

```

问题

https://kubernetes.io/zh/docs/tasks/administer-cluster/dns-debugging-resolution/

coredns pods 有 CrashLoopBackOff 或者 Error 状态
如果有些节点运行的是旧版本的 Docker，同时启用了 SELinux，你或许会遇到 coredns pods 无法启动的情况。 要解决此问题，你可以尝试以下选项之一：

升级到 Docker 的较新版本。

禁用 SELinux.

修改 coredns 部署以设置 allowPrivilegeEscalation 为 true：

```bash
kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
```

CoreDNS 处于 CrashLoopBackOff 时的另一个原因是当 Kubernetes 中部署的 CoreDNS Pod 检测 到环路时。有许多解决方法 可以避免在每次 CoreDNS 监测到循环并退出时，Kubernetes 尝试重启 CoreDNS Pod 的情况。

**警告： 禁用 SELinux 或设置 allowPrivilegeEscalation 为 true 可能会损害集群的安全性**

官方镜像：https://hub.docker.com/r/coredns/coredns/tags?page=1&ordering=last_updated

```bash
[ERROR] plugin/errors: 2 9033357495637921255.4493923830098969119. HINFO: plugin/loop: no next plugin found
INFO] plugin/ready: Still waiting on: "kubernetes"


#kubectl describe pod coredns-865df68d57-bq76k -n kube-system 
 Warning  Unhealthy  8s (x16 over 2m36s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 503



解析延时
https://github.com/kelseyhightower/kubernetes-the-hard-way/issues/561
modprobe br-netfilter
sysctl -w net.bridge.bridge-nf-call-iptables=1
```







修改lB IP

```BASH
grep "127.0.0.1" /opt/kubernetes/cfg/*
sed -i "s/127.0.0.1:6443/172.16.0.37:16443/g" /opt/kubernetes/cfg/*
grep "172.16.0.37" /opt/kubernetes/cfg/*

```





### ETCD-calico

https://docs.projectcalico.org/getting-started/kubernetes/hardway/

```bash
#Calico 用于政策和网络
curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -O

#Calico 用于策略，flannel 用于网络
curl https://docs.projectcalico.org/manifests/canal.yaml -O
RBAC
如果在启用RBAC的群集上部署Calico，则应首先应用ClusterRole和ClusterRoleBinding规范：
```

calico 走了很多弯路来自对json语法问题。。。

https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-etcd-datastore

二进制的方式直接使用etcd存储来做。上面艰难方式也是可以做的。就是生成证书这些比较麻烦一些

第一部分修改的地方

```BASH
---                                                                                                                                                        
# Source: calico/templates/calico-etcd-secrets.yaml
# The following contains k8s Secrets for use with a TLS enabled etcd cluster.
# For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: calico-etcd-secrets
  namespace: kube-system
data:
  # Populate the following with etcd TLS configuration if desired, but leave blank if
  # not using TLS for etcd.
  # The keys below should be uncommented and the values populated with the base64
  # encoded contents of each file that would be associated with the TLS data.
  # Example command for encoding a file contents: cat <file> | base64 -w 0
  etcd-key: 
  etcd-cert: 
  etcd-ca: 
  #这里使用base64加密 ca.pem etcd.pem etcd-key.pem
  cat <file> | base64 -w 0
  然后把得到的密钥填写到这里就可以了
---

第二个地方
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  # Configure this with the location of your etcd cluster.
  #这里一定要注意缩进修改之后格式会变要注意
  etcd_endpoints: "https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379"
  # If you're using TLS enabled etcd uncomment the following.
  # You must also populate the Secret below with these files.
  etcd_ca: "/calico-secrets/etcd-ca"   # "/calico-secrets/etcd-ca"
  etcd_cert: "/calico-secrets/etcd-cert" # "/calico-secrets/etcd-cert"
  etcd_key: "/calico-secrets/etcd-key"  # "/calico-secrets/etcd-key"


第三个地方
            # - name: CALICO_IPV4POOL_CIDR
            #   value: "192.168.0.0/16"      
            可以改成cluster CIDR 10.244.0.0/16
            
```





查看集群角色

```BASH
[root@master01 ~]# kubectl -n kube-system get ClusterRoleBinding -o wide

```

kubectl run -it --rm dns-test --image=busybox:1.28.4  sh

