kubernetes 踩坑实录，手撕证书和组件里面涵盖系统O 和CN注意不能修改

## 准备

```BASH
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system  # 生效

[root@master01 ~/yml]# yum install -y bash-completion
[root@master01 ~/yml]# echo 'source <(kubectl completion bash)' >>~/.bashrc 
source <(kubectl completion bash)

sed -ri 's/.*swap.*/#&/' /etc/fstab   

如果遇到问题第一步先看日志：/var/log/message 或 journalctl -u etcd
```

互信

```bash
[root@harbor ~]# ssh-keygen -t rsa
[root@harbor ~]# ssh-copy-id root@172.16.0.160
[root@harbor ~]# ssh-copy-id root@172.16.0.161
[root@harbor ~]# ssh-copy-id root@172.16.0.165
[root@harbor ~]# ssh-copy-id root@172.16.0.166
[root@harbor ~]# ssh-copy-id root@172.16.0.170
```



## 一览表

| 主机         | IP                 | 组件                                                         |
| ------------ | ------------------ | ------------------------------------------------------------ |
| **master01** | 172.16.0.160       | apiserver、controller-manager、scheduler、etcd、nginx、keepalived、docker、cfssl |
| **master02** | 172.16.0.161       | apiserver、controller-manager、scheduler、etcd、nginx、keepalived、docker |
| **node01**   | 172.16.0.165       | kubelet、proxy、docker                                       |
| **node02**   | 172.16.0.166       | kubelet、proxy、docker                                       |
| ***node03*** | ***172.16.0.167*** | ***kubelet、proxy、docker***                                 |
| **harbor**   | 172.16.0.170       | harbor、docker                                               |

一键vmware环境

```bash
[root@master01 ~]# cat uuid.sh 
#!/bin/bash
#uuid  ip
path_eth0="/etc/sysconfig/network-scripts/ifcfg-eth0"
sed -i "/UUID/c UUID=$(uuidgen)" $path_eth0
sed -i "s/$1/$2/g" $path_eth0
echo "$3" >/etc/hostname
systemctl stop firewalld && systemctl disable firewalld
sed -i "s/SELINUX=.*/SELINUX=disabled/g" /etc/selinux/config
\cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
systemctl enable chronyd
yum install -y vim wget net-tools bash-completion tree nmap dos2unix lrzsz nc lsof tcpdump htop iftop iotop sysstat nethogs bind-utils
reboot

```

好了现在用另外的方法部署一次二进制K8S不知道有多少坑要踩

## 安装bind 内网DNS服务

```BASH
[root@harbor ~]# yum install bind -y
[root@harbor ~]# vim  /etc/named.conf 
options {
//	listen-on port 53 { 127.0.0.1; };	
	listen-on port 53 { 172.16.0.170; };
//	listen-on-v6 port 53 { ::1; };
	directory 	"/var/named";
	dump-file 	"/var/named/data/cache_dump.db";
	statistics-file "/var/named/data/named_stats.txt";
	memstatistics-file "/var/named/data/named_mem_stats.txt";
	recursing-file  "/var/named/data/named.recursing";
	secroots-file   "/var/named/data/named.secroots";
//	allow-query     { localhost; };
	allow-query     { any; };

    forwarders      {172.16.0.1;};  #上级DNS也就是路由地址
	recursion yes;          #递归方式

	dnssec-enable no;
	dnssec-validation no;
//this is value is yes
[root@harbor ~]# named-checkconf  #检查配置
```

配置区域文件

```BASH
[root@harbor ~]# vim /etc/named.rfc1912.zones 
#添加下面两个空间域名
zone "host.com" IN {
     type master;
     file "host.com.zone";
     allow-update {172.16.0.170;}; 
};

zone "test.com" IN {
     type master;
     file "test.com.zone";
     allow-update { 172.16.0.170;};
} ;                                    
```

配置主机文件

```bash
[root@harbor ~]# cat >/var/named/host.com.zone <<EOF
$ORIGIN host.com.
$TTL 600 ;10 minutes #过期时间10分钟
@        IN  SOA dns.host.com. dnsadmin.host.com (
         2021062101 ;serial
		 10800      ;refresh (3 hours)
		 900        ;retry (15 minutes)
		 604800     ;expire (1 week)
		 86400      ;minimum (1 day)
		 )
        NS         dns.host.com.
$TTL 60 ; 1 minute
dns          A      172.16.0.170
harbor       A      172.16.0.170
master01     A      172.16.0.160
master02     A      172.16.0.161
node01       A      172.16.0.165
node02       A      172.16.0.166
EOF
```

配置业务文件

```BASH
[root@harbor ~]# cat >/var/named/test.com.zone<<EOF
$ORIGIN test.com.
$TTL 600  ; 10 minutes
@       IN SOA  dns.test.com. dnsadmin.test.com. (
        2020010501 ; serial
        10800      ; refresh (3 hours)
        900        ; retry (15 minutes)
        604800     ; expire (1 week)
        86400      ; minimum (1 day)
        )
        NS   dns.test.com.
$TTL 60 ; 1 minute
dns                A    172.16.0.170
harbor             A    172.16.0.170
EOF
```

启动bind服务，并测试

```BASH
[root@harbor ~]# named-checkconf  #检查配置文件
[root@harbor ~]# systemctl start named && systemctl enable named
[root@harbor ~]# systemctl status named|grep Ac
   Active: active (running) since Mon 2021-06-21 14:49:48 CST; 21s ago
[root@harbor ~]# yum install bind-utils -y  
[root@harbor ~]# dig -t A harbor.host.com @172.16.0.170 +shor   #检查是否可以解析
172.16.0.170
```

修改主机DNS

```BASH
[root@master01 ~]# sed -i "s/114.114.114.115/172.16.0.170/g" /etc/sysconfig/network-scripts/ifcfg-eth0 
#sed -i "20d" /etc/sysconfig/network-scripts/ifcfg-eth0 
删除DNS2
[root@master01 ~]# systemctl restart network
[root@master01 ~]# ping -c 3 www.baidu.com
PING www.a.shifen.com (110.242.68.4) 56(84) bytes of data.
64 bytes from 110.242.68.4 (110.242.68.4): icmp_seq=1 ttl=50 time=64.6 ms
64 bytes from 110.242.68.4 (110.242.68.4): icmp_seq=2 ttl=50 time=65.0 ms
64 bytes from 110.242.68.4 (110.242.68.4): icmp_seq=3 ttl=50 time=66.4 ms
[root@master01 ~]# echo "master01.host.com" >/etc/hostname

[root@master02 ~]# sed -i "s/114.114.114.115/172.16.0.170/g" /etc/sysconfig/network-scripts/ifcfg-eth0 
[root@master02 ~]# echo "master02.host.com" >/etc/hostname


[root@node01 ~]# sed -i "s/114.114.114.115/172.16.0.170/g" /etc/sysconfig/network-s cripts/ifcfg-eth0 
[root@node01 ~]# echo "node01.host.com" >/etc/hostname 


[root@node02 ~]# sed -i "s/114.114.114.115/172.16.0.170/g" /etc/sysconfig/network-scripts/ifcfg-eth0 
[root@node02 ~]# echo "node02.host.com" >/etc/hostname 

#检查是否正确解析
[root@harbor ~]# dig -t A master01.host.com @172.16.0.170 +shor
172.16.0.160
[root@harbor ~]# dig -t A master02.host.com @172.16.0.170 +shor
172.16.0.161
[root@harbor ~]# dig -t A node01.host.com @172.16.0.170 +shor
172.16.0.165
[root@harbor ~]# dig -t A node02.host.com @172.16.0.170 +shor
172.16.0.166

```



## cfssl 自签证书

下载：https://github.com/cloudflare/cfssl/releases/tag/v1.6.0

| 名称                             | 目标名称                |
| -------------------------------- | ----------------------- |
| cfssl_1.6.0_linux_amd64          | /usr/bin/cfssl          |
| cfssljson_1.6.0_linux_amd64      | /usr/bin/cfssljson      |
| cfssl-certinfo_1.6.0_linux_amd64 | /usr/bin/cfssl-certinfo |

```BASH
chmod +x ./cfssl*   #别忘记给执行权限
[root@master01 /server/soft]# ll
total 39572
-rw-r--r-- 1 root root 16377936 Jun 20 13:46 cfssl_1.6.0_linux_amd64
-rw-r--r-- 1 root root 13245520 Jun 20 13:46 cfssl-certinfo_1.6.0_linux_amd64
-rw-r--r-- 1 root root 10892112 Jun 20 13:46 cfssljson_1.6.0_linux_amd64
[root@master01 /server/soft]# chmod +x cfssl*
[root@master01 /server/soft]# ll
total 39572
-rwxr-xr-x 1 root root 16377936 Jun 20 13:46 cfssl_1.6.0_linux_amd64
-rwxr-xr-x 1 root root 13245520 Jun 20 13:46 cfssl-certinfo_1.6.0_linux_amd64
-rwxr-xr-x 1 root root 10892112 Jun 20 13:46 cfssljson_1.6.0_linux_amd64
[root@master01 /server/soft]# mv cfssl_1.6.0_linux_amd64 /usr/bin/cfssl
mv cfssljson_1.6.0_linux_amd64 /usr/bin/cfssljson
mv cfssl-certinfo_1.6.0_linux_amd64 /usr/bin/cfssl-certinfo

[root@harbor /sever/soft]# mkdir -p ~/tls/{etcd,k8s}

```

## Docker

docker-compose
https://github.com/docker/compose/releases

```BASH
[root@master01 /server/soft]# tar xf docker-20.10.7.tgz 

[root@harbor /sever/soft]# for i in {160,161,165,166,170};do scp -r docker/*  docker-compose root@172.16.0.$i:/usr/bin/;done

#验正 所有机器
[root@harbor /server/soft]# docker --version
[root@harbor /server/soft]# docker-compose --version


#添加systemd管理docker服务
[root@master01 /server/soft]# cat > /usr/lib/systemd/system/docker.service << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target
[Service]
Type=notify
ExecStart=/usr/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s
[Install]
WantedBy=multi-user.target
EOF

[root@harbor /sever/soft]# for i in {160,161,165,166,170};do scp /usr/lib/systemd/system/docker.service root@172.16.0.$i:/usr/lib/systemd/system/;done

#重载docker环境启动并设置开机自启
[root@master01 /server/soft]# systemctl daemon-reload && systemctl start docker && systemctl enable docker

#查看重启状态
[root@master01 /server/soft]# systemctl status docker |grep Active
   Active: active (running) since Mon 2021-06-21 16:16:34 CST; 23s ago



#查看docker当前版本
[root@master01 /server/soft]# yum install -y yum-utils
[root@master01 /server/soft]# yum-config-manager --add-repo \
                   https://download.docker.com/linux/centos/docker-ce.repo
[root@master01 /server/soft]# yum list docker-ce --showduplicates | sort -r

```

#/etc/docker/daemon.json 文件不修改了。国外的速度也挺快的。估计docker 国内有镜像服务器了

```BASH
#!/bin/sh
yum install -y yum-utils
yum-config-manager --add-repo \
                   https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce docker-ce-cli containerd.io -y
```

shell 一键安装 docker 

## Harbor安装

下载：https://github.com/goharbor/harbor/releases

```bash

[root@harbor /server/soft]# tar xf harbor-offline-installer-v2.3.0-rc3.tgz -C /opt/
[root@harbor /opt]# ls harbor/
common.sh             harbor.yml.tmpl       LICENSE               
harbor.v2.3.0.tar.gz  install.sh            prepare       
#ln -s /opt/harbor-vx.x.x /opt/harbor   生产用软连接方式
[root@harbor /server/soft]# cd /opt/harbor/
[root@harbor /opt/harbor]# cp harbor.yml.tmpl harbor.yml
```

证书：

```BASH

[root@harbor /opt/harbor]# cd /root/tls/harbor/
[root@harbor ~/TLS/harbor]# cat > ca-csr.json <<EOF
{
    "CN": "h",   
    "hosts": [],     
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "yunnan",
            "L": "kunming",
            "O": "test",
            "OU": "ops"
        }
    ],
    "ca": {
        "expiry": "175200h" 
    }
}
EOF

CN: Common Name ,浏览器使用该字段验证网站是否合法, 一般写的是域名。非常重要。浏览器使用该字段验证网站是否合法
C: Country,国家
ST:State,州，省
L: Locality ,地区,城市
O: Organization Name ,组织名称,公司名称
OU: Organization Unit Name ,组织单位名称,公司部门  dev 开发  ops 运维
ca.expiry 证书有效期 175200h=20年   175200/24/365=20年

[root@harbor ~/TLS/harbor]# cat >ca-config.json<<EOF
{
    "signing": {
        "default": {
            "expiry": "175200h"
        },
        "profiles": {
            "harbor": {
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
          
        }
    }
}
EOF

#signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；
#server 表示服务端连接客户端时携带的证书，用于客户端验证服务端身份
#client 表示客户端连接服务端时携带的证书，用于服务端验证客户端身份
#peer 表示相互之间连接时使用的证书，如etcd节点之间验证
#ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；最好是可以分开这样逻辑清晰

[root@harbor ~/TLS/harbor]# cfssl gencert -initca ca-csr.json |cfssljson -bare ca -
#生成ca.pem ca-key.pem
ca.csr  ca-csr.json  ca-key.pem  ca.pem

ca-csr.json   ca-config.json 是根证书

[root@harbor ~/TLS/harbor]# cat >harbor-csr.json <<EOF
{
    "CN": "harbor",
    "hosts": [
		"172.16.0.170"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "kunming",
            "L": "kunming",
            "O": "test",
            "OU": "ops"
        }
    ]
}
EOF

#这里文件会多加几个字符进去删除即可。

harbor.json  服务证书需要调用上面的根证书
[root@harbor ~/TLS/harbor]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=harbor harbor-csr.json | cfssljson -bare harbor

#注释hosts 字段指定授权使用该证书的当前部署节点 IP，如果后续使用域名访问 harbor 则还需要添加域名；
#编译成证书

[root@harbor ~/tls/harbor]# ls
ca-config.json  ca-csr.json  ca.pem      harbor-csr.json  harbor.pem
ca.csr          ca-key.pem   harbor.csr  harbor-key.pem


[root@harbor /opt/harbor]# mkdir -p /etc/docker/certs.d/harbor
[root@harbor /opt/harbor]# cp /root/tls/harbor/harbor.pem /etc/docker/certs.d/harbor/harbor.crt

#用户访问家目录~
[root@harbor ~/tls/harbor]# cp /root/tls/harbor/harbor.pem ~
[root@harbor ~/tls/harbor]# for i in {160,161,165,166};do scp /root/tls/harbor/harbor.pem root@172.16.0.$i:~ ;done

[root@master01 ~]# mkdir -p /etc/docker/certs.d/harbor
[root@master02 ~]# mkdir -p /etc/docker/certs.d/harbor
[root@node01 ~]# mkdir -p /etc/docker/certs.d/harbor
[root@node02 ~]# mkdir -p /etc/docker/certs.d/harbor
[root@harbor ~/tls/harbor]# for i in {160,161,165,166};do scp /root/tls/harbor/harbor.pem root@172.16.0.$i:/etc/docker/certs.d/harbor/harbor.crt ;done


#data是数据存放目录
[root@harbor /opt/harbor]# mkdir /data
[root@harbor ~/tls/harbor]# cd /opt/harbor/
[root@harbor /opt/harbor]# vim harbor.yml
172.16.0.170
#http:
#  port: 80
https:
  # https port for harbor, default is 443
  port: 443
  # The path of cert and key files for nginx
  certificate: /root/tls/harbor/harbor.pem
  private_key: /root/tls/harbor/harbor-key.pem
harbor_admin_password: Harbor12345
data_volume: /data
#修改上面项目
[root@harbor /opt/harbor]# ./prepare 
#检查环境
[root@harbor /opt/harbor]# ./install.sh  
...
✔ ----Harbor has been installed and started successfully.----
#harbor启动 停止
[root@harbor /opt/harbor]# cd /opt/harbor
[root@harbor /opt/harbor]# /usr/bin/docker-compose stop
Stopping harbor-jobservice ... done
Stopping nginx             ... done
Stopping harbor-core       ... done
Stopping harbor-db         ... done
Stopping registry          ... done
Stopping registryctl       ... done
Stopping harbor-portal     ... done
Stopping redis             ... done
Stopping harbor-log        ... done
[root@harbor /opt/harbor]# docker-compose start
Starting log         ... done
Starting registry    ... done
Starting registryctl ... done
Starting postgresql  ... done
Starting portal      ... done
Starting redis       ... done
Starting core        ... done
Starting jobservice  ... done
Starting proxy       ... done
[root@harbor /opt/harbor]# docker-compose up -d
#重新创建容器
[root@harbor /opt/harbor]# docker-compose down
#删除所有harbor容器


#设置开机自启动
[root@harbor /opt/harbor]# chmod +x /etc/rc.d/rc.local 
[root@harbor /opt/harbor]# echo 'cd /opt/harbor && docker-compose start' >> /etc/rc.local  
```

测试

```bash
[root@harbor /opt/harbor]# cat >/etc/docker/daemon.json<<EOF 
{
"insecure-registries": ["172.16.0.170"] 
}
EOF
[root@harbor ~]# for i in {160,161,165,166};do scp /etc/docker/daemon.json root@172.16.0.$i:/etc/docker/ ;done

#服务端重启下harbor服务
[root@harbor /opt/harbor]# docker-compose stop && docker-compose start

#所有客户端操作
[root@master01 ~]# systemctl daemon-reload && systemctl restart docker

[root@harbor ~]# docker login 172.16.0.170
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[root@harbor ~]# docker pull busybox
Using default tag: latest
latest: Pulling from library/busybox
b71f96345d44: Pull complete 
Digest: sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest
[root@harbor ~]# docker tag busybox:latest harbor.test.com/library/busybox:1.33.1
[root@harbor ~]# docker push harbor.test.com/library/busybox:1.33.1
The push refers to repository [harbor.test.com/library/busybox]
5b8c72934dfc: Pushed 
1.33.1: digest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b size: 527


```

![image-20210623223513966](C:\Users\goo\AppData\Roaming\Typora\typora-user-images\image-20210623223513966.png)





## ETCD集群

证书

```BASH
[root@harbor ~/TLS/etcd]# cat > ca-csr.json << EOF
{
    "CN": "etcd",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing"
        }
    ]
}
EOF

[root@harbor ~/TLS/etcd]# cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "175200h"
    },
    "profiles": {
      "etcd": {
         "expiry": "175200h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF
[root@harbor ~/TLS/etcd]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

[root@harbor ~/TLS/etcd]# cat > etcd-csr.json << EOF
{
    "CN": "etcd",
    "hosts": [
    "localhost",
    "127.0.0.1",
    "172.16.0.160",
    "172.16.0.161",
    "172.16.0.162",
    "172.16.0.163",
    "172.16.0.164",
    "172.16.0.165"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing"
        }
    ]
}
EOF

[root@harbor ~/TLS/etcd]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd etcd-csr.json | cfssljson -bare etcd

[root@harbor ~/tls/etcd]# ls
ca-config.json  ca-csr.json  ca.pem    etcd-csr.json  etcd.pem
ca.csr          ca-key.pem   etcd.csr  etcd-key.pem
```



```BASH
[root@master01 /server/soft]# tar xf etcd-v3.5.0-linux-amd64.tar.gz -C /opt/

[root@master01 ~]# mkdir -p /opt/etcd/{bin,cfg,ssl}
三台etcd

[root@harbor /server/soft]# cd /opt/etcd-v3.5.0-linux-amd64/
[root@harbor /opt/etcd-v3.5.0-linux-amd64]# for i in 160 161 165 ;do scp etcd etcdctl root@172.16.0.$i:/opt/etcd/bin;done
  
[root@harbor /opt/etcd]# for i in {160,161,165};do scp /root/tls/etcd/*.pem root@172.16.0.$i:/opt/etcd/ssl/ ;done

[root@master01 ~]# ls /opt/etcd/ssl/
ca-key.pem  ca.pem  etcd-key.pem  etcd.pem

[root@master01 /opt]# cat > /opt/etcd/cfg/etcd.conf << EOF
#[Member]
ETCD_NAME="etcd-1"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://172.16.0.160:2380"
ETCD_LISTEN_CLIENT_URLS="https://172.16.0.160:2379"
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.16.0.160:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://172.16.0.160:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://172.16.0.160:2380,etcd-2=https://172.16.0.161:2380,etcd-3=https://172.16.0.165:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF

//ETCD_NAME：节点名称，集群中唯一
//ETCD_DATA_DIR：数据目录
//ETCD_LISTEN_PEER_URLS：集群通信监听地址
//ETCD_LISTEN_CLIENT_URLS：客户端访问监听地址
//ETCD_INITIAL_ADVERTISE_PEER_URLS：集群通告地址
//ETCD_ADVERTISE_CLIENT_URLS：客户端通告地址
//ETCD_INITIAL_CLUSTER：集群节点地址
//ETCD_INITIAL_CLUSTER_TOKEN：集群Token
//ETCD_INITIAL_CLUSTER_STATE：加入集群的当前状态，new是新集群，existing表示加入已有集群

systemd管理
[root@master01 /opt]# cat > /usr/lib/systemd/system/etcd.service << EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
[Service]
Type=notify
EnvironmentFile=/opt/etcd/cfg/etcd.conf
ExecStart=/opt/etcd/bin/etcd \
--cert-file=/opt/etcd/ssl/etcd.pem \
--key-file=/opt/etcd/ssl/etcd-key.pem \
--peer-cert-file=/opt/etcd/ssl/etcd.pem \
--peer-key-file=/opt/etcd/ssl/etcd-key.pem \
--trusted-ca-file=/opt/etcd/ssl/ca.pem \
--peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \
--logger=zap
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF

#三台都操作并 etcd --version 验证
[root@master01 ~]# echo 'export PATH=$PATH:/opt/etcd/bin' >>/etc/profile
[root@master01 ~]# source /etc/profile

#同步文件
[root@master01 /opt]# scp -r /opt/etcd/ root@172.16.0.161:/opt/
[root@master01 /opt]# scp -r /opt/etcd/ root@172.16.0.165:/opt/
[root@master01 /opt]# scp /usr/lib/systemd/system/etcd.service root@172.16.0.161:/usr/lib/systemd/system/
[root@master01 /opt]# scp /usr/lib/systemd/system/etcd.service root@172.16.0.165:/usr/lib/systemd/system/


#修改参数
[root@master02 ~]# cat >/opt/etcd/cfg/etcd.conf <<EOF
#[Member]
ETCD_NAME="etcd-2"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://172.16.0.161:2380"
ETCD_LISTEN_CLIENT_URLS="https://172.16.0.161:2379"
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.16.0.161:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://172.16.0.161:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://172.16.0.160:2380,etcd-2=https://172.16.0.161:2380,etcd-3=https://172.16.0.165:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"  
EOF

[root@node01 ~]# cat >/opt/etcd/cfg/etcd.conf <<EOF
#[Member]
ETCD_NAME="etcd-3"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://172.16.0.165:2380"
ETCD_LISTEN_CLIENT_URLS="https://172.16.0.165:2379"
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.16.0.165:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://172.16.0.165:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://172.16.0.160:2380,etcd-2=https://172.16.0.161:2380,etcd-3=https://172.16.0.165:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new" 
EOF


[root@master01 /opt]# systemctl daemon-reload && systemctl start etcd && systemctl enable etcd
[root@master02 ~]# systemctl daemon-reload && systemctl start etcd && systemctl enable etcd
[root@node01 ~]# systemctl daemon-reload && systemctl start etcd && systemctl enable etcd


[root@master01 /opt]# systemctl status etcd|grep Active
   Active: active (running) since Tue 2021-06-22 21:01:46 CST; 39s ago

[root@master02 ~]# systemctl status etcd|grep Active
   Active: active (running) since Tue 2021-06-22 21:01:46 CST; 1min 0s ago

[root@node01 ~]# systemctl status etcd|grep Active
   Active: active (running) since Tue 2021-06-22 21:01:48 CST; 1min 2s ago
   
[root@master01 ~]# ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/etcd.pem --key=/opt/etcd/ssl/etcd-key.pem --endpoints="https://172.16.0.160:2379,https://172.16.0.161:2379,https://172.16.0.165:2379" endpoint health --write-out=table
   
   
   
[root@master01 ~]# ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/etcd.pem --key=/opt/etcd/ssl/etcd-key.pem --endpoints="https://172.16.0.160:2379,https://172.16.0.161:2379,https://172.16.0.165:2379" endpoint health
https://172.16.0.160:2379 is healthy: successfully committed proposal: took = 10.384439ms
https://172.16.0.165:2379 is healthy: successfully committed proposal: took = 11.382729ms
https://172.16.0.161:2379 is healthy: successfully committed proposal: took = 15.257159ms
[root@master01 ~]# ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/etcd.pem --key=/opt/etcd/ssl/etcd-key.pem --endpoints="https://172.16.0.160:2379,https://172.16.0.161:2379,https://172.16.0.165:2379" endpoint health --write-out=table
+---------------------------+--------+-------------+-------+
|         ENDPOINT          | HEALTH |    TOOK     | ERROR |
+---------------------------+--------+-------------+-------+
| https://172.16.0.160:2379 |   true | 10.753503ms |       |
| https://172.16.0.161:2379 |   true | 10.325548ms |       |
| https://172.16.0.165:2379 |   true | 11.325204ms |       |
+---------------------------+--------+-------------+-------+

/opt/etcd/bin]# ./etcdctl endpoint health

```

# master01

## apiserver



```BASH
#apiserver
[root@harbor ~/TLS/k8s]# cat > ca-csr.json << EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

[root@harbor ~/TLS/k8s]# cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "175200h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "175200h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF

[root@harbor ~/TLS/k8s]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

[root@harbor ~/TLS/k8s]# cat > apiserver-csr.json << EOF
{
    "CN": "kubernetes",
    "hosts": [
      "10.0.0.1",
      "127.0.0.1",
      "172.16.0.160",
      "172.16.0.161",
      "172.16.0.162",
      "172.16.0.163",
      "172.16.0.164",
      "172.16.0.165",
      "172.16.0.166",
      "172.16.0.167",
      "172.16.0.168",
      "172.16.0.169",
      "172.16.0.170",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF
[root@harbor ~/TLS/k8s]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-csr.json | cfssljson -bare apiserver
[root@harbor ~/tls/k8s]# ls
apiserver.csr       apiserver-key.pem  ca-config.json  ca-csr.json  ca.pem
apiserver-csr.json  apiserver.pem      ca.csr          ca-key.pem


[root@master01 ~]# mkdir /opt/kubernetes/{bin,cfg,ssl,logs} -p


[root@harbor /sever/soft]# tar xf kubernetes-server-linux-amd64.tar.gz 
[root@harbor /server/soft]# cd kubernetes/server/bin/
[root@harbor /sever/soft/kubernetes/server/bin]# scp {kube-apiserver,kube-controller-manager,kube-scheduler} root@172.16.0.160:/opt/kubernetes/bin/

[root@harbor /sever/soft/kubernetes/server/bin]# for i in {160,161};do scp {kubectl,kubeadm} root@172.16.0.$i:/usr/bin/;done

[root@harbor /sever/soft/kubernetes/server/bin]# scp /root/tls/k8s/*.pem root@172.16.0.160:/opt/kubernetes/ssl/


[root@master01 ~]# cat > /opt/kubernetes/cfg/kube-apiserver.conf << EOF
KUBE_APISERVER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--etcd-servers=https://172.16.0.160:2379,https://172.16.0.161:2379,https://172.16.0.165:2379 \\
--bind-address=172.16.0.160 \\
--secure-port=6443 \\
--advertise-address=172.16.0.160 \\
--allow-privileged=true \\
--service-cluster-ip-range=10.0.0.0/24 \\
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\
--authorization-mode=RBAC,Node \\
--enable-bootstrap-token-auth=true \\
--token-auth-file=/opt/kubernetes/cfg/token.csv \\
--service-node-port-range=30000-32767 \\
--kubelet-client-certificate=/opt/kubernetes/ssl/apiserver.pem \\
--kubelet-client-key=/opt/kubernetes/ssl/apiserver-key.pem \\
--tls-cert-file=/opt/kubernetes/ssl/apiserver.pem  \\
--tls-private-key-file=/opt/kubernetes/ssl/apiserver-key.pem \\
--client-ca-file=/opt/kubernetes/ssl/ca.pem \\
--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\
--service-account-issuer=api \\
--service-account-signing-key-file=/opt/kubernetes/ssl/apiserver-key.pem \\
--etcd-cafile=/opt/etcd/ssl/ca.pem \\
--etcd-certfile=/opt/etcd/ssl/etcd.pem \\
--etcd-keyfile=/opt/etcd/ssl/etcd-key.pem \\
--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\
--proxy-client-cert-file=/opt/kubernetes/ssl/apiserver.pem \\
--proxy-client-key-file=/opt/kubernetes/ssl/apiserver-key.pem \\
--requestheader-allowed-names=kubernetes \\
--requestheader-extra-headers-prefix=X-Remote-Extra- \\
--requestheader-group-headers=X-Remote-Group \\
--requestheader-username-headers=X-Remote-User \\
--enable-aggregator-routing=true \\
--audit-log-maxage=30 \\
--audit-log-maxbackup=3 \\
--audit-log-maxsize=100 \\
--audit-log-path=/opt/kubernetes/logs/k8s-audit.log"
EOF

注：上面两个\ \ 第一个是转义符，第二个是换行符，使用转义符是为了使用EOF保留换行符。
–logtostderr：启用日志
—v：日志等级
–log-dir：日志目录
–etcd-servers：etcd集群地址
–bind-address：监听地址
–secure-port：https安全端口
–advertise-address：集群通告地址
–allow-privileged：启用授权
–service-cluster-ip-range：Service虚拟IP地址段
–enable-admission-plugins：准入控制模块
–authorization-mode：认证授权，启用RBAC授权和节点自管理
–enable-bootstrap-token-auth：启用TLS bootstrap机制
–token-auth-file：bootstrap token文件
–service-node-port-range：Service nodeport类型默认分配端口范围
–kubelet-client-xxx：apiserver访问kubelet客户端证书
–tls-xxx-file：apiserver https证书
–etcd-xxxfile：连接Etcd集群证书
–audit-log-xxx：审计日志
1.20版本必须加的参数：--service-account-issuer，--service-account-signing-key-file
--etcd-xxxfile：连接Etcd集群证书
--audit-log-xxx：审计日志
启动聚合层相关配置：--requestheader-client-ca-file，--proxy-client-cert-file，--proxy-client-key-file，--requestheader-allowed-names，--requestheader-extra-headers-prefix，--requestheader-group-headers，--requestheader-username-headers，--enable-aggregator-routing  


[root@master01 /opt/kubernetes/yml]# kubectl config view -o jsonpath='{"Cluster name\tServer\n"}{range .clusters[*]}{.name}{"\t"}{.cluster.server}{"\n"}{end}'
Cluster name	Server
kubernetes	https://172.16.0.160:6443

```

创建token文件

```BASH
#随机创建一个
[root@master01 ~]# head -c 16 /dev/urandom | od -An -t x | tr -d ' '
67b53632dbaf0d830a8303a59befb03f
[root@master01 ~]# cat > /opt/kubernetes/cfg/token.csv << EOF
67b53632dbaf0d830a8303a59befb03f,kubelet-bootstrap,10001,"system:node-bootstrapper"
EOF
# 格式：token，用户名，UID，用户组

```

systemd管理apiserver

```BASH
[root@master01 ~]# cat > /usr/lib/systemd/system/kube-apiserver.service << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-apiserver.conf
ExecStart=/opt/kubernetes/bin/kube-apiserver \$KUBE_APISERVER_OPTS
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF

[root@master01 ~]# systemctl daemon-reload && systemctl start kube-apiserver && systemctl enable kube-apiserver

[root@master01 ~]# systemctl status kube-apiserver |grep Active

```

## kube-controller-manager

```BASH
[root@master01 /opt]# cat > /opt/kubernetes/cfg/kube-controller-manager.conf << EOF
KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--leader-elect=true \\
--kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig \\
--bind-address=127.0.0.1 \\
--allocate-node-cidrs=true \\
--cluster-cidr=10.244.0.0/16 \\
--service-cluster-ip-range=10.0.0.0/24 \\
--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\
--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \\
--root-ca-file=/opt/kubernetes/ssl/ca.pem \\
--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\
--cluster-signing-duration=175200h0m0s"
EOF

配置说明：
--kubeconfig：连接apiserver配置文件
--leader-elect：当该组件启动多个时，自动选举（HA）
--cluster-signing-cert-file/--cluster-signing-key-file：自动为kubelet颁发证书的CA，与apiserver保持一致


[root@harbor ~/TLS/k8s]# cat > kube-controller-manager-csr.json << EOF
{
  "CN": "system:kube-controller-manager",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing", 
      "ST": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF
[root@harbor ~/TLS/k8s]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
[root@harbor ~/tls/k8s]# ls
apiserver.csr       ca-config.json  ca.pem                            kube-controller-manager.pem
apiserver-csr.json  ca.csr          kube-controller-manager.csr
apiserver-key.pem   ca-csr.json     kube-controller-manager-csr.json
apiserver.pem       ca-key.pem      kube-controller-manager-key.pem


[root@harbor ~/tls/k8s]# scp kube-controller-manager*.pem root@172.16.0.160:/opt/kubernetes/ssl/

[root@master01 ~]# cd /opt/kubernetes/ssl/
#必须在上面这个证书目录运行下面的命令，下面会查找证书

KUBE_CONFIG="/opt/kubernetes/cfg/kube-controller-manager.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443"

#set-cluster  # 创建需要连接的集群信息，可以创建多个k8s集群信息
[root@master01 /opt/kubernetes/ssl]# kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
#创建用户账号，即用户登陆使用的客户端私有和证书，可以创建多个证书 
[root@master01 /opt/kubernetes/ssl]# kubectl config set-credentials kube-controller-manager \
  --client-certificate=./kube-controller-manager.pem \
  --client-key=./kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
# 设置context，即确定账号和集群对应关系  
[root@master01 /opt/kubernetes/ssl]# kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-controller-manager \
  --kubeconfig=${KUBE_CONFIG}
  
# 设置当前使用哪个context
[root@master01 /opt/kubernetes/ssl]# kubectl config use-context default \
  --kubeconfig=${KUBE_CONFIG}

[root@master01 /opt/kubernetes/ssl]# cat > /usr/lib/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-controller-manager.conf
ExecStart=/opt/kubernetes/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

#设置环境变量
]# echo 'export PATH=/opt/kubernetes/bin:$PATH' >>/etc/profile
]# source /etc/profile


[root@master01 /opt/kubernetes/ssl]# systemctl daemon-reload && systemctl start kube-controller-manager && systemctl enable kube-controller-manager
[root@master01 /opt/kubernetes/ssl]# systemctl status kube-controller-manager.service |grep Active
   Active: active (running) since Tue 2021-06-22 23:21:12 CST; 53s ago

```

## kube-scheduler

```BASH
[root@master01 /opt/kubernetes/ssl]# cat > /opt/kubernetes/cfg/kube-scheduler.conf << EOF
KUBE_SCHEDULER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--leader-elect=true \\
--kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig \\
--bind-address=127.0.0.1"
EOF

参数说明：
--kubeconfig：连接apiserver配置文件
--leader-elect：当该组件启动多个时，自动选举（HA）

```

签证

```BASH
#kube-scheduler
[root@harbor ~/TLS/k8s]# cat > kube-scheduler-csr.json << EOF
{
  "CN": "system:kube-scheduler",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF
[root@harbor ~/TLS/k8s]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
[root@harbor ~/tls/k8s]# ls
apiserver.csr       ca-csr.json                       kube-controller-manager.pem
apiserver-csr.json  ca-key.pem                        kube-scheduler.csr
apiserver-key.pem   ca.pem                            kube-scheduler-csr.json
apiserver.pem       kube-controller-manager.csr       kube-scheduler-key.pem
ca-config.json      kube-controller-manager-csr.json  kube-scheduler.pem
ca.csr              kube-controller-manager-key.pem
[root@harbor ~/tls/k8s]# scp kube-scheduler*.pem root@172.16.0.160:/opt/kubernetes/ssl
[root@master01 /opt/kubernetes/ssl]# ls
apiserver-key.pem  ca-key.pem  kube-controller-manager-key.pem  kube-scheduler-key.pem
apiserver.pem      ca.pem      kube-controller-manager.pem      kube-scheduler.pem


[root@master01 /opt/kubernetes/ssl]# pwd
/opt/kubernetes/ssl
#注意目录
KUBE_CONFIG="/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443" 

[root@master01 /opt/kubernetes/ssl]# kubectl config set-cluster kubernetes \
 --certificate-authority=/opt/kubernetes/ssl/ca.pem \
 --embed-certs=true \
 --server=${KUBE_APISERVER} \
 --kubeconfig=${KUBE_CONFIG}
 
[root@master01 /opt/kubernetes/ssl]# kubectl config set-credentials kube-scheduler \
 --client-certificate=./kube-scheduler.pem \
 --client-key=./kube-scheduler-key.pem \
 --embed-certs=true \
 --kubeconfig=${KUBE_CONFIG}
 
[root@master01 /opt/kubernetes/ssl]# kubectl config set-context default \
 --cluster=kubernetes \
 --user=kube-scheduler \
 --kubeconfig=${KUBE_CONFIG}
 
[root@master01 /opt/kubernetes/ssl]# kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

[root@master01 /opt/kubernetes/ssl]# cat > /usr/lib/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-scheduler.conf
ExecStart=/opt/kubernetes/bin/kube-scheduler \$KUBE_SCHEDULER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

[root@master01 /opt/kubernetes/ssl]# systemctl daemon-reload && systemctl start kube-scheduler && systemctl enable kube-scheduler
[root@master01 /opt/kubernetes/ssl]# systemctl status kube-scheduler.service |grep Active
   Active: active (running) since Tue 2021-06-22 23:31:15 CST; 37s ago

```

## 生成kubelet连接集群的证书



```BASH
#admin-csr
[root@harbor ~/TLS/k8s]# cat > admin-csr.json <<EOF
{
  "CN": "kubernetes-admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF
[root@harbor ~/TLS/k8s]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
[root@harbor ~/tls/k8s]# ls
admin.csr           apiserver-key.pem  ca.pem                            kube-scheduler-csr.json
admin-csr.json      apiserver.pem      kube-controller-manager.csr       kube-scheduler-key.pem
admin-key.pem       ca-config.json     kube-controller-manager-csr.json  kube-scheduler.pem
admin.pem           ca.csr             kube-controller-manager-key.pem
apiserver.csr       ca-csr.json        kube-controller-manager.pem
apiserver-csr.json  ca-key.pem         kube-scheduler.csr

[root@harbor ~/tls/k8s]# scp admin*.pem root@172.16.0.160:/opt/kubernetes/ssl

[root@master01 /opt/kubernetes/ssl]# ls
admin-key.pem  apiserver-key.pem  ca-key.pem  kube-controller-manager-key.pem  kube-scheduler-key.pem
admin.pem      apiserver.pem      ca.pem      kube-controller-manager.pem      kube-scheduler.pem

[root@master01 /opt/kubernetes/ssl]# mkdir -p /root/.kube

KUBE_CONFIG="/root/.kube/config"
KUBE_APISERVER="https://172.16.0.160:6443"

[root@master01 /opt/kubernetes/ssl]# kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes/ssl]# kubectl config set-credentials cluster-admin \
  --client-certificate=./admin.pem \
  --client-key=./admin-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes/ssl]# kubectl config set-context default \
  --cluster=kubernetes \
  --user=cluster-admin \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes/ssl]# kubectl config use-context default --kubeconfig=${KUBE_CONFIG}


[root@master01 /opt/kubernetes/ssl]# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.16.0.160:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: cluster-admin
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: cluster-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

[root@master01 /opt/kubernetes/ssl]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
controller-manager   Healthy   ok                              
scheduler            Healthy   ok                              
etcd-1               Healthy   {"health":"true","reason":""}   
etcd-0               Healthy   {"health":"true","reason":""}   
etcd-2               Healthy   {"health":"true","reason":""}   

#授权kubelet-bootstrap用户允许请求证书
[root@master01 ~]# kubectl create clusterrolebinding kubelet-bootstrap \
 --clusterrole=system:node-bootstrapper \
 --user=kubelet-bootstrap


[root@master01 /opt/kubernetes]# for i in {kube-apiserver,kube-controller-manager,kube-scheduler};do systemctl status $i |grep Active ;done
   Active: active (running) since Wed 2021-06-23 08:37:29 CST; 38min ago
   Active: active (running) since Wed 2021-06-23 08:37:29 CST; 38min ago
   Active: active (running) since Wed 2021-06-23 08:37:30 CST; 38min ago
   
[root@master01 /opt/kubernetes/ssl]# kubectl version
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.2", GitCommit:"092fbfbf53427de67cac1e9fa54aaa09a28371d7", GitTreeState:"clean", BuildDate:"2021-06-16T12:59:11Z", GoVersion:"go1.16.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.2", GitCommit:"092fbfbf53427de67cac1e9fa54aaa09a28371d7", GitTreeState:"clean", BuildDate:"2021-06-16T12:53:14Z", GoVersion:"go1.16.5", Compiler:"gc", Platform:"linux/amd64"}


```



## kubelet

```BASH
[root@master /server/soft/kubernetes/server/bin]# scp kubelet kube-proxy root@172.16.0.160:/opt/kubernetes/bin/

#kubelet node节点全安装
mkdir /opt/kubernetes/{bin,cfg,ssl,logs} -p

#161等下会复制
[root@harbor /server/soft/kubernetes/server/bin]# for i in 165 166 ;do scp kubelet kube-proxy root@172.16.0.$i:/opt/kubernetes/bin;done

[root@master01 /opt/kubernetes]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.21.2
k8s.gcr.io/kube-controller-manager:v1.21.2
k8s.gcr.io/kube-scheduler:v1.21.2
k8s.gcr.io/kube-proxy:v1.21.2
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0

[root@master01 /opt/kubernetes]# docker search docker.io/pause:3.4.1
NAME             DESCRIPTION               STARS     OFFICIAL   AUTOMATED
louwy001/pause   k8s.gcr.io/pause:3.4.1    0                    
ninokop/pause    k8s.gcr.io/pause:3.4.1    0        
[root@master01 /opt/kubernetes]# docker pull louwy001/pause:3.4.1
[root@master01 /opt/kubernetes]# docker images
[root@master01 /opt/kubernetes]# docker tag 0f8457a4c2ec k8s.gcr.io/pause:3.4.1
[root@master01 /opt/kubernetes]# docker images
REPOSITORY         TAG       IMAGE ID       CREATED        SIZE
k8s.gcr.io/pause   3.4.1     0f8457a4c2ec   5 months ago   683kB

[root@master01 /opt/kubernetes]# cat > /opt/kubernetes/cfg/kubelet-config.yml << EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /opt/kubernetes/ssl/ca.pem 
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110
EOF


[root@master01 /opt/kubernetes]# cat > /opt/kubernetes/cfg/kubelet.conf << EOF
KUBELET_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--hostname-override=master01 \\
--network-plugin=cni \\
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\
--config=/opt/kubernetes/cfg/kubelet-config.yml \\
--cert-dir=/opt/kubernetes/ssl \\
--pod-infra-container-image=louwy001/pause:3.4.1"
EOF

参数说明：
--hostname-override：显示名称，集群中唯一
--network-plugin：启用CNI
--kubeconfig：空路径，会自动生成，后面用于连接apiserver
--bootstrap-kubeconfig：首次启动向apiserver申请证书
--config：配置参数文件
--cert-dir：kubelet证书生成目录
--pod-infra-container-image：管理Pod网络容器的镜像

[root@master01 /opt/kubernetes]# cat /opt/kubernetes/cfg/token.csv 
67b53632dbaf0d830a8303a59befb03f,kubelet-bootstrap,10001,"system:node-bootstrapper"

#生成kubelet初次加入集群引导kubeconfig文件
KUBE_CONFIG="/opt/kubernetes/cfg/bootstrap.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443" 
TOKEN="67b53632dbaf0d830a8303a59befb03f" 
# 与token.csv里保持一致

# 生成 kubelet bootstrap kubeconfig 配置文件
[root@master01 /opt/kubernetes]# kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes]# kubectl config set-credentials "kubelet-bootstrap" \
  --token=${TOKEN} \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes]# kubectl config set-context default \
  --cluster=kubernetes \
  --user="kubelet-bootstrap" \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes]# kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

[root@master01 /opt/kubernetes]# cat > /usr/lib/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
After=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet.conf
ExecStart=/opt/kubernetes/bin/kubelet \$KUBELET_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

[root@master01 /opt/kubernetes]# systemctl daemon-reload && systemctl start kubelet && systemctl enable kubelet

[root@master01 /opt/kubernetes]# systemctl status kubelet |grep Active
   Active: active (running) since Thu 2021-06-24 11:40:48 CST; 25s ago


```

批准kubectl 证书请求

```BASH
[root@master01 /opt/kubernetes]# kubectl get csr
NAME                                                   AGE   SIGNERNAME                                    REQUESTOR           CONDITION
node-csr-bkCICKzNq46LCMBh68Nbi0wAzGCyGb0krVXVH8_TZq4   52s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
[root@master01 /opt/kubernetes]# kubectl certificate approve node-csr-bkCICKzNq46LCMBh68Nbi0wAzGCyGb0krVXVH8_TZq4
certificatesigningrequest.certificates.k8s.io/node-csr-bkCICKzNq46LCMBh68Nbi0wAzGCyGb0krVXVH8_TZq4 approved
[root@master01 /opt/kubernetes]# kubectl get node
NAME     STATUS     ROLES    AGE   VERSION
master   NotReady   <none>   5s    v1.21.2
NotReady 网络插件没有部署导致的
```

## kube-proxy

```BASH
[root@master01 /opt/kubernetes]# cat > /opt/kubernetes/cfg/kube-proxy.conf << EOF
KUBE_PROXY_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--config=/opt/kubernetes/cfg/kube-proxy-config.yml"
EOF

[root@master01 /opt/kubernetes]# cat > /opt/kubernetes/cfg/kube-proxy-config.yaml << EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig
hostnameOverride: master01
clusterCIDR: 10.0.0.0/24
EOF

[root@harbor ~/TLS/k8s]# cat > kube-proxy-csr.json << EOF
{
   "CN": "system:kube-proxy",
   "hosts": [],
   "key": {
     "algo": "rsa",
     "size": 2048
   },
   "names": [
     {
       "C": "CN",
       "L": "BeiJing",
       "ST": "BeiJing",
       "O": "k8s",
       "OU": "System"
     }
   ]
}
EOF

[root@harbor ~/TLS/k8s]#  cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
[root@harbor ~/tls/k8s]# ls
admin.csr           ca-config.json                    kube-controller-manager.pem  kube-proxy.pem
admin-csr.json      ca.csr                            kubelet.csr                  kube-scheduler.csr
admin-key.pem       ca-csr.json                       kubelet-csr.json             kube-scheduler-csr.json
admin.pem           ca-key.pem                        kubelet-key.pem              kube-scheduler-key.pem
apiserver.csr       ca.pem                            kubelet.pem                  kube-scheduler.pem
apiserver-csr.json  kube-controller-manager.csr       kube-proxy.csr
apiserver-key.pem   kube-controller-manager-csr.json  kube-proxy-csr.json
apiserver.pem       kube-controller-manager-key.pem   kube-proxy-key.pem
[root@harbor ~/tls/k8s]# scp kube-proxy*.pem root@172.16.0.160:/opt/kubernetes/ssl/


#生成kubeconfig文件：
KUBE_CONFIG="/opt/kubernetes/cfg/kube-proxy.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443"

[root@master01 /opt/kubernetes]# kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes]# cd ssl/
[root@master01 /opt/kubernetes/ssl]#  kubectl config set-credentials kube-proxy \
  --client-certificate=./kube-proxy.pem \
  --client-key=./kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes/ssl]#  kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=${KUBE_CONFIG}
  
[root@master01 /opt/kubernetes/ssl]#  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}


[root@master01 /opt/kubernetes/ssl]# cat > /usr/lib/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-proxy.conf
ExecStart=/opt/kubernetes/bin/kube-proxy \$KUBE_PROXY_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

[root@master01 /opt/kubernetes/ssl]# systemctl daemon-reload && systemctl start kube-proxy && systemctl enable kube-proxy

[root@master01 /opt/kubernetes/ssl]# systemctl status kube-proxy|grep Active
   Active: active (running) since Thu 2021-06-24 11:52:22 CST; 29s ago


```



























kube-proxy 共有3种流量调度模式，分别是 namespace，iptables，ipvs，其中ipvs性能最好。

```BASH
[root@master01 /opt/kubernetes/ssl]# for i in $(ls /usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs|grep -o "^[^.]*");do echo $i; /sbin/modinfo -F filename $i >/dev/null 2>&1 && /sbin/modprobe $i;done
[root@master01 /opt/kubernetes/ssl]# lsmod | grep ip_vs
```

# master02

```BASH
[root@master01 /opt]# scp -r /opt/kubernetes/* root@172.16.0.161:/opt/kubernetes/
[root@master01 /opt]# scp /usr/lib/systemd/system/kube* root@172.16.0.161:/usr/lib/systemd/system/
[root@master01 /opt]# scp -r /root/.kube root@172.16.0.161:~
[root@master01 ~]# scp /usr/bin/{kubectl,kubeadm} root@172.16.0.161:/usr/bin/

#删除自动生成的证书
[root@master02 ~]# rm -f /opt/kubernetes/cfg/kubelet.kubeconfig 
[root@master02 ~]# rm -f /opt/kubernetes/ssl/kubelet*


[root@master02 ~]# vim /opt/kubernetes/cfg/kube-apiserver.conf 
--bind-address=172.16.0.161 \
--secure-port=6443 \
--advertise-address=172.16.0.161 \   

[root@master02 ~]# vim /opt/kubernetes/cfg/kubelet.conf 
--hostname-override=master02 \ 

[root@master02 ~]# vim /opt/kubernetes/cfg/kube-proxy-config.yml
hostnameOverride: master02  

[root@master02 ~]# for i in {kube-apiserver,kube-controller-manager,kube-scheduler,kubelet,kube-proxy};do systemctl daemon-reload && systemctl start $i && systemctl enable $i;systemctl status $i |grep Active ;done

[root@master02 ~]# vim .kube/config 
server: https://172.16.0.161:6443


[root@master02 ~]# systemctl restart kubelet kube-proxy
[root@master02 ~]# kubectl get csr
NAME                                                   AGE   SIGNERNAME                                    REQUESTOR           CONDITION
node-csr-nfBpbg9N7QW7p3i-TMOAzn1IsmRNCn-cYOIDd8Y949k   43s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
[root@master02 ~]# kubectl certificate approve node-csr-nfBpbg9N7QW7p3i-TMOAzn1IsmRNCn-cYOIDd8Y949k

[root@master02 ~]# kubectl  get node
NAME       STATUS     ROLES    AGE    VERSION
master     NotReady   <none>   124m   v1.21.2
master02   NotReady   <none>   37s    v1.21.2

```

# node

```BASH
#加入集群
[root@master02 ~]# for i in 165 166 170;do scp -r /opt/kubernetes root@172.16.0.165:/opt/
; done

[root@master02 ~]# for i in 165 166 170;do scp /usr/lib/systemd/system/{kubelet,kube-proxy}.service root@172.16.0.$i:/usr/lib/systemd/system/ ;done 

#165.166.170三台操作
[root@node01 ~]# rm -f /opt/kubernetes/cfg/kubelet.kubeconfig
[root@node01 ~]# rm -f /opt/kubernetes/ssl/kubelet*


[root@node01 ~]# sed -i "s/master02/node01/g" /opt/kubernetes/cfg/kubelet.conf 
[root@node01 ~]# sed -i "s/master02/node01/g" /opt/kubernetes/cfg/kube-proxy-config.yml 
[root@node01 ~]# systemctl daemon-reload && systemctl start kubelet kube-proxy && systemctl enable kubelet kube-proxy

[root@node02 ~]# sed -i "s/master02/node02/g" /opt/kubernetes/cfg/kubelet.conf 
[root@node02 ~]# sed -i "s/master02/node02/g" /opt/kubernetes/cfg/kube-proxy-config.yml 
[root@node02 ~]# systemctl daemon-reload && systemctl start kubelet kube-proxy && systemctl enable kubelet kube-proxy

#测试环境不浪费
[root@harbor ~]# sed -i "s/master02/harbor/g" /opt/kubernetes/cfg/kubelet.conf 
[root@harbor ~]# sed -i "s/master02/harbor/g" /opt/kubernetes/cfg/kube-proxy-config.yml 
[root@harbor ~/tls/k8s]# systemctl daemon-reload && systemctl start kubelet kube-proxy && systemctl enable kubelet kube-proxy


#node01
[root@master01 ~]# kubectl get csr
[root@master01 ~]# kubectl certificate approve node-csr-kiU6H0q09iwN16B7-tebYWUXIbTW8bSIs6rY6M9_Qro   

#node02
[root@master01 ~]# kubectl get csr
[root@master01 ~]# kubectl certificate approve node-csr-r2F0LehbVMnE2cVd95ROgAfy57KPcTPr-uFwd5u_MIM

#harbor
[root@master01 ~]# kubectl get csr
[root@master01 ~]# kubectl certificate approve node-csr-HXvgrmswocCymGSTvL5hSXVuV957h-BOeHN2Vl9ttuI

[root@master01 ~]# kubectl get csr
NAME                                                   AGE     SIGNERNAME                                    REQUESTOR           CONDITION
node-csr-HXvgrmswocCymGSTvL5hSXVuV957h-BOeHN2Vl9ttuI   44s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issued
node-csr-kiU6H0q09iwN16B7-tebYWUXIbTW8bSIs6rY6M9_Qro   5m1s    kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issued
node-csr-nfBpbg9N7QW7p3i-TMOAzn1IsmRNCn-cYOIDd8Y949k   47m     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issued
node-csr-r2F0LehbVMnE2cVd95ROgAfy57KPcTPr-uFwd5u_MIM   2m48s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Approved,Issued
[root@master01 ~]# kubectl get node
NAME       STATUS     ROLES    AGE     VERSION
harbor     NotReady   <none>   17s     v1.21.2
master     NotReady   <none>   170m    v1.21.2
master02   NotReady   <none>   46m     v1.21.2
node01     NotReady   <none>   3m59s   v1.21.2
node02     NotReady   <none>   75s     v1.21.2

```

### 细节处理

```BASH
pause:3.4.1 的镜像还没有准备好的
[root@master01 ~]# docker search docker.io/pause:3.4.1
NAME             DESCRIPTION               STARS     OFFICIAL   AUTOMATED
louwy001/pause   k8s.gcr.io/pause:3.4.1    0                    
ninokop/pause    k8s.gcr.io/pause:3.4.1    0         
[root@master01 /opt/kubernetes]# docker pull louwy001/pause:3.4.1
[root@master01 /opt/kubernetes]# docker images
[root@master01 /opt/kubernetes]# docker tag 0f8457a4c2ec k8s.gcr.io/pause:3.4.1
[root@master01 /opt/kubernetes]# docker images
REPOSITORY         TAG       IMAGE ID       CREATED        SIZE
k8s.gcr.io/pause   3.4.1     0f8457a4c2ec   5 months ago   683kB

[root@master01 ~]# docker login harbor.test.com
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[root@master01 ~]# docker tag 0f8457a4c2ec harbor.test.com/k8s.gcr.io/pause:3.4.1
[root@master01 ~]# docker images
REPOSITORY                         TAG       IMAGE ID       CREATED        SIZE
louwy001/pause                     3.4.1     0f8457a4c2ec   5 months ago   683kB
xyz349925756/k8s                   3.4.1     0f8457a4c2ec   5 months ago   683kB
harbor.test.com/k8s.gcr.io/pause   3.4.1     0f8457a4c2ec   5 months ago   683kB
k8s.gcr.io/pause                   3.4.1     0f8457a4c2ec   5 months ago   683kB
k8s/pause                          3.4.1     0f8457a4c2ec   5 months ago   683kB
[root@master01 ~]# docker push harbor.test.com/k8s.gcr.io/pause:3.4.1
The push refers to repository [harbor.test.com/k8s.gcr.io/pause]
915e8870f7d1: Pushed 
3.4.1: digest: sha256:9ec1e780f5c0196af7b28f135ffc0533eddcb0a54a0ba8b32943303ce76fe70d size: 526

[root@master01 ~]# docker image rm -f `docker images -a -q`
[root@master01 ~]# docker pull harbor.test.com/k8s.gcr.io/pause:3.4.1
#所有kubelet node下载
[root@harbor /opt/harbor]# docker tag 0f8457a4c2ec k8s.gcr.io/pause:3.4.1 
#删除不要的
[root@master01 ~]# docker rmi harbor.test.com/k8s.gcr.io/pause:3.4.1

```



```BASH
[root@master01 ~]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.21.2
k8s.gcr.io/kube-controller-manager:v1.21.2
k8s.gcr.io/kube-scheduler:v1.21.2
k8s.gcr.io/kube-proxy:v1.21.2
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0

[root@master01 /opt/kubernetes/bin]# echo 'export PATH=$PATH:/opt/kubernetes/bin' >>/etc/profile
[root@master01 /opt/kubernetes/bin]# kube-apiserver --version
Kubernetes v1.21.2
[root@master01 /opt/kubernetes/bin]# kube-controller-manager --version
Kubernetes v1.21.2
[root@master01 /opt/kubernetes/bin]# kube-scheduler --version
Kubernetes v1.21.2
[root@master01 /opt/kubernetes/bin]# kubelet --version
Kubernetes v1.21.2
[root@master01 /opt/kubernetes/bin]# kube-proxy --version
Kubernetes v1.21.2
[root@master01 /opt/etcd]# /opt/etcd/etcd --version
etcd Version: 3.5.0
Git SHA: 946a5a6f2
Go Version: go1.16.3
Go OS/Arch: linux/amd64

```

从上面可以看出我们部署到尾声了，那么还有那些没有部署呢？

网络:calico 组件

## calico

```BASH
[root@master01 ~/yml]# ls
calicoctl-linux-amd64  calico.yaml  coredns.yaml  recommended.yaml
[root@master01 ~/yml]# kubectl create -f calico.yaml 


```





## COREDNS

是否需要部署















































# 报错

```BASH
0/1 nodes are available: 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn't tolerate.












[root@master01 /server/soft]# rm -f /etc/cni/net.d/*

[root@master01 /server/soft]# curl -H "Authorization: Bearer $token" -k https://172.16.0.160:6443/api/v1/namespaces/default/pods



journalctl -l -u kube-apiserver
journalctl -l -u kube-controller-manager
journalctl -l -u kube-scheduler
journalctl -l -u kubelet
journalctl -l -u kube-proxy
```

