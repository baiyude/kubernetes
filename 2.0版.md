kubernetes虐我千百遍，一定要手撕kubernetes，方可解恨！

# 环境规划

| 主机名       | 服务                    | CN                                 | O                              | 备注                                      |
| ------------ | ----------------------- | ---------------------------------- | ------------------------------ | ----------------------------------------- |
| master01     | etcd                    | etcd                               | system:etcd                    |                                           |
| 172.16.0.160 | kube-apiserver          | kubernetes                         | system:masters                 | etcd的客户端证书                          |
|              | kube-controller-manager | **system:kube-controller-manager** | system:kube-controller-manager |                                           |
|              | kube-scheduler          | **system:kube-scheduler**          | system:kube-scheduler          |                                           |
|              | kubelet                 | **system:node**:master01           | system:masters                 | apiserver的客户端system:node:`<nodeName>` |
|              | kube-proxy              | system:kube-proxy                  | system:node-proxy              |                                           |
|              | nginx                   |                                    |                                |                                           |
|              | keepalived              |                                    |                                |                                           |
|              | docker                  |                                    |                                |                                           |
| master02     | etcd                    |                                    |                                |                                           |
| 172.16.0.161 | kube-apiserver          |                                    |                                |                                           |
|              | kube-controller-manager |                                    |                                |                                           |
|              | kube-scheduler          |                                    |                                |                                           |
|              | kubelet                 |                                    |                                |                                           |
|              | kube-proxy              |                                    |                                |                                           |
|              | nginx                   |                                    |                                |                                           |
|              | keepalived              |                                    |                                |                                           |
|              | docker                  |                                    |                                |                                           |
| node01       | kubelet                 |                                    |                                |                                           |
| 172.16.0.165 | kube-proxy              |                                    |                                |                                           |
|              | docker                  |                                    |                                |                                           |
| node01       | kubelet                 |                                    |                                |                                           |
| 172.16.0.166 | kube-proxy              |                                    |                                |                                           |
|              | docker                  |                                    |                                |                                           |
| harbor       | kubelet                 |                                    |                                |                                           |
| 172.16.0.170 | kube-proxy              |                                    |                                |                                           |
|              | docker                  |                                    |                                |                                           |
|              | harbor                  | harbor                             | harbor                         |                                           |
|              | cfssl                   |                                    |                                |                                           |

根证书说明

| 名称(json)          |          CN          |  profile   |       O        |
| ------------------- | :------------------: | :--------: | :------------: |
| ca-csr              |        harbor        |     -      |     harbor     |
| ca-config           |          -           |   harbor   |                |
| **ca-csr**          |      kubernetes      |     -      |   kubernetes   |
| **ca-config**       |          -           | kubernetes |       -        |
| **admin-csr**       | **kubernetes-admin** |     -      | system:masters |
| **service account** |   service-accounts   |            |   Kubernetes   |

## 目录结构规划

所有可执行命令全部移动到/usr/local/bin

```BASH
{ #ETCD master01 master02 node01
[root@master01 ~]# mkdir -p /opt/etcd/{cfg,ssl}
[root@master02 ~]# mkdir -p /opt/etcd/{cfg,ssl}
[root@node01 ~]# mkdir -p /opt/etcd/{cfg,ssl}
}

{  #kubenetes all 
mkdir -p /opt/kubernetes/{cfg,ssl,logs}
}
{ #harbor  cfssl 服务器操作 
[root@harbor ~]# mkdir tls/{etcd,kubernetes}
}
```

自动补全

```BASH
echo 'source /usr/share/bash-completion/bash_completion' >> ~/.bashrc
echo 'source <(kubectl completion bash)' >> ~/.bashrc
source ~/.bashrc
#将补全脚本添加到目录 /etc/bash_completion.d 中：
kubectl completion bash >/etc/bash_completion.d/kubectl
```



### 服务检查脚本

```BASH
[root@master01 ~]# cat service_check.sh
#!/bin/bash
#后面做个判断的
echo "Kube-apiserver_Check......"
#apiserver 检查
echo "+-------------------------------------------------------+";
for host in master01 master02;do for i in kube-apiserver kube-controller-manager kube-scheduler;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";

echo "Etcd_Check......"
echo "+-------------------------------------------------------+";
#etcd检查
for host in master01 master02 node01;do echo -e "    $host  etcd is  |  \c" && ssh $host systemctl status etcd|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";

echo "docker_Check......"
echo "+-------------------------------------------------------+";
#etcd检查
for host in master01 master02 node01 node02 harbor;do echo -e "    $host  docker is  |  \c" && ssh $host systemctl status docker|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";

echo "Kube-proxy kubelet_Check......"
#kubelet,proxy检查
echo "+-------------------------------------------------------+";
for host in master01 master02 node01 node02 harbor;do for i in kube-proxy kubelet;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";
```





## cfssl

下载：https://github.com/cloudflare/cfssl/releases/tag/v1.6.0

| 名称                             | 目标名称                |
| -------------------------------- | ----------------------- |
| cfssl_1.6.0_linux_amd64          | /usr/bin/cfssl          |
| cfssljson_1.6.0_linux_amd64      | /usr/bin/cfssljson      |
| cfssl-certinfo_1.6.0_linux_amd64 | /usr/bin/cfssl-certinfo |

```BASH
[root@harbor /server/soft]# chmod +x cfssl*
#执行权限
[root@harbor /server/soft]# ll
-rwxr-xr-x 1 root root  16377936 Jun 24 21:39 cfssl_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  13245520 Jun 24 21:39 cfssl-certinfo_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  10892112 Jun 24 21:39 cfssljson_1.6.0_linux_amd64
#把命令添加到/usr/bin
[root@harbor /server/soft]# mv cfssl_1.6.0_linux_amd64 /usr/bin/cfssl
[root@harbor /server/soft]# mv cfssljson_1.6.0_linux_amd64 /usr/bin/cfssljson
[root@harbor /server/soft]# mv cfssl-certinfo_1.6.0_linux_amd64 /usr/bin/cfssl-certinfo
```



**ca-config.json说明**

- default默认策略，指定了证书的默认有效期是一年(8760h)

- kubernetes：表示该配置(profile)的用途是为kubernetes生成证书及相关的校验工作

- signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE
- server auth：表示可以该CA 对 server 提供的证书进行验证
- client auth：表示可以用该 CA 对 client 提供的证书进行验证

- expiry：也表示过期时间，如果不写以default中的为准

- key encipherment 密钥加密

**ca-csr.json 说明**

**CN**: Common Name 浏览器使用该字段验证网站是否合法，一般是域名。非常重要。浏览器使用该字段验证网站是否合法
**key**：生成证书的算法
**hosts**：表示哪些主机名(域名)或者IP可以使用此csr申请的证书，为空或者""表示所有的都可以使用
**names**：一些其它的属性

- C: Country， 国家

- ST: State，州或者是省份

- L: Locality Name，地区，城市

- O: Organization Name，组织名称，公司名称(在k8s中常用于指定Group，进行RBAC绑定)

- OU: Organization Unit Name，组织单位名称，公司部门

### Harbor

```BASH
cat > ca-csr.json <<EOF
{
    "CN": "harbor",   
    "hosts": [],     
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "yunnan",
            "L": "kunming",
            "O": "harbor",
            "OU": "System"
        }
    ],
    "ca": {
        "expiry": "175200h" 
    }
}
EOF

cat >ca-config.json<<EOF
{
    "signing": {
        "default": {
            "expiry": "175200h"
        },
        "profiles": {
            "harbor": {
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
          
        }
    }
}
EOF

cfssl gencert -initca ca-csr.json |cfssljson -bare ca
#ca.csr ca-key.pem  ca.pem

cat >harbor-csr.json <<EOF
{
    "CN": "harbor",
    "hosts": [
      "172.16.0.170"                                                
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "kunming",
            "L": "kunming",
            "O": "harbor",
            "OU": "System"
        }
    ]
}
EOF

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=harbor harbor-csr.json | cfssljson -bare harbor
#harbor.pem  harbor-key.pem
```

### Etcd

```BASH
[root@harbor ~]# cd tls/etcd/
[root@harbor ~/tls/etcd]# 
cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "175200h"
    },
    "profiles": {
      "etcd": {
         "expiry": "175200h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF

cat > ca-csr.json << EOF
{
    "CN": "etcd CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "kunming",
            "ST": "yunnan" ,
            "O": "system:etcd",
            "OU": "System"
        }
    ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
#ca.pem ca-key.pem

cat > server-csr.json << EOF
{
    "CN": "etcd",
    "hosts": [
    "172.16.0.160",
    "172.16.0.161",
    "172.16.0.165"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "kunming",
            "ST": "yunnan",
            "O": "system:etcd",
            "OU": "System"
        }
    ]
}
EOF

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=etcd server-csr.json | cfssljson -bare server

#server-key.pem  server.pem
```

下去直接把etcd*配置好再返回这里操作*

### kubernetes

```BASH
[root@harbor /opt/harbor]# cd /root/tls/kubernetes/
[root@harbor ~/tls/kubernetes]# 
cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "175200h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "175200h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF
cat > ca-csr.json << EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "kunming",
            "ST": "yunnan",
            "O": "kubernetes",
            "OU": "System"
        }
    ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

#ca.csr  ca-key.pem  ca.pem

```

#### Apiserver

```BASH
cat > server-csr.json << EOF
{
    "CN": "kubernetes",
    "hosts": [
      "10.0.0.1",
      "127.0.0.1",
      "172.16.0.160",
      "172.16.0.161",
      "172.16.0.165",
      "172.16.0.166",
      "172.16.0.168",
      "172.16.0.170",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "kunming",
            "ST": "yunnan",
            "O": "kubernetes",
            "OU": "System"
        }
    ]
}
EOF

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes server-csr.json | cfssljson -bare server

#server.csr  server-key.pem  server.pem

```



#### kube-controller-manager

```BASH
cat > kube-controller-manager-csr.json << EOF
{
  "CN": "system:kube-controller-manager",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "kunming", 
      "ST": "yunnan",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

#kube-controller-manager.csr kube-controller-manager-key.pem  kube-controller-manager.pem

```

#### kube-scheduler

```BASH
cat > kube-scheduler-csr.json << EOF
{
  "CN": "system:kube-scheduler",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "kunming",
      "ST": "yunnan",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes kube-scheduler-csr.json  | cfssljson -bare kube-scheduler

#kube-scheduler.csr  kube-scheduler-key.pem  kube-scheduler.pem

```

#### kubectl 

```BASH
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "kunming",
      "ST": "yunnan",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes admin-csr.json | cfssljson -bare admin

#admin.csr  admin-key.pem  admin.pem

```

#### kube-proxy

```BASH
cat > kube-proxy-csr.json << EOF
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "kunming",
      "ST": "yunnan",
      "O": "kubernetes",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

#kube-proxy.csr  kube-proxy-key.pem  kube-proxy.pem

```

上面kubernetes生成的所有文件

```BASH
[root@harbor ~/tls/kubernetes]# ls
admin.csr       ca.csr                       kube-controller-manager-csr.json  kube-proxy-key.pem       kube-scheduler.pem
admin-csr.json  ca-csr.json                  kube-controller-manager-key.pem   kube-proxy.pem           server.csr
admin-key.pem   ca-key.pem                   kube-controller-manager.pem       kube-scheduler.csr       server-csr.json
admin.pem       ca.pem                       kube-proxy.csr                    kube-scheduler-csr.json  server-key.pem
ca-config.json  kube-controller-manager.csr  kube-proxy-csr.json               kube-scheduler-key.pem   server.pem

```

## 生成kubeconfig文件

### token配置文件

```BASH
cat > /opt/kubernetes/cfg/token.csv << EOF
e599e4ce448cb6b22e5167ebb83f6a49,kubelet-bootstrap,10001,"system:node-bootstrapper"
EOF
```

格式：token，用户名，UID，用户组

token也可自行生成替换：

```BASH
head -c 16 /dev/urandom | od -An -t x | tr -d ' '
```

准备生成证书所需的工具

```BASH
[root@harbor /server/soft]# tar xf kubernetes-server-linux-amd64.tar.gz 
[root@harbor /server/soft]# cd kubernetes/server/bin/
[root@harbor /server/soft/kubernetes/server/bin]# for i in `cat /root/host.txt`;do scp kubectl kube-proxy kubelet $i:/usr/local/bin;done
kubectl                              100%   44MB  72.5MB/s   00:00    
kube-proxy                           100%   41MB  74.1MB/s   00:00    
kubelet                              100%  113MB  77.1MB/s   00:01    
kubectl                              100%   44MB  53.7MB/s   00:00    
kube-proxy                           100%   41MB  78.6MB/s   00:00    
kubelet                              100%  113MB  72.3MB/s   00:01    
kubectl                              100%   44MB 106.2MB/s   00:00    
kube-proxy                           100%   41MB 116.7MB/s   00:00    
kubelet                              100%  113MB 124.5MB/s   00:00    
kubectl                              100%   44MB  70.1MB/s   00:00    
kube-proxy                           100%   41MB 113.3MB/s   00:00    
kubelet                              100%  113MB 121.1MB/s   00:00    
kubectl                              100%   44MB 103.2MB/s   00:00    
kube-proxy                           100%   41MB  94.7MB/s   00:00    
kubelet                              100%  113MB  97.6MB/s   00:01    
[root@harbor /server/soft/kubernetes/server/bin]# for i in master01 master02 ;do scp kube-apiserver kube-controller-manager kube-scheduler $i:/usr/local/bin;done
kube-apiserver                       100%  116MB  81.2MB/s   00:01    
kube-controller-manager              100%  111MB  80.4MB/s   00:01    
kube-scheduler                       100%   45MB  44.9MB/s   00:01    
kube-apiserver                       100%  116MB  69.2MB/s   00:01    
kube-controller-manager              100%  111MB  55.6MB/s   00:01    
kube-scheduler                       100%   45MB  80.1MB/s   00:00    
```

### kube-controlller-manager

```BASH
[root@harbor /server/soft/kubernetes/server/bin]# cd /root/tls/kubernetes
[root@harbor ~/tls/kubernetes]# 
--------------------------------------------------------------------
KUBE_CONFIG="/opt/kubernetes/cfg/kube-controller-manager.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=./ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
 
kubectl config set-credentials kube-controller-manager \
  --client-certificate=./kube-controller-manager.pem \
  --client-key=./kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-controller-manager \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

--------------------------------------------------------------------
[root@harbor ~/tls/kubernetes]# ls /opt/kubernetes/cfg/
kube-controller-manager.kubeconfig  token.csv
```

### kube-scheduler

```BASH
-----------------------------------------------------------
KUBE_CONFIG="/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=./ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials kube-scheduler \
  --client-certificate=./kube-scheduler.pem \
  --client-key=./kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-scheduler \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
-----------------------------------------------------------
[root@harbor ~/tls/kubernetes]# ls /opt/kubernetes/cfg/
kube-controller-manager.kubeconfig  kube-scheduler.kubeconfig  token.csv

```

### kubectl

```BASH

[root@harbor ~/tls/kubernetes]# for i in master01 master02 ;do ssh $i mkdir -p /root/.kube ;done

-------------------------------------------------------------
KUBE_CONFIG="/root/.kube/config"
KUBE_APISERVER="https://172.16.0.160:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=./ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials cluster-admin \
  --client-certificate=./admin.pem \
  --client-key=./admin-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=cluster-admin \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
--------------------------------------------------------------
[root@harbor ~/tls/kubernetes]# ls /root/.kube/
config
[root@harbor ~/tls/kubernetes]# for i in master01 master02;do scp /root/.kube/config $i:~/.kube/;done
config                         100% 6234     6.4MB/s   00:00    
config                         100% 6234     2.5MB/s   00:00    

```

### kubelet

```BASH

------------------------------------------------------------
KUBE_CONFIG="/opt/kubernetes/cfg/bootstrap.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443" 
TOKEN="e599e4ce448cb6b22e5167ebb83f6a49" 
# 与token.csv里保持一致

# 生成 kubelet bootstrap kubeconfig 配置文件
kubectl config set-cluster kubernetes \
  --certificate-authority=./ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials "kubelet-bootstrap" \
  --token=${TOKEN} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user="kubelet-bootstrap" \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
--------------------------------------------------------------
[root@harbor ~/tls/kubernetes]# ls /opt/kubernetes/cfg/
bootstrap.kubeconfig  kube-controller-manager.kubeconfig  kube-scheduler.kubeconfig  token.csv

```

### kube-proxy

```BASH
---------------------------------------------------------
KUBE_CONFIG="/opt/kubernetes/cfg/kube-proxy.kubeconfig"
KUBE_APISERVER="https://172.16.0.160:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=./ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials kube-proxy \
  --client-certificate=./kube-proxy.pem \
  --client-key=./kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
--------------------------------------------------------
[root@harbor ~/tls/kubernetes]# ls /opt/kubernetes/cfg/
bootstrap.kubeconfig  kube-controller-manager.kubeconfig  kube-proxy.kubeconfig  kube-scheduler.kubeconfig  token.csv

```



## 组件部署

### docker

docker-compose
https://github.com/docker/compose/releases

```BASH
[root@harbor /server/soft]# tar xf docker-20.10.7.tgz 
[root@harbor /server/soft]# for host in master01 master02 node01 node02 harbor;do scp docker/* root@$host:/usr/bin/;done
[root@harbor /server/soft]# for host in master01 master02 node01 node02 harbor;do scp docker-compose-Linux-x86_64 root@$host:/usr/bin/docker-compose ;done

{   #xshell 撰写操作 下文使用all 替代！
[root@harbor /server/soft]# docker --version
[root@harbor /server/soft]# chmod +x /usr/bin/docker-compose
[root@harbor /server/soft]# docker-compose --version
}

#systemd管理docker
[root@harbor /server/soft]# cat > /usr/lib/systemd/system/docker.service << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target
[Service]
Type=notify
ExecStart=/usr/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s
[Install]
WantedBy=multi-user.target
EOF

[root@harbor /server/soft]# for host in master01 master02 node01 node02 harbor;do scp /usr/lib/systemd/system/docker.service root@$host:/usr/lib/systemd/system/;done

{  #开启docker all
[root@harbor /server/soft]# systemctl daemon-reload && systemctl start docker && systemctl enable docker
[root@harbor /server/soft]# systemctl status docker |grep Active
   Active: active (running) since Sat 2021-06-26 15:40:35 CST; 24s ago

}
```



### harbor

下载：https://github.com/goharbor/harbor/releases

```BASH
#解压harbor
[root@harbor /server/soft]# tar xf harbor-offline-installer-v2.3.0-rc3.tgz  -C /opt/
[root@harbor /server/soft]# cd  /opt/harbor/
[root@harbor /opt/harbor]# ls
common.sh  harbor.v2.3.0.tar.gz  harbor.yml.tmpl  install.sh  LICENSE  prepare
[root@harbor /opt/harbor]# cp harbor.yml.tmpl{,.bak}
[root@harbor /opt/harbor]# mv harbor.yml.tmpl harbor.yml
[root@harbor /opt/harbor]# cd /root/tls/harbor
```

访问设置

```BASH
{# all
[root@harbor ~/tls/harbor]# mkdir -p /etc/docker/certs.d/harbor
}
#服务器端操作
[root@harbor ~/tls/harbor]# cp /root/tls/harbor/harbor.pem /etc/docker/certs.d/harbor/harbor.crt
[root@harbor ~/tls/harbor]# cp /root/tls/harbor/harbor.pem ~
[root@harbor ~/tls/harbor]# cd
[root@harbor ~]# cat host.txt 
master01
master02
node01
node02
harbor
#后期下发使用
[root@harbor ~]# for host in `cat /root/host.txt`;do scp /etc/docker/certs.d/harbor/harbor.crt root@$host:/etc/docker/certs.d/harbor/ ;done
[root@harbor ~]# for host in `cat /root/host.txt`;do scp /root/harbor.pem root@$host:~ ;done
[root@harbor ~]# mkdir /data     #容器存储持久化
[root@harbor /opt/harbor]# vim harbor.yml
{
 {
  hostname: 172.16.0.170
#http:
# port: 80
  certificate: /root/tls/harbor/harbor.pem
  private_key: /root/tls/harbor/harbor-key.pem  
harbor_admin_password: 12345      #因为演示密码旧简单点
database:
  password: root123   #自己根据实际情况修改
  }
}
[root@harbor /opt/harbor]# ./prepare #环境检查
[root@harbor /opt/harbor]# ./install.sh  #安装harbor
✔ ----Harbor has been installed and started successfully.----
#看到这个表示成功
[root@harbor /opt/harbor]# docker-compose ps #容器开启和端口映射信息
#设置开机启动
[root@harbor /opt/harbor]# chmod +x /etc/rc.d/rc.local 
[root@harbor /opt/harbor]# echo 'cd /opt/harbor && docker-compose start' >/root/docker-compose.sh
[root@harbor /opt/harbor]# echo 'sh /root/docker-compose.sh' >>/etc/rc.d/rc.local 
```

测试

```BASH
[root@harbor /opt/harbor]# cat >/etc/docker/daemon.json<<EOF 
{
"insecure-registries": ["172.16.0.170"] 
}
EOF
[root@harbor /opt/harbor]# for host in `cat /root/host.txt`;do scp /etc/docker/daemon.json root@$host:/etc/docker/ ;done
{ #all
[root@harbor /opt/harbor]# systemctl daemon-reload && systemctl restart docker
}
#随便找一台worker 验证一下是否可以登陆
[root@node02 ~]# docker login 172.16.0.170
Username: admin
Password: 
Error response from daemon: login attempt to http://172.16.0.170/v2/ failed with status: 502 Bad Gateway

#上面问题是修改了daemon.json 文件服务没有重启
[root@harbor /opt/harbor]# docker-compose stop && docker-compose start

#重启一下在尝试
[root@node02 ~]# docker login 172.16.0.170
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

####################功能测试####################################

[root@node02 ~]# docker pull busybox   
Using default tag: latest
latest: Pulling from library/busybox
b71f96345d44: Pull complete 
Digest: sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest
[root@node02 ~]# docker images
REPOSITORY   TAG       IMAGE ID       CREATED       SIZE
busybox      latest    69593048aa3a   2 weeks ago   1.24MB
[root@node02 ~]# docker tag 69593048aa3a 172.16.0.170/library/busybox:1.33.1
[root@node02 ~]# docker images
REPOSITORY                     TAG       IMAGE ID       CREATED       SIZE
busybox                        latest    69593048aa3a   2 weeks ago   1.24MB
172.16.0.170/library/busybox   1.33.1    69593048aa3a   2 weeks ago   1.24MB
[root@node02 ~]# docker push 172.16.0.170/library/busybox:1.33.1
The push refers to repository [172.16.0.170/library/busybox]
5b8c72934dfc: Pushed 
1.33.1: digest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b size: 527
##################################################################
```

### Etcd

```BASH
[root@harbor ~/tls/etcd]# for i in master01 master02 node01;do scp ca.pem server*.pem $i:/opt/etcd/ssl/;done
ca.pem                             100% 1322   666.4KB/s   00:00    
server-key.pem                     100% 1675   729.3KB/s   00:00    
server.pem                         100% 1440   869.3KB/s   00:00    
ca.pem                             100% 1322   719.2KB/s   00:00    
server-key.pem                     100% 1675   842.7KB/s   00:00    
server.pem                         100% 1440   954.3KB/s   00:00    
ca.pem                             100% 1322   730.8KB/s   00:00    
server-key.pem                     100% 1675     1.5MB/s   00:00    
server.pem                         100% 1440     1.2MB/s   00:00    
[root@harbor /server/soft]# for i in master01 master02 node01;do scp etcd-v3.5.0-linux-amd64/etcd* $i:/usr/local/bin/;done
etcd                               100%   22MB  61.4MB/s   00:00    
etcdctl                            100%   17MB  51.4MB/s   00:00    
etcdutl                            100%   15MB  55.9MB/s   00:00    
etcd                               100%   22MB  71.1MB/s   00:00    
etcdctl                            100%   17MB  45.3MB/s   00:00    
etcdutl                            100%   15MB  58.6MB/s   00:00    
etcd                               100%   22MB  66.8MB/s   00:00    
etcdctl                            100%   17MB  63.7MB/s   00:00    
etcdutl                            100%   15MB  80.8MB/s   00:00    

----------------------------------------------------------------------------------------
•	ETCD_NAME：节点名称，集群中唯一
•	ETCD_DATA_DIR：数据目录
•	ETCD_LISTEN_PEER_URLS：集群通信监听地址
•	ETCD_LISTEN_CLIENT_URLS：客户端访问监听地址
•	ETCD_INITIAL_ADVERTISE_PEERURLS：集群通告地址
•	ETCD_ADVERTISE_CLIENT_URLS：客户端通告地址
•	ETCD_INITIAL_CLUSTER：集群节点地址
•	ETCD_INITIALCLUSTER_TOKEN：集群Token,用于区分不同集群
•	ETCD_INITIALCLUSTER_STATE：加入集群的当前状态，new是新集群，existing表示加入已有集群
----------------------------------------------------------------------------------------

########################################################################################
ETCD_IP=$(hostname -i)
ETCD_NAME=$(hostname -s)
-------------------------------
cat > /etc/systemd/system/etcd.service << EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
 
  --cert-file=/opt/etcd/ssl/server.pem \\
  --key-file=/opt/etcd/ssl/server-key.pem \\
  --peer-cert-file=/opt/etcd/ssl/server.pem \\
  --peer-key-file=/opt/etcd/ssl/server-key.pem \\
  --trusted-ca-file=/opt/etcd/ssl/ca.pem \\
  --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\
  --listen-peer-urls https://${ETCD_IP}:2380 \\
  --listen-client-urls https://${ETCD_IP}:2379 \\
  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\
  --advertise-client-urls https://${ETCD_IP}:2379 \\
  --initial-cluster master01=https://172.16.0.160:2380,master02=https://172.16.0.161:2380,node01=https://172.16.0.165:2380 \\
  --initial-cluster-token etcd-cluster \\
  --initial-cluster-state new \\
  
  --data-dir=/var/lib/etcd/default.etcd \\
  --logger=zap


[Install]
WantedBy=multi-user.target
EOF

-----------------------------------
{ #all
[root@master01 ~]# systemctl daemon-reload && systemctl start etcd && systemctl enable etcd
}

#验证
------------------------------------------------------------------------------------------
ETCDCTL_API=3 /usr/local/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://172.16.0.160:2379,https://172.16.0.161:2379,https://172.16.0.165:2379" endpoint health --write-out=table
+---------------------------+--------+-------------+-------+
|         ENDPOINT          | HEALTH |    TOOK     | ERROR |
+---------------------------+--------+-------------+-------+
| https://172.16.0.160:2379 |   true | 12.286317ms |       |
| https://172.16.0.165:2379 |   true | 12.904411ms |       |
| https://172.16.0.161:2379 |   true | 36.507744ms |       |
+---------------------------+--------+-------------+-------+
------------------------------------------------------------------------------------------
```

### Apiserver

```BASH
[root@harbor ~/tls/kubernetes]# for i in master01 master02 ;do scp ca.pem ca-key.pem server.pem server-key.pem $i:/opt/kubernetes/ssl/ ;done
ca.pem                                          100% 1326   626.9KB/s   00:00    
ca-key.pem                                      100% 1679   648.0KB/s   00:00    
server.pem                                      100% 1623   136.5KB/s   00:00    
server-key.pem                                  100% 1679   445.6KB/s   00:00    
ca.pem                                          100% 1326   541.9KB/s   00:00    
ca-key.pem                                      100% 1679   652.2KB/s   00:00    
server.pem                                      100% 1623   650.3KB/s   00:00    
server-key.pem                                  100% 1679   884.6KB/s   00:00    
[root@harbor ~/tls/kubernetes]# for i in master01 master02 ;do scp /opt/kubernetes/cfg/token.csv $i:/opt/kubernetes/cfg/ ;done
token.csv                                       100%   84    36.7KB/s   00:00    
token.csv                                       100%   84    72.3KB/s   00:00    

-------------------------------------------------------------------------------------------------
[root@master01 ~]# cat > /etc/systemd/system/kube-apiserver.service << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver  \\
  --logtostderr=false \\
  --v=2 \\
  --log-dir=/opt/kubernetes/logs \\
  --etcd-servers=https://172.16.0.160:2379,https://172.16.0.161:2379,https://172.16.0.165:2379 \\
  --bind-address=172.16.0.160 \\
  --secure-port=6443 \\
  --advertise-address=172.16.0.160 \\
  --allow-privileged=true \\
  --service-cluster-ip-range=10.0.0.0/24 \\
  --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\
  --authorization-mode=RBAC,Node \\
  --enable-bootstrap-token-auth=true \\
  --token-auth-file=/opt/kubernetes/cfg/token.csv \\
  --service-node-port-range=30000-32767 \\
  --kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \\
  --kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \\
  --tls-cert-file=/opt/kubernetes/ssl/server.pem  \\
  --tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\
  --client-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\
  --service-account-issuer=api \\
  --service-account-signing-key-file=/opt/kubernetes/ssl/server-key.pem \\
  --etcd-cafile=/opt/etcd/ssl/ca.pem \\
  --etcd-certfile=/opt/etcd/ssl/server.pem \\
  --etcd-keyfile=/opt/etcd/ssl/server-key.pem \\
  --requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \\
  --proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \\
  --requestheader-allowed-names=kubernetes \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --enable-aggregator-routing=true \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/opt/kubernetes/logs/k8s-audit.log
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
-------------------------------------------------------------------------------------------
注：上面两个\ \ 第一个是转义符，第二个是换行符，使用转义符是为了使用EOF保留换行符。
•	--logtostderr：启用日志
•	---v：日志等级
•	--log-dir：日志目录
•	--etcd-servers：etcd集群地址
•	--bind-address：监听地址
•	--secure-port：https安全端口
•	--advertise-address：集群通告地址
•	--allow-privileged：启用授权
•	--service-cluster-ip-range：Service虚拟IP地址段
•	--enable-admission-plugins：准入控制模块
•	--authorization-mode：认证授权，启用RBAC授权和节点自管理
•	--enable-bootstrap-token-auth：启用TLS bootstrap机制
•	--token-auth-file：bootstrap token文件
•	--service-node-port-range：Service nodeport类型默认分配端口范围
•	--kubelet-client-xxx：apiserver访问kubelet客户端证书
•	--tls-xxx-file：apiserver https证书
•	1.20版本必须加的参数：--service-account-issuer，--service-account-signing-key-file
•	--etcd-xxxfile：连接Etcd集群证书
•	--audit-log-xxx：审计日志
•	启动聚合层相关配置：--requestheader-client-ca-file，--proxy-client-cert-file，--proxy-client-key-file，--requestheader-allowed-names，--requestheader-extra-headers-prefix，--requestheader-group-headers，--requestheader-username-headers，--enable-aggregator-routing
-----------------------------------------------------------------------------------------

[root@master01 ~]# systemctl daemon-reload && systemctl start kube-apiserver && systemctl enable kube-apiserver
[root@master01 ~]# systemctl status kube-apiserver.service 

```



### kube-controllre-manager

```BASH

[root@harbor ~/tls/kubernetes]# scp /opt/kubernetes/cfg/* master01:/opt/kubernetes/cfg/
bootstrap.kubeconfig                           100% 2122   155.4KB/s   00:00    
kube-controller-manager.kubeconfig             100% 6310     3.0MB/s   00:00    
kube-proxy.kubeconfig                          100% 6248     6.9MB/s   00:00    
kube-scheduler.kubeconfig                      100% 6268     6.4MB/s   00:00    
token.csv                                      100%   84   137.9KB/s   00:00    

--------------------------------------------------------------------
cat > /etc/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --logtostderr=false \\
  --v=2 \\
  --log-dir=/opt/kubernetes/logs \\
  --leader-elect=true \\
  --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig \\
  --bind-address=127.0.0.1 \\
  --allocate-node-cidrs=true \\
  --cluster-cidr=10.244.0.0/16 \\
  --service-cluster-ip-range=10.0.0.0/24 \\
  --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \\
  --root-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\
  --cluster-signing-duration=175200h0m0s
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
-----------------------------------------------------------------------
•	--kubeconfig：连接apiserver配置文件
•	--leader-elect：当该组件启动多个时，自动选举（HA）
•	--cluster-signing-cert-file/--cluster-signing-key-file：自动为kubelet颁发证书的CA，与apiserver保持一致
------------------------------------------------------------------------

[root@master01 ~]# systemctl daemon-reload && systemctl start kube-controller-manager && systemctl enable kube-controller-manager
[root@master01 ~]# systemctl status kube-controller-manager.service 

```



### kube-scheduler

```BASH

------------------------------------------------------------------
cat > /etc/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --logtostderr=false \\
  --v=2 \\
  --log-dir=/opt/kubernetes/logs \\
  --leader-elect=true \\
  --kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig \\
  --bind-address=127.0.0.1
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
------------------------------------------------------------------
•	--kubeconfig：连接apiserver配置文件
•	--leader-elect：当该组件启动多个时，自动选举（HA）
------------------------------------------------------------------
[root@master01 ~]# systemctl daemon-reload && systemctl start kube-scheduler && systemctl enable kube-scheduler

```

#### 授权用户

```BASH
kubectl create clusterrolebinding kubelet-bootstrap \
--clusterrole=system:node-bootstrapper \
--user=kubelet-bootstrap
#在其中一台master操作即可
```



### master02部署

```BASH
[root@master01 ~]# scp /etc/systemd/system/kube-{apiserver,controller-manager,scheduler}.service master02:/etc/systemd/system/
kube-apiserver.service                 100% 1958     2.2MB/s   00:00    
kube-controller-manager.service        100%  803   741.2KB/s   00:00    
kube-scheduler.service                 100%  388   387.4KB/s   00:00    
[root@master02 ~]# vim /etc/systemd/system/kube-apiserver.service 
----修改IP地址
--------------------------------------------------------------------------------------
[root@master01 /opt/kubernetes/ssl]# for i in kube-apiserver kube-controller-manager kube-scheduler ;do systemctl daemon-reload ; systemctl enable $i ; systemctl start $i ;done

[root@master02 ~]# sh service.sh 
Kube-apiserver_Check......
+-------------------------------------------------------+
    master01  kube-apiserver is : running
    master01  kube-controller-manager is : running
    master01  kube-scheduler is : running
    master02  kube-apiserver is : running
    master02  kube-controller-manager is : running
    master02  kube-scheduler is : running
+-------------------------------------------------------+
Etcd_Check......
+-------------------------------------------------------+
    master01  etcd is  |  running
    master02  etcd is  |  running
    node01  etcd is  |  running
+-------------------------------------------------------+
docker_Check......
+-------------------------------------------------------+
    master01  docker is  |  running
    master02  docker is  |  running
    node01  docker is  |  running
    node02  docker is  |  running
    harbor  docker is  |  running
+-------------------------------------------------------+
Kube-proxy kubelet_Check......
+-------------------------------------------------------+
    master01  kube-proxy is : Unit kube-proxy.service could not be found.
    master01  kubelet is : Unit kubelet.service could not be found.
    master02  kube-proxy is : Unit kube-proxy.service could not be found.
    master02  kubelet is : Unit kubelet.service could not be found.
    node01  kube-proxy is : Unit kube-proxy.service could not be found.
    node01  kubelet is : Unit kubelet.service could not be found.
    node02  kube-proxy is : Unit kube-proxy.service could not be found.
    node02  kubelet is : Unit kubelet.service could not be found.
    harbor  kube-proxy is : Unit kube-proxy.service could not be found.
    harbor  kubelet is : Unit kubelet.service could not be found.
+-------------------------------------------------------+

[root@master02 ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
controller-manager   Healthy   ok                              
scheduler            Healthy   ok                              
etcd-1               Healthy   {"health":"true","reason":""}   
etcd-0               Healthy   {"health":"true","reason":""}   
etcd-2               Healthy   {"health":"true","reason":""}   

```

### 准备工作

```BASH
[root@harbor /server/soft/kubernetes/server/bin]# scp kubeadm master01:/usr/local/bin/
[root@harbor /server/soft/kubernetes/server/bin]# scp kubeadm master02:/usr/local/bin/
[root@master01 ~]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.21.2
k8s.gcr.io/kube-controller-manager:v1.21.2
k8s.gcr.io/kube-scheduler:v1.21.2
k8s.gcr.io/kube-proxy:v1.21.2
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0
[root@master01 ~]# docker search pause:3.4.1
NAME             DESCRIPTION               STARS     OFFICIAL   AUTOMATED
louwy001/pause   k8s.gcr.io/pause:3.4.1    0                    
ninokop/pause    k8s.gcr.io/pause:3.4.1    0                    

[root@master01 ~]# docker pull louwy001/pause:3.4.1
3.4.1: Pulling from louwy001/pause
fac425775c9d: Pull complete 
Digest: sha256:9ec1e780f5c0196af7b28f135ffc0533eddcb0a54a0ba8b32943303ce76fe70d
Status: Downloaded newer image for louwy001/pause:3.4.1
docker.io/louwy001/pause:3.4.1
[root@master01 ~]# docker images
REPOSITORY       TAG       IMAGE ID       CREATED        SIZE
louwy001/pause   3.4.1     0f8457a4c2ec   5 months ago   683kB

```



### kubelet

```BASH
=============================================================================
cat > /opt/kubernetes/cfg/kubelet-config.yml << EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /opt/kubernetes/ssl/ca.pem 
authorization:
  mode: Webhook
EOF
================================================================================

KUBELET_NAME=$(hostname -s)

cat > /etc/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
After=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --logtostderr=false \\
  --v=2 \\
  --log-dir=/opt/kubernetes/logs \\
  --hostname-override=${KUBELET_NAME} \\
  --network-plugin=cni \\
  --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\
  --bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\
  --config=/opt/kubernetes/cfg/kubelet-config.yml \\
  --cert-dir=/opt/kubernetes/ssl \\

Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

--------------------------------------------------------------------------------
•	--hostname-override：显示名称，集群中唯一
•	--network-plugin：启用CNI
•	--kubeconfig：空路径，会自动生成，后面用于连接apiserver
•	--bootstrap-kubeconfig：首次启动向apiserver申请证书
•	--config：配置参数文件
•	--cert-dir：kubelet证书生成目录
•	--pod-infra-container-image：管理Pod网络容器的镜像
--------------------------------------------------------------------------------

[root@master01 ~]# systemctl daemon-reload && systemctl start kubelet && systemctl enable kubelet
[root@master01 ~]# systemctl status kubelet

```



### kube-proxy

```BASH
=============================================================
KUBE_PROXY_NAME=$(hostname -s)

cat > /opt/kubernetes/cfg/kube-proxy-config.yml << EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig
hostnameOverride: ${KUBE_PROXY_NAME}
clusterCIDR: 10.0.0.0/24
EOF
==============================================================

----------------------------------------------------------------------------
cat > /etc/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --logtostderr=false \\
  --v=2 \\
  --log-dir=/opt/kubernetes/logs \\
  --config=/opt/kubernetes/cfg/kube-proxy-config.yml
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
----------------------------------------------------------------------------
[root@master01 ~]#systemctl daemon-reload && systemctl start kube-proxy && systemctl enable kube-proxy
[root@master01 ~]# systemctl status kube-proxy.service 

```





### 批准kubelet证书申请并加入集群

```BASH
# 查看kubelet证书请求
kubectl get csr

# 批准申请
kubectl certificate approve   node-csr-uCEGPOIiDdlLODKts8J658HrFq9CZ--K6M4G7bjhk8A

# 查看节点
kubectl get node
NAME       STATUS     ROLES    AGE   VERSION
master01   NotReady   <none>   1s    v1.21.2

```

### 网络组件部署calico

```BASH
[root@master01 /opt/kubernetes]# mkdir yaml
[root@master01 /opt/kubernetes]# cd yaml/
[root@master01 /opt/kubernetes/yaml]# curl https://docs.projectcalico.org/manifests/calico.yaml -O
[root@master01 /opt/kubernetes/yaml]# kubectl create -f calico.yaml 
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-kube-controllers created



```





### 授权apiserver访问kubelet

```BASH
cat > apiserver-to-kubelet-rbac.yaml << EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
      - pods/log
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF

kubectl apply -f apiserver-to-kubelet-rbac.yaml

```



## node部署



```BASH
rm -f /opt/kubernetes/cfg/kubelet.kubeconfig 
rm -f /opt/kubernetes/ssl/kubelet*
删除kubelet证书和kubeconfig文件

vi /opt/kubernetes/cfg/kubelet.conf
vi /opt/kubernetes/cfg/kube-proxy-config.yml
修改主机名

```



```shell
# 创建 .conf 文件以在启动时加载模块
cat <<EOF | sudo tee /etc/modules-load.d/crio.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 配置 sysctl 参数，这些配置在重启之后仍然起作用
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sudo sysctl --system


[root@master01 ~]# docker tag 0f8457a4c2ec k8s.gcr.io/pause:3.4.1

```
